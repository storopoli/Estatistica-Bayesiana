---
title: "Comparação de Modelos"
description: "Como comparar modelos Bayesianos usando métricas objetivas"
author:
  - name: Jose Storopoli
    url: https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en
    affiliation: UNINOVE
    affiliation_url: https://www.uninove.br
    orcid_id: 0000-0002-0559-5176
date: August 1, 2021
citation_url: https://storopoli.io/Estatistica-Bayesiana/aux-Model_Comparison.html
slug: storopoli2021bayescompararmodelosR
bibliography: bib/bibliografia.bib
csl: bib/apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"/>

Depois de estimarmos um modelo Bayesiano, muitas vezes queremos medir sua precisão preditiva, por si só ou para fins de comparação, seleção ou cálculo de média do modelo [@geisser1979predictive].

Nas aulas desta disciplina nos debruçamos sobre diferentes gráficos de *Posterior Predictive Check* de diferentes modelos. Esta é uma maneira subjetiva e arbitrária de analisarmos e compararmos modelos entre si usando sua precisão preditiva. Há uma maneira objetiva de compararmos modelos Bayesianos com uma métrica robusta que nos ajude a selecionar qual o melhor modelo dentre o rol de modelos candidatos. Ter uma maneira objetiva de comparar modelos e escolher o melhor dentre eles é muito importante pois no *workflow* Bayesiano geralmente temos diversas iterações entre *prioris* e funções de verossimilhança o que ocasiona na criação de diversos modelos diferentes [@gelmanBayesianWorkflow2020].

## Métricas de Comparação de Modelos.

Temos diversas métricas de comparação de modelos que usam a precisão preditiva, sendo as principais:

* *Leave-one-out cross-validation* (LOO) [@vehtariPracticalBayesianModel2015]
* *Deviance Information Criterion* (DIC) [@spiegelhalter2002bayesian],  mas sabe-se que tem alguns problemas, que surgem em parte por não ser totalmente bayesiano, pois se baseia em uma estimativa pontual [@van2005dic]
* *Widely Applicable Information Criteria* (WAIC) [@watanabe2010asymptotic], totalmente Bayesiano no sentido de que usa toda a distribuição posterior, e é assintoticamente igual ao LOO [@vehtariPracticalBayesianModel2015]

Destes, já descartamos o DIC por termos problemas e ser baseado em uma estimativa pontual (afinal somos Bayesianos, se estivéssemos interessados em estimativas pontuais estaríamos ainda maximizando funções de verossimilhança e achando a moda como os frequentistas fazem).

LOO é computacionalmente intensivo, afinal na validação cruzada (*cross-validation*) re-estimamos o modelo para cada partição dos dados. *Leave-one-out* quer dizer que para um *dataset* de tamanho $N$ estimaremos $N-1$ modelos para $N-1$ partições do *dataset*. Ou seja, deixamos uma observação para fora e estimamos o modelo usando a partição de dados sem essa observações e repetimos para todas as observações do *dataset*. Para superar essa dificuldade, LOO pode ser aproximado por amostragem de importância [@gelfand1996model]. Mas tal estimativa pode ser imprecisa. Usando uma amostragem de importância com suavização de Pareto (*Pareto smoothed importance sampling* -- PSIS) podemos aproximar uma estimativa confiável ajustando uma distribuição de Pareto à cauda superior da distribuição dos pesos de importância. PSIS nos permite calcular LOO usando pesos de importância que de outra forma seriam instáveis. Tal aproximação é cunhada de PSIS-LOO [@vehtariPracticalBayesianModel2015].

WAIC pode ser visto como uma melhoria do DIC para modelos bayesianos. Apesar de WAIC ser assintoticamente igual ao LOO, PSIS-LOO é mais robusto no quando usamos *prioris* não-informativas ou na presença observações influentes (*outliers*).

## Como mensuramos precisão preditiva?

Bayesianos mensuram precisão preditiva usando simulações da distribuição posterior do modelo $\tilde{y}$. Para isso temos a distribuição preditiva posterior (*predictive posterior distribution*):

$$
p(\tilde{y} \mid y) = \int p(\tilde{y}_i \mid \theta) p(\theta \mid y) d \theta
$$

Onde $p(\theta \mid y)$ é a distribuição posterior do modelo (aquela que o `rstanarm` e `brms` estima para nós). A fórmula acima significa que calculamos a integral de toda a probabilidade conjunta da distribuição posterior preditiva com a distribuição posterior do nosso modelo: $p(\tilde{y}_i \mid \theta) p(\theta \mid y)$. Quanto maior a distribuição preditiva posterior $p(\tilde{y} \mid y)$ melhor será a precisão preditiva do modelo. Para mantermos comparabilidade com um dado *dataset* nos calculamos a esperança dessa medida (do inglês *expectation* que pode ser também interpretada como a média ponderada) para cada um dos $n \in N$ observações do *dataset*:

$$
\operatorname{elpd} = \sum_i^N \int p_t(\tilde{y}_i) \log p(\tilde{y}_i \mid y) d \tilde{y}
$$

onde $\operatorname{elpd}$ é esperança do log da densidade preditiva pontual (*expected log pointwise predictive density*) e $p_t(\tilde{y}_i)$ é a distribuição representando o verdadeiro processo generativo dos dados para $\tilde{y}_i$. Os $p_t(\tilde{y}_i)$ são desconhecidos e geralmente usamos validação cruzada ou WAIC para aproximar a estimação de $\operatoname{elpd}$.

### *Leave-one-out Cross-Validation* (LOO)

Podemos calcular o $\operatorname{elpd}$ usando LOO:

$$
\operatorname{elpd}^{\text{loo}} = sum_i^N \log p(y_i \mid y_{-i})
$$

onde

$$
p(y_i \mid y_{-i}) = \int p(y_i \mid \theta) p(\theta \mid y_{-i}) d \theta
$$

que é a densidade preditiva com uma observação a menos condicionada nos dados sem a observação $i$ ($i_{-i}$).

### *Widely Applicable Information Criteria*^[também chamado às vezes de *Watanabe-Akaike Information Criteria*.] (WAIC)

WAIC [@watanabe2010asymptotic] também é uma abordagem alternativa para calcularmos o $\operatorname{elpd}$ e é definida como:

$$
\widehat{\operatorname{elpd}}_{\text{waic}} = \widehat{\operatorname{lpd}} - \widehat{p}_{\text{waic}}
$$

onde $\widehat{\operatorname{lpd}}$ é a estimação do do log da densidade preditiva pontual (*log pointwise predictive density*):

$$
\widehat{\operatorname{lpd}} = \sum_i^N \log p(y_i \mid y) = sum_i^N \log \int p(y_i \mid \theta) p(\theta \mid y) d \theta
$$

e $\widehat{p}_{\text{waic}}$ é o número estimado efetivo de paramêtros e calculado com base em:

$$
\widehat{p}_{\text{waic}} = \sum_i^N \operatorname{var}_{\text{post}} (\log p(y_i \mid \theta))
$$

que conseguimos calcular using a variância posterior do log da densidade preditiva para cada observação $y_i$:

$$
\widehat{p}_{\text{waic}} = \sum_i^N V^S_{s=1} (\log p(y_i \mid \theta^s))
$$

onde $V^S_{s=1}$ representa a variância da amostra:

$$
V^S_{s=1} a_s = \frac{1}{S-1} \sum^S_{s=1} (a_s - \bar{a})^2
$$

### *$K$-fold Cross-Validation*

Da mesma maneira que conseguimos cacular $\operatorname{elpd}$ usando LOO com $N-1$ partições do *dataset* podemos também calcular com qualquer número de partições que quisermos. Tal abordagem é chamada de validação cruzada usando $K$ partições (*$K$-fold Cross-Validation*). Ao contrário de WAIC e LOO, *$K$-fold Cross-Validation* não conseguimos aproximar $\operatorname{elpd}$ e precisamos fazer a computação atual do $\operatorname{elpd}$ que quase sempre envolve um alto custo computacional.

## Implementações em Stan, `rstanarm` e `brms`

Stan e suas interfaces (`rstanarm` e `brms`) conseguem com o pacote `loo` [@loo]

Relembrar o exemplo da Aula 8. função `loo()` `waic()` e `kfold()` padrão `K = 10`.

E `loo_compare(x, ..., criterion = c("loo", "kfold", "waic"), detail = FALSE)`

## Ambiente

```{r SessionInfo}
sessionInfo()
```

{
  "articles": [
    {
      "path": "0-Estatistica-Bayesiana.html",
      "title": "O que √© Estat√≠stica Bayesiana?",
      "description": "No√ß√µes de Probabilidade, Estat√≠stica Frequentista versus Estat√≠stica Bayesiana",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2020",
      "contents": "\n\nContents\nO que √© probabilidade?\nDefini√ß√£o Matem√°tica\nProbabilidade Condicional\nProbabilidade Conjunta\nTeorema de Bayes\n\nEstat√≠stica Bayesiana\nEstat√≠stica Frequentista\nEstat√≠stica Bayesiana vs Frequentista\nExemplo Pr√°tico\n\nO que √© o maldito \\(p\\)-valor?\nTeorema de Bayes\nVantagens da Estat√≠stica Bayesiana\nDesvantagens\nStan\nAmbiente\n\n\nA estat√≠stica Bayesiana1 √© uma abordagem de an√°lise de dados baseada no teorema de Bayes, onde o conhecimento dispon√≠vel sobre os par√¢metros em um modelo estat√≠stico √© atualizado com as informa√ß√µes dos dados observados (Gelman et al., 2013). O conhecimento pr√©vio √© expresso como uma distribui√ß√£o a priori2 e combinado com os dados observados na forma de uma fun√ß√£o de verossimilhan√ßa3 para determinar a distribui√ß√£o a posteriori4 . A posteriori tamb√©m pode ser usada para fazer previs√µes sobre eventos futuros.\nEstat√≠stica Bayesiana est√° revolucionando todos os campos das ci√™ncias baseadas em evid√™ncias5 (van de Schoot et al., 2021). A insatisfa√ß√£o com m√©todos tradicionais de infer√™ncia estat√≠stica (estat√≠stica frequentista) e o advento dos computadores com o crescimento exponencial de poder computacional6 proporcionaram a ascens√£o da estat√≠stica Bayesiana por ser uma abordagem robusta √† m√°s-pr√°ticas cient√≠ficas por√©m computacionalmente intensiva.\nPor√©m antes de entrarmos em estat√≠stica Bayesiana, temos que falar de probabilidade: o motor da infer√™ncia Bayesiana.\nO que √© probabilidade?\n\nProbabilidade n√£o existe!\nde Finetti (1974)7\n\nEssas s√£o as primeiras palavras no pref√°cio do c√©lebre livro de Bruno de Finetti (figura 1), um dos mais importantes matem√°ticos e fil√≥sofos da probabilidade. Sim, a probabilidade n√£o existe. Ou melhor, probabilidade como uma quantidade f√≠sica, chance objetiva, N√ÉO existe. De Finetti mostrou que, em certo sentido preciso, se dispensarmos a quest√£o da chance objetiva nada se perde. A matem√°tica do racioc√≠nio indutivo permanece exatamente a mesma.\n\n\n\nFigure 1: Bruno de Finetti. Figura de https://www.wikipedia.org\n\n\n\nConsidere jogar uma moeda de enviesada. As tentativas s√£o consideradas independentes e, como resultado, exibem outra propriedade importante. A ordem n√£o importa. Dizer que a ordem n√£o importa √© dizer que se voc√™ pegar qualquer sequ√™ncia finita de cara e coroa e permutar os resultados da maneira que quiser, a sequ√™ncia resultante ter√° a mesma probabilidade. Dizemos que essa probabilidade √© invariante sob permuta√ß√µes.\nOu, dito de outra forma, a √∫nica coisa que importa √© a frequ√™ncia relativa. As sequ√™ncias de resultados que t√™m as mesmas frequ√™ncias de cara e coroa consequentemente possuem a mesma probabilidade. A frequ√™ncia √© considerada uma estat√≠stica suficiente8. Dizer que a ordem n√£o importa ou dizer que a √∫nica coisa que importa √© a frequ√™ncia s√£o duas maneiras de dizer exatamente a mesma coisa. Essa propriedade √© chamada de permutabilidade por de Finetti. E √© a mais importantes propriedade da probabilidade que faz com que possamos manipul√°-la matematicamente (ou filosoficamente) mesmo que ela n√£o exista como uma ‚Äúcoisa‚Äù f√≠sica.\nAinda desenvolvendo o argumento: ‚ÄúO racioc√≠nio probabil√≠stico ‚Äì-sempre entendido como subjetivo-‚Äì decorre apenas da incerteza de algo. N√£o faz diferen√ßa se a incerteza diz respeito a um futuro imprevis√≠vel9, ou a um passado despercebido, ou a um passado duvidosamente relatado ou esquecido10‚Ä¶ A √∫nica coisa relevante √© a incerteza ‚Äì a extens√£o de nosso pr√≥prio conhecimento e ignor√¢ncia. O fato real de se os eventos considerados s√£o ou n√£o determinados em algum sentido, ou conhecidos por outras pessoas, e assim por diante, √© irrelevante‚Äù (tradu√ß√£o minha de de Finetti (1974)).\nConcluindo: n√£o importa o que √© probabilidade, voc√™ consegue us√°-la de qualquer maneira, mesmo que ela seja um frequ√™ncia absoluta (ex: probabilidade de eu plantar bananeira de sunga na Avenida Paulista √© ZERO pois a probabilidade de um evento que nunca ocorreu ocorrer no futuro √© ZERO) ou um palpite subjetivo (ex: talvez a probabilidade n√£o seja ZERO, mas 0,00000000000001; bem improv√°vel, mas n√£o imposs√≠vel).\nDefini√ß√£o Matem√°tica\nCom a intui√ß√£o filos√≥fica de probabilidade elaborada, vamos √†s intui√ß√µes matem√°ticas. A probabilidade de um evento √© um n√∫mero real11 entre 0 e 1, onde, grosso modo, 0 indica a impossibilidade do evento e 1 indica a certeza. Quanto maior a probabilidade de um evento, mais prov√°vel √© que o evento ocorrer√°. Um exemplo simples √© o lan√ßamento de uma moeda justa (imparcial). Como a moeda √© justa, os dois resultados (‚Äúcara‚Äù e ‚Äúcoroa‚Äù) s√£o igualmente prov√°veis; a probabilidade de ‚Äúcara‚Äù √© igual √† probabilidade de ‚Äúcoroa‚Äù; e uma vez que nenhum outro resultado √© poss√≠vel, a probabilidade de ‚Äúcara‚Äù ou ‚Äúcoroa‚Äù √© 1/2 (que tamb√©m pode ser escrita como 0,5 ou 50%).\nSobre nota√ß√£o, definimos que \\(A\\) √© um evento e \\(P(A)\\) a probabilidade do evento, logo:\n\\[\n\\{P(A) \\in \\mathbb{R} : 0 \\geq P(A) \\geq 1 \\}.\n\\]\nIsto quer dizer o ‚Äúprobabilidade do evento A ocorrer √© o conjunto de todos os numeros reais entre 0 e 1; incluindo 0 e 1.‚Äù Al√©m disso temos tr√™s axiomas12, oriundos de Kolmogorov (1933) (figura 2):\nN√£o-negatividade: Para todo \\(A\\), \\(P(A) \\geq 0\\). Toda probabilidade √© positiva (maior ou igual a zero), independente do evento.\nAditividade: Para dois mutuamente exclusivos \\(A\\) e \\(B\\) (n√£o podem ocorrer ao mesmo tempo):13 \\(P(A) = 1 - P(B)\\) e \\(P(B) = 1 - P(A)\\).\nNormaliza√ß√£o: A probabilidade de todos os eventos poss√≠veis \\(A_1, A_2, \\dots\\) devem somar 1: \\(\\sum_{n \\in mathbb{N}} A_n = 1\\).\n\n\n\nFigure 2: Andrey Nikolaevich Kolmogorov. Figura de https://www.wikipedia.org\n\n\n\nCom esses tr√™s simples (e intuitivos) axiomas, conseguimos derivar e construir toda a matem√°tica da probabilidade.\nProbabilidade Condicional\nUm conceito importante √© a probabilidade condicional que podemos definir como ‚Äúprobabilidade de um evento ocorrer caso outro tenha ocorrido ou n√£o.‚Äù A nota√ß√£o que usamos √© \\(P( A \\mid B )\\), que l√™-se como ‚Äúa probabilidade de observamos \\(A\\) dado que j√° observamos \\(B\\).‚Äù\nUm bom exemplo √© o jogo de Poker Texas Hold‚Äôem, onde o jogador recebe duas cartas e podem utilizar mais cinco cartas comunit√°rias para montar sua ‚Äúm√£o.‚Äù A probabilidade de voc√™ receber um Rei üÇÆ (\\(K\\)) √© \\(\\frac{4}{52}\\):\n\\[\nP(K) = \\left(\\frac{4}{52}\\right) = \\left(\\frac{1}{13}\\right).\n\\]\nE a probabilidade de voc√™ receber um √Ås üÇ° (\\(A\\)) tamb√©m √© \\(\\frac{4}{52}\\):\n\\[\nP(A) = \\left(\\frac{4}{52}\\right) = \\left(\\frac{1}{13}\\right).\n\\]\nPor√©m a probabilidade de voc√™ receber um Rei üÇÆ como segunda carta dado que voc√™ recebeu um √Ås üÇ° como primeira carta √©:\n\\[\nP(K \\mid A) = \\left(\\frac{4}{51}\\right).\n\\]\nComo temos uma carta a menos \\(51\\) j√° que voc√™ recebeu o √Ås üÇ° (visto que \\(A\\) foi observado), temos 4 Reis ainda no baralho üÇÆ üÇæ üÉé üÉû, logo \\(\\frac{4}{51}\\).\nProbabilidade Conjunta\nProbabilidade condicional nos leva √† um outro conceito importante: probabilidade conjunta. Probabilidade conjunta √© a ‚Äúprobabilidade de observados dois eventos ocorrem.‚Äù Continuando no nosso exemplo do Poker, a probabilidade de voc√™ receber como duas cartas iniciais um √Ås üÇ° (\\(A\\)) e um Rei üÇÆ (\\(K\\)) √©:\n\\[\n\\begin{aligned}\nP(A,K) &= P(A) \\cdot P(K \\mid A) \\\\\n&= P \\left(\\frac{1}{13}\\right) \\cdot P \\left(\\frac{4}{51}\\right)\\\\\n&= P \\left(\\frac{4}{51 \\cdot 13}\\right) \\\\\n&\\approx 0.006.\n\\end{aligned}\n\\]\nNote que \\(P(A,K) = P(K,A)\\):\n\\[\n\\begin{aligned}\nP(K,A) = P(K) \\cdot P(A \\mid K) \\\\\n&= P \\left(\\frac{1}{13}\\right) \\cdot P \\left(\\frac{4}{51}\\right)\\\\\n&= P \\left(\\frac{4}{51 \\cdot 13}\\right) \\\\\n&\\approx 0.006.\n\\end{aligned}\n\\]\nNo nosso exemplo de Poker temos uma certa simetria:\n\\[\nP(K \\mid A) = P(A \\mid K).\n\\]\nMas sem sempre essa simetria existe (Na verdade muito raramente ela existe). A identidade que temos √© a seguinte:\n\\[\nP(A) \\cdot P(K \\mid A) = P(K) \\cdot P(A \\mid K).\n\\]\nEnt√£o essa simetria s√≥ existe quando as taxas basais dos eventos condicionais s√£o iguais:\n\\[\nP(A) = P(K).\n\\]\nQue √© o que ocorre no nosso exemplo.\nProbabilidade Condicional n√£o √© ‚Äúcomutativa‚Äù\n\\[P(A \\mid B) \\neq P(B \\mid A)\\]\nVeja um exemplo pr√°tico. Digamos que eu estou me sentindo bem e come√ßo a tossir na fila do mercado. O que voc√™ acha que ir√° acontecer? Todo mundo vai achar que estou com COVID, o que √© equivalente √† pensar em \\(P(\\text{tosse} \\mid \\text{covid})\\). Vendo os sintomas mais comuns do COVID, caso voc√™ esteja com COVID, a chance de voc√™ tossir √© muito alta. Mas na verdade tossimos muito mais frequentemente que temos COVID (\\(P(\\text{tosse}) \\neq P(\\text{COVID})\\)), logo:\n\\[\nP(\\text{COVID} \\mid \\text{tosse}) \\neq P(\\text{tosse} \\mid \\text{COVID}).\n\\]\nTeorema de Bayes\nEste √© o ultimo conceito de probabilidade que precisamos abordar antes de mergulhar na estat√≠stica Bayesiana14, mas √© o mais importante. Note que n√£o √© coincid√™ncia sem√¢ntica que estat√≠stica Bayesiana e teorema de Bayes possuem o mesmo prefixo.\nThomas Bayes (1701 - 1761, figura 3) foi um estat√≠stico, fil√≥sofo e ministro presbiteriano ingl√™s conhecido por formular um caso espec√≠fico do teorema que leva seu nome: o teorema de Bayes. Bayes nunca publicou o que se tornaria sua realiza√ß√£o mais famosa; suas notas foram editadas e publicadas ap√≥s sua morte pelo seu amigo Richard Price15. Em seus √∫ltimos anos, Bayes se interessou profundamente por probabilidade. Alguns especulam que ele foi motivado a refutar o argumento de David Hume contra a cren√ßa em milagres com base nas evid√™ncias do testemunho em ‚ÄúAn Inquiry Concerning Human Understanding.‚Äù\n\n\n\nFigure 3: Thomas Bayes. Figura de https://www.wikipedia.org\n\n\n\nVamos logo para o Teorema. Lembra que temos a seguinte identidade na probabilidade:\n\\[\n\\begin{aligned}\nP(A,B) &= P(B,A) \\\\\nP(A) \\cdot P(B \\mid A) &= P(B) \\cdot P(A \\mid B).\n\\end{aligned}\n\\]\nPois bem, agora passe o \\(P(B)\\) do lado direito para o lado esquerdo dividindo:\n\\[\n\\begin{aligned}\nP(A) \\cdot P(B \\mid A) &= \\overbrace{P(B)}^{\\text{isso vai para $\\leftarrow$}} \\cdot P(A \\mid B) \\\\\n\\frac{P(A) \\cdot P(B \\mid A)}{P(B)} &= P(A \\mid B) \\\\\nP(A \\mid B) &= \\frac{P(A) \\cdot P(B \\mid A)}{P(B)}.\n\\end{aligned}\n\\]\nE esse √© o resultado final:\n\\[\nP(A \\mid B) = \\frac{P(A) \\cdot P(B \\mid A)}{P(B)}.\n\\]\nA estat√≠stica Bayesiana usa esse teorema como motor de infer√™ncia dos par√¢metros de um modelo condicionado aos dados observados.\nEstat√≠stica Bayesiana\nAgora que voc√™ j√° sabe o que √© probabilidade e o que √© o teorema de Bayes, vou propor o seguinte modelo:\n\\[\n\\underbrace{P(\\theta \\mid y)}_{\\textit{Posteriori}} = \\frac{\\overbrace{P(y \\mid  \\theta)}^{\\text{Verossimilhan√ßa}} \\cdot \\overbrace{P(\\theta)}^{\\textit{Priori}}}{\\underbrace{P(y)}_{\\text{Constante Normalizadora}}},\n\\]\nonde:\n\\(\\theta\\) ‚Äì par√¢metro(s) de interesses\n\\(y\\) ‚Äì dados observados\nPriori ‚Äì probabilidade pr√©via do valor do(s) par√¢metro(s)16\nVerossimilhan√ßa ‚Äì probabilidade dos dados observados condicionados aos valores do(s) par√¢metro(s)\nPosteriori ‚Äì probabilidade posterior do valor do(s) par√¢metros ap√≥s observamos os dados \\(y\\)\nConstante Normalizadora ‚Äì \\(P(y)\\) n√£o faz sentido intuitivo. Essa probabilidade √© transformada e pode ser interepretada como algo que existe apenas para que o resultado de \\(P(y \\mid \\theta) P(\\theta)\\) seja algo entre 0 e 1 ‚Äì uma probabilidade v√°lida.\nA estat√≠sica Bayesiana nos permite quantificar diretamente a incerteza relacionada ao valor de um ou mais par√¢metros do nosso modelo condicionado ao dados observados. Isso √© a caracter√≠stica principal da estat√≠stica Bayesiana. Pois estamos estimando diretamente \\(P(\\theta \\mid y)\\) por meio do teorema de Bayes. A estimativa resultante √© totalmente intuitiva: simplesmente quantifica a intercerteza que temos sobre o valor de um ou mais par√¢metro condicionado nos dados, nos pressupostos do nosso modelo (verossimilhan√ßa) e na probabilidade pr√©via que temos sobre tais valores.\nEstat√≠stica Frequentista\nPara contrastar com a estat√≠stica Bayesiana, vamos ver como a estat√≠stica cl√°ssica frequentista17. E j√° aviso, n√£o √© algo intuitivo que nem a estat√≠stica Bayesiana.\nPara a estat√≠stica frequentista o pesquisador est√° proibido de fazer conjecturas probabil√≠sticas sobre par√¢metros. Pois eles n√£o s√£o incertos, muito pelo contr√°rio √© uma quantidade determinada. A √∫nica quest√£o √© que n√£o observamos diretamente os par√¢metros, mas eles s√£o determin√≠sticos e n√£o permitem qualquer margem de incerteza. Logo, para a abordagem frequentista, par√¢metros s√£o quantidades de interesse n√£o observadas na qual n√£o fazemos conjecturas probabil√≠sticas.\nO que √© ent√£o incerto na estat√≠stica frequentista? Resposta curta: os dados observados. Para a abordagem frequentista a sua amostra √© incerta. √â sobre ela que voc√™ pode fazer conjecturas probabil√≠sticas. Portanto, a incerteza √© expressa na probabilidade de eu obter dados similares aos que eu obtive se eu amostrasse de uma popula√ß√£o de interesse infinitas amostras do mesmo tamanho que a minha amostra18. A incerteza √© condicionada √† uma abordagem frequentista, em outras palavras, a incerteza s√≥ existe se eu considerar um processo de amostragem infinito e extrair desse processo uma frequ√™ncia. A probabilidade s√≥ existe se representar uma frequ√™ncia. Mesmo se isso ocasionar em um ‚Äúprocesso de amostragem infinito de uma popula√ß√£o que eu nunca observei,‚Äù por mais estranho que isso soe19.\nPara a abordagem frequentista n√£o existe probabilidade posteriori nem priori pois ambas envolvem par√¢metros, e vimos que isso √© proibido em solo frequentista. Tudo o que √© necess√°rio para a infer√™ncia estat√≠stica est√° contida na verossimilhan√ßa. Al√©m disso, por raz√µes de facilidade de computa√ß√£o, pois boa parte desses m√©todos foram inventados na primeira m√©tade do s√©culo XX (sem a ajuda do computador), apenas √© computado o valor dos par√¢metros que maximizam a fun√ß√£o da verossimilhan√ßa20. Desse processo de otimiza√ß√£o extra√≠mos a moda da verossimilhan√ßa. A moda funciona perfeitamente no mundo de conto de fadas que se pressup√µe que tudo segue uma distribui√ß√£o normal, pois a moda √© igual a mediana e a m√©dia ‚Äì \\(\\text{m√©dia} = \\text{mediana} \\text{moda}\\). S√≥ tem um problema, raramente esse pressuposto √© verdadeiro (figura 4), ainda mais quando falamos de par√¢metros num contexto de pluralidade de par√¢metros e rela√ß√µes complexas entre par√¢metros (modelos complexos).\n\n\n\nFigure 4: Pressupostos vs Realidade. Figura de Katherine Hoffman. Reprodu√ß√£o Autorizada.\n\n\n\nVale aqui uma breve explica√ß√£o sociol√≥gica e computacional porque a estat√≠stica cl√°ssica pro√≠be conjecturas probabil√≠sticas sobre par√¢metros e trabalhamos com otimiza√ß√£o (achar o valor m√°ximo de uma fun√ß√£o) do que aproxima√ß√£o ou estima√ß√£o da densidade completa da verossimilhan√ßa (em outras palavras, ‚Äúlevantar a capivara toda‚Äù da verossimilhan√ßa ao inv√©s de somente a moda).\nSobre a quest√£o sociol√≥gica, a ci√™ncia no come√ßo do s√©culo XX partia do princ√≠pio que ela √© objetiva e toda subjetividade deve ser banida. Logo, como a estima√ß√£o da probabilidade a posteriori de par√¢metros envolve a elucida√ß√£o de uma probabilidade a priori de par√¢metros, tal m√©todo n√£o deve ser permitido na ci√™ncia, pois traz subjetividade (sabemos hoje que nada no comportamento humano √© puramente objetivo, e a subjetividade impregna todas as empreitadas humanas).\nSobre a quest√£o computacional, na d√©cada de 1930s sem computadores era muito mais f√°cil usar pressupostos fortes sobre os dados para conseguir uma resposta de uma estima√ß√£o estat√≠stica usando deriva√ß√µes matem√°ticas do que calcular na m√£o a estima√ß√£o estat√≠stica sem depender de tais pressupostos. Por exemplo: o famoso teste \\(t\\) de Student √© um teste que diz quando conseguimos rejeitar que a m√©dia de um certo par√¢metro de interesse entre dois grupos √© igual (famosa hip√≥tese nula ‚Äì \\(H_0\\)). Esse teste parte do pressuposto que se o par√¢metro de interesse for distribu√≠do conforme uma distribui√ß√£o normal (pressuposto 1 ‚Äì normalidade da vari√°vel dependente), se a vari√¢ncia do par√¢metro de interesse varia de maneira homog√™nea dentre os grupos (pressuposto 2 ‚Äì homogeneidade das vari√¢ncias) e se o n√∫mero de observa√ß√µes nos dois grupos de interesse √© similar (pressuposto 3 ‚Äì homogeneidade do tamanho dos grupos) a diferen√ßa entre os grupos ponderada pela vari√¢nca dos grupos segue uma distribui√ß√£o \\(t\\) de Student (por isso o nome do teste). Ent√£o a estima√ß√£o estat√≠stica se resume a calcular a m√©dia de dois grupos, a vari√¢ncia de cada um deles para um par√¢metro de interesse e buscar o tal do \\(p\\)-valor numa tabela e ver se conseguimos rejeitar a \\(H_0\\). Isto √© v√°lido quando tudo o que fazemos √© calculado na m√£o, hoje com um computador 1 milh√£o de vezes mais potente que o computador da Apollo 11 (levou a humanidade √† lua) no seu bolso, n√£o sei se ainda √© valido.\nPara concluir, vamos falar sobre os famosos intervalos de confian√ßa, que n√£o s√£o uma medida que quantifica a incerteza do valor de um par√¢metro (lembre-se conjecturas probabil√≠sticas sobre par√¢metros s√£o proibidos em frequentist-land). Segure seu queixo, intervalos de confian√ßa s√£o:\n\nUm intervalo de confian√ßa de X% para um par√¢metro √© um intervalo \\((a, b)\\) gerado por um procedimento que em amostragem repetida tem uma probabilidade de X% de conter o valor verdadeiro do par√¢metro, para todos os valores poss√≠veis do par√¢metro\nNeyman (1937) (o ‚Äúpai‚Äù dos intervalos de confian√ßa)\n\nMais uma vez a ideia da amostragem repetida infinita vezes de uma popula√ß√£o que voc√™ nunca viu. Por exemplo: digamos que voc√™ executou uma an√°lise estat√≠stica para comparar efic√°cia de uma pol√≠tica p√∫blica em dois grupos e voc√™ obteve a diferen√ßa entre a m√©dia desses grupos. Voc√™ pode expressar essa diferen√ßa como um intervalo de confian√ßa. Geralmente escolhemos a confian√ßa de 95%. Voc√™ ent√£o escreve no seu artigo que a ‚Äúdiferen√ßa entre grupos observada √© de 10.5 - 23.5 (95% IC).‚Äù Isso quer dizer que 95 estudos de 100, que usem o mesmo tamanho de amostra e popula√ß√£o-alvo, aplicando o mesmo teste estat√≠stico, esperar√£o encontrar um resultado de diferen√ßas de m√©dia entre grupos entre 10.5 e 23.5. Aqui as unidades s√£o arbitr√°rias, mas para continuar o exemplo vamos supor que sejam expectativa de vida.\nInfelizmente com estat√≠stica frequentista voc√™ tem que escolher uma das duas qualidades para explica√ß√µes: intuitiva ou precisa21.\nEstat√≠stica Bayesiana vs Frequentista\nO que discutimos acima de resume nessa tabela abaixo:\n\nEstat√≠stica Bayesiana\nEstat√≠stica Frequentista\nDados\nFixos ‚Äì N√£o Aleat√≥rios\nIncertos ‚Äì Aleat√≥rios\nPar√¢metros\nIncertos ‚Äì Aleatorios\nFixos ‚Äì N√£o Aleat√≥rios\nInfer√™ncia\nIncerteza sobre o valor do par√¢metro\nIncerteza sobre um processo de amostragem de uma popula√ß√£o infinita\nProbabilidade\nSubjetiva\nObjetiva (mas com diversos pressupostos dos modelos)\nExemplo Pr√°tico\nImagine que voc√™ est√° avaliando um jogador de basquete. Voc√™ precisa decidir se ir√° contrat√°-lo para o seu time. A principal caracter√≠stica que voc√™ examinar√° ser√° a taxa de acerto de cestas de 3 pontos. Esse √© o nosso par√¢metro de interesse e a partir de agora vamos cham√°-lo de \\(\\theta\\) (letra grega22 theta). \\(\\theta\\) pode assumir qualquer valor entre 0 e 1, sendo 0 representando uma taxa de acerto de 0% do jogador‚Äîele sempre erra as tentativas de cestas de 3 pontos; e 1 representando uma taxa de acerto 100% do jogador‚Äî ele sempre acerta as tentativas de cestas de 3 pontos.\n√â claro que \\(\\theta\\) raramente ser√° 0 ou 1, mas sim um valor entre esses dois extremos. Podemos representar \\(\\theta\\) com uma distribui√ß√£o beta. A distribui√ß√£o beta √© especificada por dois par√¢metros com valores sempre positivos (\\(\\geq 0\\)): \\(\\alpha\\)23 e \\(\\beta\\)24. Voc√™ pode pensar em \\(\\alpha - 1\\) como o n√∫mero de acertos e \\(\\beta - 1\\) como o n√∫mero de erros. Na figura 5 √© poss√≠vel ver uma distribui√ß√£o beta para v√°rios valores iguais de par√¢metros \\(\\alpha\\) e \\(\\beta\\). Veja que conforme o valor de \\(\\alpha\\) e \\(\\beta\\) aumentam a probabilidade de \\(\\theta\\) tende a convergir para o valor de 0.5 (50%).\n\n\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n\nggplot(data = data.frame(x = c(0, 1))) +\n  labs(\n    title = \"Comparativo de Distribui√ß√µes Beta\",\n    subtitle = expression(alpha == beta),\n    x = expression(theta),\n    y = \"Densidade\",\n    color = \"Par√¢metros\"\n  ) +\n  stat_function(aes(color = \"list(alpha, beta) ==  1\"), fun = dbeta, args = list(\n    shape1 = 1, shape2 = 1), size = 2) +\n  stat_function(aes(color = \"list(alpha, beta) ==  2\"), fun = dbeta, args = list(\n    shape1 = 2, shape2 = 2), size = 2) +\n  stat_function(aes(color = \"list(alpha, beta) ==  3\"), fun = dbeta, args = list(\n    shape1 = 3, shape2 = 3), size = 2) +\n    stat_function(aes(color = \"list(alpha, beta) ==  4\"), fun = dbeta, args = list(\n    shape1 = 4, shape2 = 4), size = 2) +\n  scale_color_brewer(palette = \"Set1\",\n    labels = scales::label_parse())\n\n\n\n\nFigure 5: Comparativo de Distribui√ß√µes Beta ‚Äì Valores Iguais de \\(\\alpha\\) e \\(\\beta\\)\n\n\n\nConforme os valores de \\(\\alpha\\) e \\(\\beta\\) diferem um do outro come√ßamos a ver a probabilidade de \\(\\theta\\) se distanciar de 0.5 e a distribui√ß√£o come√ßa a ser assim√©trica, tendenciando para algum extremo. Na figura 6 √© poss√≠vel ver uma distribui√ß√£o beta para v√°rios valores diferentes de par√¢metros \\(\\alpha\\) e \\(\\beta\\).\n\n\nlibrary(ggplot2)\n\nggplot(data = data.frame(x = c(0, 1))) +\n  labs(\n    title = \"Comparativo de Distribui√ß√µes Beta\",\n    subtitle = expression(alpha != beta),\n    x = expression(theta),\n    y = \"Densidade\",\n    color = \"Par√¢metros\"\n  ) +\n  stat_function(aes(color = \"list(alpha == 3, beta == 2)\"), fun = dbeta, args = list(\n    shape1 = 3, shape2 = 2), size = 2) +\n  stat_function(aes(color = \"list(alpha == 2, beta ==  3)\"), fun = dbeta, args = list(\n    shape1 = 2, shape2 = 3), size = 2) +\n  stat_function(aes(color = \"list(alpha == 4, beta ==  2)\"), fun = dbeta, args = list(\n    shape1 = 4, shape2 = 2), size = 2) +\n    stat_function(aes(color = \"list(alpha == 2, beta ==  4)\"), fun = dbeta, args = list(\n    shape1 = 2, shape2 = 4), size = 2) +\n  scale_color_brewer(palette = \"Set1\",\n    labels = scales::label_parse())\n\n\n\n\nFigure 6: Comparativo de Distribui√ß√µes Beta ‚Äì Valores Diferentes de \\(\\alpha\\) e \\(\\beta\\)\n\n\n\nO que √© o maldito \\(p\\)-valor?\nTeste t: Ôøº\\(P(D \\mid \\text{efeito nulo})\\) ANOVA: Ôøº\\(P(D \\mid \\text{n√£o h√° diferen√ßa entre os grupos})\\) Regress√£o: Ôøº\\(P(D \\mid \\text{coeficiente √© nulo})\\) Shapiro-Wilk: \\(P(D \\mid \\text{amostra √© normal})\\)\nMas o que estamos realmente interessados √© na \\(P(H_0)\\)\nTeorema de Bayes\nRanca fora a constante Normalizadora\n\\[\\underbrace{P(\\theta \\mid y)}_{\\text{Posterior}} \\propto \\overbrace{P(y \\mid \\theta)}^{\\text{Likelihood}} \\cdot \\overbrace{P(\\theta)}^{\\text{Prior}} = \\underbrace{P(\\theta, y)}_{\\text{Probabilidade Conjunta}}\\]\n\\(\\propto\\) (comando \\(\\LaTeX\\) \\propto) quer dizer ‚Äúproporcional √†.‚Äù\nAnima√ß√£o com uma distribui√ß√£o beta de um flip of a coin com updated beliefs by posterior\nVantagens da Estat√≠stica Bayesiana\nAbordagem Natural para expressar incerteza\nHabilidade de incorporar informa√ß√µes pr√©via\nMaior flexibilidade do modelo\nDistribui√ß√£o posterior completa dos par√¢metros\nIntervalos de Confian√ßa vs Intervalos de Credibilidade\nPoint Estimate vs Full Posterior Density\nMesmo com Intervalos de Confian√ßa voc√™ est√° falando ainda de Point Estimate ‚Äì Optimization of Likelihood when the derivative is at zero\nMLE estimation is the value of the parameters such that the most likely dataset of size N to randomly draw from a population is the dataset that you actually drew. Every other potential dataset that could be drawn from this population is going to fit worse than the dataset that you actually have.\n\nPropaga√ß√£o natural da incerteza\nFisher ‚Äî ‚Äúwere Fisher alive today, he would be a Bayesian‚Äù\nFisher published an article (Fisher, 1962) examining the possibilities of Bayesian methods, but with the prior probabilities to be determined experimentally!\nFisher, R. A. (1962), ‚ÄòSome examples of Bayes‚Äô method of the experimental determination of probability a priori‚Äô, J. Roy. Stat. Soc. B 24, 118‚Äì124.\n\nModelos hier√°rquicos. lme4 n√£o computa p-valores para os random effects\nExemplo do teste t normal e o bayesiano\nDesvantagens\nVelocidade lenta de estimativa do modelo (30 segundos ao inv√©s de 3 segundos)\nFalar do poder computacional ‚Äî flops Falar da facilidade de testes ortodoxos de computa√ß√£o\nStan\n\n\nknitr::include_graphics(\"images/stan_billions_subtitled.mp4\")\n\n\n\n\nrstan\nrstanarm\nbrms\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.4 (2021-02-15)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] ggplot2_3.3.3\n\nloaded via a namespace (and not attached):\n [1] RColorBrewer_1.1-2 bslib_0.2.4        compiler_4.0.4    \n [4] pillar_1.5.0       jquerylib_0.1.3    highr_0.8         \n [7] tools_4.0.4        digest_0.6.27      downlit_0.2.1     \n[10] jsonlite_1.7.2     evaluate_0.14      lifecycle_1.0.0   \n[13] tibble_3.1.0       gtable_0.3.0       pkgconfig_2.0.3   \n[16] rlang_0.4.10       DBI_1.1.1          distill_1.2       \n[19] yaml_2.2.1         parallel_4.0.4     xfun_0.21         \n[22] withr_2.4.1        dplyr_1.0.4        stringr_1.4.0     \n[25] knitr_1.31         generics_0.1.0     vctrs_0.3.6       \n[28] sass_0.3.1         systemfonts_1.0.1  tidyselect_1.1.0  \n[31] grid_4.0.4         glue_1.4.2         R6_2.5.0          \n[34] textshaping_0.3.1  jpeg_0.1-8.1       fansi_0.4.2       \n[37] rmarkdown_2.7      farver_2.1.0       purrr_0.3.4       \n[40] magrittr_2.0.1     scales_1.1.1       htmltools_0.5.1.1 \n[43] ellipsis_0.3.1     assertthat_0.2.1   colorspace_2.0-0  \n[46] labeling_0.4.2     ragg_1.1.1         utf8_1.1.4        \n[49] stringi_1.5.3      munsell_0.5.0      crayon_1.4.1      \n\n\n\n\nde Finetti, B. (1974). Theory of Probability (Volume 1). New York: John Wiley & Sons.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis. Chapman and Hall/CRC.\n\n\nKolmogorov, A. N. (1933). Foundations of the Theory of Probability. Berlin: Julius Springer.\n\n\nNau, R. F. (2001). De Finetti was Right: Probability Does Not Exist. Theory and Decision, 51(2), 89‚Äì124. https://doi.org/10.1023/A:1015525808214\n\n\nNeyman, J. (1937). Outline of a theory of statistical estimation based on the classical theory of probability. Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences, 236(767), 333‚Äì380.\n\n\nvan de Schoot, R., Depaoli, S., King, R., Kramer, B., M√§rtens, K., Tadesse, M. G., ‚Ä¶ Yau, C. (2021). Bayesian statistics and modelling. Nature Reviews Methods Primers, 1(1, 1), 1‚Äì26. https://doi.org/10.1038/s43586-020-00001-2\n\n\nmai√∫sculo, pois se refere ao teorema de Bayes que √© um sobrenome.‚Ü©Ô∏é\ndo ingl√™s prior distribution.‚Ü©Ô∏é\ndo ingl√™s likelihood function.‚Ü©Ô∏é\ndo ingl√™s posterior distribution‚Ü©Ô∏é\npessoalmente, como um bom Popperiano, n√£o acreditado que haja ci√™ncia sem ser baseada em evid√™ncias; o que n√£o usa evid√™ncias pode ser considerado como filosofia (ex: matem√°tica) ou pr√°ticas sociais (n√£o menos ou mais importantes que a ci√™ncia, apenas uma demarca√ß√£o do que √© ci√™ncia e do que n√£o √©; ex: direito).‚Ü©Ô∏é\nseu smartphone (iPhone 12 - 4GB RAM) possui 1.000.000x (1 milh√£o) mais poder computacional que o computador de bordo da Apollo 11 (4kB RAM) que levou o homem √† lua. Detalhe: esse computador de bordo era respons√°vel pela navega√ß√£o, rota e controles do m√≥dulo lunar.‚Ü©Ô∏é\ncaso o leitor queira uma discuss√£o aprofundada veja Nau (2001).‚Ü©Ô∏é\ndo ingl√™s sufficient statistic.‚Ü©Ô∏é\nobserva√ß√£o minha: relacionado √† abordagem Bayesiana subjetiva.‚Ü©Ô∏é\nobserva√ß√£o minha: relacionado √† abordagem frequentista objetiva.‚Ü©Ô∏é\num n√∫mero que pode ser expressado como um ponto em uma linha cont√≠nua que se origina em menos infinito e termina e mais infinito \\((-\\infty, +\\infty)\\); para quem gosta de computa√ß√£o √© um ponto flutuante float ou double.‚Ü©Ô∏é\nna matem√°tica axiomas s√£o afirma√ß√µes pressupostas como verdadeiras que servem como premissas or pontos de partidas para elabora√ß√£o de argumentos e teoremas. Muitas vezes os axiomas s√£o question√°veis, por exemplo geometria n√£o-Euclidiana refuta o quinto axioma de Euclides sobre linhas paralelas. At√© agora n√£o h√° nenhum questionamento que tenha suportado o escrut√≠nio do tempo e da ci√™ncia sobre os tr√™s axiomas da probabilidade‚Ü©Ô∏é\npor exemplo, o resultado de uma moeda dado √© um dos 2 eventos mutualmente exclusivos: cara ou coroa.‚Ü©Ô∏é\npalavra de escoteiro.‚Ü©Ô∏é\no nome formal do teorema √© Bayes-Price-Laplace, pois Thomas Bayes foi o primeiro a descobrir, Richard Price pegou seus rascunhos, formalizou em nota√ß√£o matem√°tica e apresentou para a Royal Society of London, e Pierre Laplace redescobriu o teorema sem ter tido contato pr√©vio no final do s√©culo XVIII na Fran√ßa ao usar probabilidade para infer√™ncia estat√≠stica com dados do Censo na era Napole√¥nica.‚Ü©Ô∏é\nvou cobrir probabilidades pr√©vias ‚Äìpriori‚Äì no conte√∫do da aula 3 - Priors‚Ü©Ô∏é\ntamb√©m chamada de ortodoxa.‚Ü©Ô∏é\neu avisei que n√£o era intuitivo‚Ä¶‚Ü©Ô∏é\nseu ‚Äúsentido aranha‚Äù deve estar disparando agora‚Ä¶‚Ü©Ô∏é\npara os que gostam de matem√°tica, calculamos em qual ponto de \\(\\theta\\) a derivada da verossimilhan√ßa √© zero ‚Äì \\(\\mathcal{L}^\\prime = 0\\).‚Ü©Ô∏é\nisto foi copiado de Andrew Gelman ‚Äì Estat√≠stico Bayesiano.‚Ü©Ô∏é\nna estat√≠stica geralmente temos a conven√ß√£o de usar letras romanas (\\(a, b, c, d, \\dots\\)) para quantidades que sabemos o valor (exemplo: m√©dia de uma amostra); e letras gregas (\\(\\alpha, \\beta, \\gamma, \\dots\\)) para quantidades que n√£o sabemos o valor preciso e queremos estimar (exemplo: m√©dia de uma popula√ß√£o estimada a partir da m√©dia de uma amostra).‚Ü©Ô∏é\nletra grega alpha.‚Ü©Ô∏é\nletra grega beta.‚Ü©Ô∏é\n",
      "last_modified": "2021-03-03T19:06:09-03:00"
    },
    {
      "path": "1-Comandos_Basicos.html",
      "title": "Comandos B√°sicos de R",
      "description": "Introdu√ß√£o ao R e aos comandos b√°sicos do R",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nLendo Arquivos de Dados\nCSV\nExcel\n\nGr√°ficos\nAmbiente\n\n\nEste arquivo √© um documento R Markdown. Ele √© uma proposta de prosa com c√≥digo em R, al√©m de ser o formato preferido nosso de comunicar nossas an√°lises. Quando renderizamos o documento no formato desejado. Todo c√≥digo que √© inserido nele √© executado e as sa√≠das s√£o incorporadas no documento final. Isto vale para tabelas e gr√°ficos. Por exemplo, podemos pedir para o R imprimir algo com a fun√ß√£o print() e o resultado ser√° o c√≥digo que foi executado e o seu resultado.\n\n\nprint(\"Voc√™ executou um c√≥digo\")\n\n\n[1] \"Voc√™ executou um c√≥digo\"\n\nO formato R Markdown √© muito flex√≠vel. Podemos fazer relat√≥rios (em PDF, Word e HTML), apresenta√ß√µes (em PDF, PowerPoint e HTML), artigos acad√™micos, livros, websites1, blogs, CVs, etc.\n\nO site do autor foi feito usando a biblioteca {postcards}(Kross, 2021) de R. O CV tamb√©m foi feito em R usando a biblioteca {vitae} (O‚ÄôHara-Wild & Hyndman, 2021).\nLendo Arquivos de Dados\nCom o R conseguimos ler diversos tipo de arquivos de dados: CSV, texto, HTML, Excel, Stata, SPSS, Planilhas Google, Banco de Dados Relacionais, entre outros‚Ä¶ Vamos demonstrar como ler arquivos de dados dos dois formatos mais comuns: CSV e Excel.\nCSV\nPara ler um arquivo CSV (.csv) no R execute a fun√ß√£o read.csv() para arquivos CSV formato americano (v√≠rgula como separador e decimais como ponto) ou a fun√ß√£o read.csv2() para arquivos CSV formato europeu/brasileiro (ponto-e-v√≠rgula como separador e decimais como v√≠rgula). N√£o esque√ßa de designar a leitura para uma vari√°vel com o designador <-.\n\n\ndf <- read.csv2(\"datasets/mtcars.csv\", row.names = 1)\nhead(df)\n\n\n                  mpg cyl disp  hp drat  wt qsec vs am gear carb\nMazda RX4          21   6  160 110  3.9 2.6   16  0  1    4    4\nMazda RX4 Wag      21   6  160 110  3.9 2.9   17  0  1    4    4\nDatsun 710         23   4  108  93  3.9 2.3   19  1  1    4    1\nHornet 4 Drive     21   6  258 110  3.1 3.2   19  1  0    3    1\nHornet Sportabout  19   8  360 175  3.1 3.4   17  0  0    3    2\nValiant            18   6  225 105  2.8 3.5   20  1  0    3    1\n\nExcel\nPara ler um arquivo Excel (.xls ou .xlsx) no R √© necess√°rio importar um pacote chamado readxl que contem a fun√ß√£o read_excel. Para importar um pacote no R executamos o comando library() com um argumento √∫nico sendo o nome do pacote. Caso n√£o tenha o pacote instalado, deve instalar ele com o comando install.packages(). N√£o esque√ßa de colocar o nome do pacote entre aspas \"nome_do_pacote\" dentro do par√™nteses da fun√ß√£o.\n\n\n# install.packages(\"readxl\")\nlibrary(readxl)\ndf <- read_excel(\"datasets/mtcars.xlsx\")\nhead(df)\n\n\n# A tibble: 6 x 12\n  ...1       mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear\n  <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Mazda R‚Ä¶  21       6   160   110  3.9   2.62  16.5     0     1     4\n2 Mazda R‚Ä¶  21       6   160   110  3.9   2.88  17.0     0     1     4\n3 Datsun ‚Ä¶  22.8     4   108    93  3.85  2.32  18.6     1     1     4\n4 Hornet ‚Ä¶  21.4     6   258   110  3.08  3.22  19.4     1     0     3\n5 Hornet ‚Ä¶  18.7     8   360   175  3.15  3.44  17.0     0     0     3\n6 Valiant   18.1     6   225   105  2.76  3.46  20.2     1     0     3\n# ‚Ä¶ with 1 more variable: carb <dbl>\n\nGr√°ficos\nGeralmente no R voc√™ pode plotar mostrar graficamente diversos objetos com o comando plot(). Quando voc√™ plota um dataset (conjunto de dados lido de um aquivo), o R retorna um gr√°fico chamado Pair Plot:\nNa diagonal: nome da vari√°vel (coluna do dataset)\nFora da diagonal: um gr√°fico de dispers√£o entre a vari√°vel no eixo horizontal e a vari√°vel no eixo vertical\nExemplo: na figura 1 veja a rela√ß√£o entre disp (cilindrada) e hp (cavalos de pot√™ncia). Ela √© uma rela√ß√£o positiva. Quanto maior disp maior hp.\n\n\nplot(mtcars)\n\n\n\n\nFigure 1: Pair Plot do dataset mtcars\n\n\n\nAmbiente\nEm todos os arquivos dessa disciplina, mostrarei o ambiente computacional usado para replica√ß√£o.\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\nKross, S. (2021). Postcards: Create beautiful, simple personal websites. Retrieved from https://CRAN.R-project.org/package=postcards\n\n\nO‚ÄôHara-Wild, M., & Hyndman, R. (2021). Vitae: Curriculum vitae for r markdown. Retrieved from https://CRAN.R-project.org/package=vitae\n\n\nesse website foi todo feito com R‚Ü©Ô∏é\n",
      "last_modified": "2021-02-12T06:30:22-03:00"
    },
    {
      "path": "2-Regressao_Linear.html",
      "title": "Regress√£o Linear",
      "description": "Regress√£o Linear Bayesiana",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nrstanarm\nRegress√£o Linear\nExemplo - Score de QI de crian√ßas\nDescritivo das vari√°veis\nModelo 1 - mom_hs\nModelo 2 - mom_iq\nModelo 3 - mom_hs + mom_iq\nModelo 4 - mom_hs * mom_iq\n\nVari√°veis qualitativas\nAtividade Pr√°tica\nWHO Life Expectancy\nWine Quality Kaggle Dataset\n\nRefer√™ncias\nAmbiente\n\n\n\nA principal ferramenta para computa√ß√£o Bayesiana √© a linguagem probabil√≠stica Stan. O nome homenageia Stanislaw Ulam: um matem√°tico polon√™s membro do projeto Manhattan (bomba at√¥mica americana) e um dos principais criadores do m√©todo de Monte Carlo de simula√ß√£o. Stan foi lan√ßado em 2012 e √© a principal ferramenta utilizada hoje para infer√™ncia estat√≠stica Bayesiana. O programa roda em linguagem C++, mas possui interfaces para R, Python, MATLAB, Julia, Stata, Mathematica, Scala e Shell.\nO problema do Stan √© que ele √© uma linguagem de programa√ß√£o e, portanto, possui um acesso dificultado a n√£o-programadores. Abaixo um c√≥digo que mostra como √© um programa escrito em Stan:\n\ndata {\n  int<lower=0> N;\n  vector<lower=0, upper=200>[N] kid_score;\n  vector<lower=0, upper=200>[N] mom_iq;\n}\nparameters {\n  vector[2] beta;\n  real<lower=0> sigma;\n}\nmodel {\n  sigma ~ cauchy(0, 2.5);\n  kid_score ~ normal(beta[1] + beta[2] * mom_iq, sigma);\n}\n\nrstanarm\nPara remediar isso, temos interfaces abstratas que interpretam a inten√ß√£o do usu√°rio e lidam com a parte mais obral de codifica√ß√£o. A principal delas √© o pacote rstanarm, que a etimologia pode ser quebrada em:\nr: pacote para R\nstan: usa a linguagem probabil√≠stica Stan\narm: acr√¥nimo para Applied Regression Modeling\nO c√≥digo anterior de Stan ficaria assim no rstanarm:\n\n\nstan_glm(kid_score ~ mom_iq, data = dataset)\n\n\n\nRegress√£o Linear\nA ideia aqui √© modelar uma vari√°vel dependente sendo a combina√ß√£o linear de vari√°veis independentes.\n\\[y = \\alpha + \\boldsymbol{\\beta} \\textbf{X} + \\epsilon\\]\nAonde \\(y\\) √© a vari√°vel dependente, \\(\\alpha\\) um constante, \\(\\boldsymbol{\\beta}\\) um vetor de coeficientes, \\(\\textbf{X}\\) uma matriz de dados e \\(\\epsilon\\) o erro do modelo.\nExemplo - Score de QI de crian√ßas\nVamos aplicar modelagem estat√≠stica Bayesiana em um dataset famoso chamado kidiq. S√£o dados de uma survey de mulheres adultas norte-americanas e seus respectivos filhos. Datado de 2007 possui 434 observa√ß√µes e 4 vari√°veis:\nkid_score: QI da crian√ßa;\nmom_hs: bin√°ria (0 ou 1) se a m√£e possui diploma de ensino m√©dio;\nmom_iq: QI da m√£e; e\nmom_age: idade da m√£e.\nVamos usar 4 modelos para modelar QI da crian√ßa (kid_score). Os primeiros dois modelos ter√£o apenas um √∫nico preditor (mom_hs ou mom_iq), o terceiro usar√° dois preditores (mom_hs + mom_iq) e o quarto incluir√° uma intera√ß√£o entre esses dois preditores (mom_hs * mom_iq),\nDescritivo das vari√°veis\nAntes de tudo, analise SEMPRE os dados em m√£os. Graficamente e com tabelas.\nGr√°ficos\n\n\n# Detectar quantos cores/processadores\noptions(mc.cores = parallel::detectCores())\noptions(Ncpus = parallel::detectCores())\n\nlibrary(rstanarm)\ndata(kidiq)\n\nboxplot(kidiq)\n\n\n\n\nTabelas\nPessoalmente uso o pacote skimr com a fun√ß√£o skim():\n\n\nlibrary(skimr)\n\nskim(kidiq)\n\n\nTable 1: Data summary\nName\nkidiq\nNumber of rows\n434\nNumber of columns\n4\n_______________________\n\nColumn type frequency:\n\nnumeric\n4\n________________________\n\nGroup variables\nNone\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nkid_score\n0\n1\n86.80\n20.41\n20\n74\n90\n102\n144\n‚ñÅ‚ñÉ‚ñá‚ñá‚ñÅ\nmom_hs\n0\n1\n0.79\n0.41\n0\n1\n1\n1\n1\n‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñá\nmom_iq\n0\n1\n100.00\n15.00\n71\n89\n98\n110\n139\n‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÇ\nmom_age\n0\n1\n22.79\n2.70\n17\n21\n23\n25\n29\n‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÇ\n\nModelo 1 - mom_hs\nPrimeiro modelo √© apenas a vari√°vel mom_hs como preditora:\n\n\nmodel_1 <- stan_glm(\n  kid_score ~ mom_hs,\n  data = kidiq\n  )\n\n\n\nPara ver os valores estimados pelo modelo usamos a fun√ß√£o print:\n\n\nprint(model_1)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs\n observations: 434\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 77.6    2.1  \nmom_hs      11.7    2.3  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 19.9    0.7  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nAl√©m disso, temos a fun√ß√£o summary que traz tudo que queremos:\n\n\nsummary(model_1)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 434\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 77.6    2.1 75.0  77.6  80.3 \nmom_hs      11.7    2.4  8.6  11.7  14.7 \nsigma       19.9    0.7 19.0  19.9  20.7 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 86.8    1.4 85.1  86.8  88.5 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3809 \nmom_hs        0.0  1.0  3920 \nsigma         0.0  1.0  3689 \nmean_PPD      0.0  1.0  3892 \nlog-posterior 0.0  1.0  1767 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nModelo 2 - mom_iq\nSegundo modelo √© apenas a vari√°vel mom_iq como preditora:\n\n\nmodel_2 <- stan_glm(\n  kid_score ~ mom_iq,\n  data = kidiq\n  )\n\n\n\nPodemos tamb√©m especificar os percentis desejados no sum√°rio:\n\n\nsummary(model_2, probs = c(0.025, 0.975))\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_iq\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 434\n predictors:   2\n\nEstimates:\n              mean   sd   2.5%   98%\n(Intercept) 25.8    5.9 14.4   37.5 \nmom_iq       0.6    0.1  0.5    0.7 \nsigma       18.3    0.6 17.1   19.6 \n\nFit Diagnostics:\n           mean   sd   2.5%   98%\nmean_PPD 86.8    1.2 84.3   89.2 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  4373 \nmom_iq        0.0  1.0  4349 \nsigma         0.0  1.0  3823 \nmean_PPD      0.0  1.0  3687 \nlog-posterior 0.0  1.0  1799 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nModelo 3 - mom_hs + mom_iq\nTerceiro modelo usa as duas vari√°veis mom_hs e mom_iq como preditoras:\n\n\nmodel_3 <- stan_glm(\n  kid_score ~ mom_hs + mom_iq,\n  data = kidiq\n  )\n\n\n\n\n\nprint(model_3)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs + mom_iq\n observations: 434\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) 25.8    5.9  \nmom_hs       6.0    2.2  \nmom_iq       0.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.1    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nModelo 4 - mom_hs * mom_iq\nQuarto modelo usa as duas vari√°veis mom_hs e mom_iq como preditoras por meio de uma intera√ß√£o entre as duas:\n\n\nmodel_4 <- stan_glm(\n  kid_score ~ mom_hs * mom_iq,\n  data = kidiq\n  )\n\n\n\n\n\nprint(model_4)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs * mom_iq\n observations: 434\n predictors:   4\n------\n              Median MAD_SD\n(Intercept)   -9.5   14.0  \nmom_hs        48.9   15.5  \nmom_iq         0.9    0.1  \nmom_hs:mom_iq -0.5    0.2  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.0    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nVari√°veis qualitativas\nPara as vari√°veis qualitativas, o R usa um tipo especial de vari√°vel chamado factor. A codifica√ß√£o √© em n√∫meros inteiros \\(1,2,\\dots,K\\) mas a rela√ß√£o √© distinta/nominal. Ou seja 1 √© distinto de 2 e n√£o 1 √© 2x menor que 2. N√£o h√° rela√ß√£o quantitativa entre os valores das vari√°veis factor.\nIsso resolve o problema de termos vari√°veis qualitativas (tamb√©m chamadas de dummy) em modelos de regress√£o. Para um factor com \\(K\\) quantidade de classes distintas, temos a possibilidade de criar \\(K-1\\) coeficientes de regress√£o. Um para cada classe e usando uma como basal (baseline).\n\n\nlibrary(gapminder)\nlevels(gapminder$continent)\n\n\n[1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\" \n\nmodel_5 <- stan_glm(lifeExp ~ gdpPercap + factor(continent), data = gapminder)\n\n\n\n\n\nprint(model_5)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      lifeExp ~ gdpPercap + factor(continent)\n observations: 1704\n predictors:   6\n------\n                          Median MAD_SD\n(Intercept)               47.9    0.3  \ngdpPercap                  0.0    0.0  \nfactor(continent)Americas 13.6    0.6  \nfactor(continent)Asia      8.7    0.6  \nfactor(continent)Europe   17.6    0.6  \nfactor(continent)Oceania  18.1    1.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 8.4    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nObs: para mudar o basal de refer√™ncia de um factor use a fun√ß√£o relevel() do R.\nAtividade Pr√°tica\nDois datasets est√£o dispon√≠veis na pasta datasets/:\nWHO Life Expectancy Kaggle Dataset: datasets/WHO_Life_Exp.csv\nWine Quality Kaggle Dataset: datasets/Wine_Quality.csv\nWHO Life Expectancy\nEsse dataset possui 193 pa√≠ses nos √∫ltimos 15 anos.\nVari√°veis\ncountry\nyear\nstatus\nlife_expectancy\nadult_mortality\ninfant_deaths\nalcohol\npercentage_expenditure\nhepatitis_b\nmeasles\nbmi\nunder_five_deaths\npolio\ntotal_expenditure\ndiphtheria\nhiv_aids\ngdp\npopulation\nthinness_1_19_years\nthinness_5_9_years\nincome_composition_of_resources\nschooling\nWine Quality Kaggle Dataset\nEsse dataset possui 1599 vinhos e est√£o relacionados com variantes tintas do vinho ‚ÄúVinho Verde‚Äù portugu√™s. Para mais detalhes, consulte a refer√™ncia [Cortez et al., 2009]. Devido a quest√µes de privacidade e log√≠stica, apenas vari√°veis f√≠sico-qu√≠micas (entradas) e sensoriais (a sa√≠da) est√£o dispon√≠veis (por exemplo, n√£o h√° dados sobre os tipos de uva, marca de vinho, pre√ßo de venda do vinho, etc.).\nfixed_acidity\nvolatile_acidity\ncitric_acid\nresidual_sugar\nchlorides\nfree_sulfur_dioxide\ntotal_sulfur_dioxide\ndensity\np_h\nsulphates\nalcohol\nquality\n\n\n###\n\n\n\nRefer√™ncias\nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:31:23-03:00"
    },
    {
      "path": "3-Distribuicoes_Estatisticas.html",
      "title": "Distribui√ß√µes Estat√≠sticas",
      "description": "Distribui√ß√µes Estat√≠sticas",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nDiscretas\nUniforme Discreta\nBinomial\nPoisson\n\nCont√≠nuas\nNormal / Gaussiana\nLog-normal\nExponencial\nDistribui√ß√£o t de Student\n\nDashboard de Distribui√ß√µes\nAmbiente\n\n\nA estat√≠stica usa distribui√ß√µes probabil√≠sticas como o motor de sua infer√™ncia na elabora√ß√£o dos valores dos par√¢metros estimados e suas incertezas.\nUma distribui√ß√£o de probabilidade √© a fun√ß√£o matem√°tica que fornece as probabilidades de ocorr√™ncia de diferentes resultados poss√≠veis para um experimento. √â uma descri√ß√£o matem√°tica de um fen√¥meno aleat√≥rio em termos de seu espa√ßo amostral e as probabilidades de eventos (subconjuntos do espa√ßo amostral)\nGeralmente usamos a nota√ß√£o X ~ Dist(par1, par2, ...). Onde X √© a vari√°vel Dist √© a distribui√ß√£o e par os par√¢metros que definem como a distribui√ß√£o se comporta.\nDiscretas\nDistribui√ß√µes de probabilidade discretas s√£o aquelas que os resultados s√£o n√∫meros discretos (tamb√©m chamados de n√∫meros inteiros): \\(\\dots, -2, 1, 0,1,2,\\dots, N\\) e \\(N \\in \\mathbb{Z}\\).\nUniforme Discreta\nA distribui√ß√£o uniforme discreta √© uma distribui√ß√£o de probabilidade sim√©trica em que um n√∫mero finito de valores s√£o igualmente prov√°veis de serem observados. Cada um dos \\(n\\) valores tem probabilidade igual \\(\\frac{1}{n}\\). Outra maneira de dizer ‚Äúdistribui√ß√£o uniforme discreta‚Äù seria ‚Äúum n√∫mero conhecido e finito de resultados igualmente prov√°veis de acontecer.‚Äù\nA distribui√ß√£o uniforme discreta possui dois par√¢metros e sua nota√ß√£o √© \\(U(a, b)\\):\nLimite Inferior (\\(a\\))\nLimite Superior (\\(b\\))\nExemplo: Um dado.\n\n\nx <- seq(1, 6)\ny <- dunif(x, min = 1, max = 6)\n\nplot(x, y, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Distribui√ß√£o Uniforme Discreta\",\n  lwd=2, col=\"red\"\n)\n\n\n\n\nBinomial\nA distribui√ß√£o binomial descreve um evento do n√∫mero de sucessos em uma sequ√™ncia de \\(n\\) experimentos independentes, cada um fazendo uma pergunta sim-n√£o.\nA distribui√ß√£o binomial √© freq√ºentemente usada para modelar o n√∫mero de sucessos em uma amostra de tamanho \\(n\\) desenhada com substitui√ß√£o de uma popula√ß√£o de tamanho \\(N\\).\nA distribui√ß√£o binomial possui dois par√¢metros e sua nota√ß√£o √© \\(Bin(n, p)\\):\nN√∫mero de Experimentos (\\(n\\))\nProbabiliade de Sucessos (\\(p\\))\nExemplo: quantidade de caras em 5 lan√ßamentos de uma moeda.\n\n\nx <- seq(0, 5)\n\nprobs <- c(0.1, 0.2, 0.5)\ncolors <- c(\"red\", \"blue\", \"darkgreen\")\nlabels <- c(\"p=0.1\", \"p=0.2\", \"p=0.5\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribui√ß√µes Binomiais\",\n  xlim = c(0, 5),\n  ylim = c(0, 1))\n\nfor (i in 1:4){\n  lines(x, dbinom(x, 5, prob = probs[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Desvio Padr√µes\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nPoisson\nA distribui√ß√£o Poisson expressa a probabilidade de um determinado n√∫mero de eventos ocorrerem em um intervalo fixo de tempo ou espa√ßo se esses eventos ocorrerem com uma taxa m√©dia constante conhecida e independentemente do tempo desde o √∫ltimo evento. A distribui√ß√£o de Poisson tamb√©m pode ser usada para o n√∫mero de eventos em outros intervalos especificados, como dist√¢ncia, √°rea ou volume.\nA distribui√ß√£o Poisson possui um par√¢metro e sua nota√ß√£o √© \\(pois(\\lambda)\\):\nTaxa (\\(\\lambda\\))\nExemplo: Quantidade de e-mails que voc√™ recebe diariamente. Quantidade de buracos que voc√™ encontra na rua.\n\n\nx <- seq(0, 20)\n\nrates <- c(1, 4, 10)\ncolors <- c(\"red\", \"blue\", \"darkgreen\")\nlabels <- c(\"taxa=1\", \"taxa=4\", \"taxa=10\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribui√ß√µes Poisson\",\n  xlim = c(0, 20),\n  ylim = c(0, 0.5))\n\nfor (i in 1:4){\n  lines(x, dpois(x, lambda = rates[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Taxas\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nCont√≠nuas\nDistribui√ß√µes de probabilidade cont√≠nuas s√£o aquelas que os resultados s√£o valores em uma faixa cont√≠nua (tamb√©m chamados de n√∫mero reais): \\([-\\infty, \\infty] \\in \\mathbb{R}\\).\nNormal / Gaussiana\nEssa distribui√ß√£o geralmente √© usada nas ci√™ncias sociais e naturais para representar vari√°veis cont√≠nuas na qual as suas distribui√ß√µes n√£o s√£o conhecidas. Esse pressuposto √© por conta do teorema do limite central. O teorema do limite central afirma que, em algumas condi√ß√µes, a m√©dia de muitas amostras (observa√ß√µes) de uma vari√°vel aleat√≥ria com m√©dia e vari√¢ncia finitas √© ela pr√≥pria uma vari√°vel aleat√≥ria cuja distribui√ß√£o converge para uma distribui√ß√£o normal √† medida que o n√∫mero de amostras aumenta. Portanto, as quantidades f√≠sicas que se espera sejam a soma de muitos processos independentes (como erros de medi√ß√£o) muitas vezes t√™m distribui√ß√µes que s√£o quase normais.\nA distribui√ß√£o normal possui dois par√¢metros e sua nota√ß√£o √© \\(N(\\mu, \\sigma^2)\\):\nM√©dia (\\(\\mu\\)): m√©dia da distribui√ß√£o e tamb√©m a moda e a mediana\nDesvio Padr√£o (\\(\\sigma\\)): a vari√¢ncia da distribui√ß√£o (\\(\\sigma^2\\)) √© uma m√©dia de dispers√£o das observa√ß√µes em rela√ß√£o √† m√©dia\nExemplo: Altura, Peso etc.\n\n\nx <- seq(-4, 4, length = 100)\n\ndps <- c(0.5, 1, 2, 5)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"gold\")\nlabels <- c(\"dp=0.5\", \"dp=1\", \"dp=2\", \"dp=5\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribui√ß√µes Normais\",\n  xlim = c(-4, 4),\n  ylim = c(0, 1))\n\nfor (i in 1:4){\n  lines(x, dnorm(x, mean = 0, sd = dps[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Desvio Padr√µes\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nLog-normal\nA distribui√ß√£o Log-normal √© uma distribui√ß√£o de probabilidade cont√≠nua de uma vari√°vel aleat√≥ria cujo logaritmo √© normalmente distribu√≠do. Assim, se a vari√°vel aleat√≥ria \\(X\\) for distribu√≠da normalmente por log, ent√£o \\(Y = \\ln (X)\\) ter√° uma distribui√ß√£o normal.\nUma vari√°vel aleat√≥ria com distribui√ß√£o logar√≠tmica aceita apenas valores reais positivos. √â um modelo conveniente e √∫til para medi√ß√µes em ci√™ncias exatas e de engenharia, bem como medicina, economia e outros campos, por ex. para energias, concentra√ß√µes, comprimentos, retornos financeiros e outros valores.\nUm processo log-normal √© a realiza√ß√£o estat√≠stica do produto multiplicativo de muitas vari√°veis aleat√≥rias independentes, cada uma das quais positiva.\nA distribui√ß√£o log-normal possui dois par√¢metros e sua nota√ß√£o √© \\(Lognormal(\\mu, \\sigma^2)\\):\nM√©dia (\\(\\mu\\)): m√©dia do logaritmo natural da distribui√ß√£o\nDesvio Padr√£o (\\(\\sigma\\)): a vari√¢ncia do logaritmo natural da distribui√ß√£o (\\(\\sigma^2\\)) √© uma m√©dia de dispers√£o das observa√ß√µes em rela√ß√£o √† m√©dia\n\n\nx <- seq(0, 3, length = 100)\n\ndps <- c(0.25, 0.5, 1, 1.5)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"gold\")\nlabels <- c(\"dp=0.25\", \"dp=0.5\", \"dp=1\", \"dp=1.5\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribui√ß√µes Log-Normais\",\n  xlim = c(0, 3),\n  ylim = c(0, 2))\n\nfor (i in 1:4){\n  lines(x, dlnorm(x, mean = 0, sd = dps[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Desvio Padr√µes\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nExponencial\nA distribui√ß√£o exponencial √© a distribui√ß√£o de probabilidade do tempo entre eventos que ocorrem de forma cont√≠nua e independente a uma taxa m√©dia constante.\nA distribui√ß√£o exponencial possui um par√¢metro e sua nota√ß√£o √© \\(Exp (\\lambda)\\):\nTaxa (\\(\\lambda\\))\nExemplo: Quanto tempo at√© o pr√≥ximo terremoto. Quanto tempo at√© o pr√≥ximo √¥nibus.\n\n\nx <- seq(0, 5, length = 100)\n\nrates <- c(0.5, 1, 1.5, 2)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"gold\")\nlabels <- c(\"taxa=0.5\", \"taxa=1.0\", \"taxa=1.5\", \"taxa=2.0\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribui√ß√µes Exponenciais\",\n  xlim = c(0, 5),\n  ylim = c(0, 1.5))\n\nfor (i in 1:4){\n  lines(x, dexp(x,rate = rates[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Taxas\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nDistribui√ß√£o t de Student\nA distribui√ß√£o t de Student surge ao estimar a m√©dia de uma popula√ß√£o normalmente distribu√≠da em situa√ß√µes onde o tamanho da amostra √© pequeno e o desvio padr√£o da popula√ß√£o √© desconhecido.\nSe tomarmos uma amostra de \\(n\\) observa√ß√µes de uma distribui√ß√£o normal, ent√£o a distribui√ß√£o t com \\(\\nu = n-1\\) graus de liberdade pode ser definida como a distribui√ß√£o da localiza√ß√£o da m√©dia da amostra em rela√ß√£o √† m√©dia verdadeira, dividida pela desvio padr√£o da amostra, ap√≥s multiplicar pelo termo padronizador \\(\\sqrt{n}\\).\nA distribui√ß√£o t √© sim√©trica e em forma de sino, como a distribui√ß√£o normal, mas tem caudas mais pesadas, o que significa que √© mais propensa a produzir valores que est√£o longe de sua m√©dia.\nA distribui√ß√£o t de Student possui um par√¢metro e sua nota√ß√£o √© \\(Student (\\nu)\\):\nGraus de Liberdade (\\(\\nu\\)): controla o quanto ela se assemelha com uma distribui√ß√£o normal\nExemplo: Uma base de dados cheia de outliers.\n\n\nx <- seq(-4, 4, length = 100)\n\ndegfs <- c(1, 3, 8, 30)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"gold\")\nlabels <- c(\"df=1\", \"df=3\", \"df=8\", \"df=30\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribui√ß√µes t de Student\",\n  xlim = c(-4, 4),\n  ylim = c(0, 0.5))\n\nfor (i in 1:4){\n  lines(x, dt(x,df = degfs[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Graus de Liberdade\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nDashboard de Distribui√ß√µes\nPara acessar todo o zool√≥gico de distribui√ß√µes use essa ferramenta do Ben Lambert (estat√≠stico do Imperial College of London): https://ben18785.shinyapps.io/distribution-zoo/\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:31:30-03:00"
    },
    {
      "path": "4-Priors.html",
      "title": "Priors",
      "description": "As famosas e controversas Priors",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nTipos de Priors\nPriors para os Modelos\nUniforme (Flat Prior)\nInformativas\nPadr√µes do rstanarm\nExemplo usando o mtcars\n\nPor qu√™ n√£o √© interessante usar priors uniformes (flat priors)\nAtividade\n\nAmbiente\n\n\nA Estat√≠stica Bayesiana √© caracterizada pelo uso de informa√ß√£o pr√©via embutida como probabilidade pr√©via \\(P(H)\\)\n\\[P(H | D)=\\frac{P(H) \\cdot P(D | H)}{P(D)}\\]\nTipos de Priors\nDe maneira geral, podemos ter 3 tipos de priors em uma abordagem Bayesiana:\nuniforme (Flat Prior): n√£o recomendada\nfracamente informativa (weakly informative): pequena restri√ß√£o com um pouco de senso comum e baixo conhecimento de dom√≠nio incorporado\ninformativa (informative): conhecimento de dom√≠nio incorporado\nPara se aprofundar mais recomendo a vignette do rstanarm sobre priors\nPriors para os Modelos\nArgumento\nUsado em\nAplica-se √†\nprior_intercept\nTodas fun√ß√µes de modelagem exceto stan_polr and stan_nlmer\nConstante (intercept) do modelo, ap√≥s centraliza√ß√£o dos preditores\nprior\nTodas fun√ß√µes de modelagem\nCoeficientes de Regress√£o, n√£o inclui coeficientes que variam por grupo em modelos multin√≠veis (veja prior_covariance)\nprior_aux\nstan_glm, stan_glmer, stan_gamm4, stan_nlmer\nPar√¢metro auxiliar (ex: desvio padr√£o (standard error - DP), interpreta√ß√£o depende do modelo\nprior_covariance\nstan_glmer, stan_gamm4, stan_nlmer\nMatrizes de covari√¢ncia em modelos multin√≠veis\nUniforme (Flat Prior)\nEspecifica-se colocando o valor NULL (nulo em R) no. Exemplo:\nprior_intercept = NULL\nprior = NULL\nprior_aux = NULL\nColocando na fun√ß√£o de modelo ficaria stan_glm(y ~ x1 + x2, data = df, prior = NULL, prior_intercept = NULL, prior_aux = NULL)\nInformativas\nColoca-se qualquer distribui√ß√£o nos argumentos. Exemplo:\nprior = normal(0, 5)\nprior_intercept = student_t(4, 0, 10)\nprior_aux = cauchy(0, 3)\nColocando na fun√ß√£o de modelo ficaria stan_glm(y ~ x1 + x2, data = df, prior = normal(0, 5), prior_intercept = student_t(4, 0, 10), prior_aux = cauchy(0, 3))\nPadr√µes do rstanarm\nAcontece se voc√™ n√£o especifica nada nos argumentos de priors. O comportamento difere conforme o modelo. Aqui divido em modelos gaussianos (segue uma likelihood gaussiana ou normal) e outros (binomial, poisson etc)\nModelos Gaussianos\nConstante(Intercept): centralizada com m√©dia \\(\\mu_y\\) e desvio padr√£o de \\(2.5 \\sigma_y\\) - prior_intercept = normal(mean_y, 2.5 * sd_y)\nCoeficientes: para cada coeficiente m√©dia \\(\\mu = 0\\) e desvio padr√£o de \\(2.5\\times\\frac{\\sigma_y}{\\sigma_{x_k}}\\) - prior = normal(0, 2.5 * sd_y/sd_xk)\nOutros Modelos (Binomial, Poisson etc.)\nConstante(Intercept): centralizada com m√©dia \\(\\mu = 0\\) e desvio padr√£o de \\(2.5 \\sigma_y\\) - prior_intercept = normal(0, 2.5 * sd_y)\nCoeficientes: para cada coeficiente m√©dia \\(\\mu = 0\\) e desvio padr√£o de \\(2.5\\times\\frac{1}{\\sigma_{x_k}}\\) - prior = normal(0, 2.5 * 1/sd_xk)\n\nOBS: em todos os modelos prior_aux, o desvio padr√£o do erro do modelo, a prior padr√£o √© uma distribui√ß√£o exponencial com taxa \\(\\frac{1}{\\sigma_y}\\): prior_aux = exponential(1/sd_y)\n\nExemplo usando o mtcars\nVamos estimar modelos Bayesianos usando o dataset j√° conhecido mtcars. Para constar, calcularemos alguns valores antes de ver o sum√°rio das priors:\n\\(\\mu_y\\): m√©dia do mpg - 20.09\n\\(2.5 \\sigma_y\\): 2.5 * sd(mtcars$mpg) - 15.07\n\\(2.5\\times\\frac{\\sigma_y}{\\sigma_{x_{\\text{wt}}}}\\): 2.5 * (sd(mtcars$mpg)/sd(mtcars$wt)) - 15.4\n\\(2.5\\times\\frac{\\sigma_y}{\\sigma_{x_{\\text{am}}}}\\): 2.5 * (sd(mtcars$mpg)/sd(mtcars$am)) - 30.2\n\\(\\frac{1}{\\sigma_y}\\): 1/sd(mtcars$mpg) - 0.17\nA fun√ß√£o prior_summary resulta um sum√°rio conciso das priors utilizadas em um modelo. Coloque como argumento o modelo estimado:\n\n\nlibrary(rstanarm)\ndefault_prior_test <- stan_glm(mpg ~ wt + am, data = mtcars, chains = 1)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.049406 seconds (Warm-up)\nChain 1:                0.034335 seconds (Sampling)\nChain 1:                0.083741 seconds (Total)\nChain 1: \n\nprior_summary(default_prior_test)\n\n\nPriors for model 'default_prior_test' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 20, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 20, scale = 15)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0], scale = [2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [0,0], scale = [15.40,30.20])\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.17)\n------\nSee help('prior_summary.stanreg') for more details\n\nAgora com priors especificadas:\nComo h√° dois coeficientes eu especifico m√©dias iguais (\\(0\\)), por√©m desvios padr√µes diferentes (\\(5\\) para wt e \\(6\\) para am) usando a fun√ß√£o de combinar do R (combine) - c()\n\n\ncustom_prior_test <- stan_glm(mpg ~ wt + am, data = mtcars, chains = 1,\n         prior = normal(c(0, 0), c(5, 6)),\n         prior_intercept = student_t(4, 0, 10),\n         prior_aux = cauchy(0, 3))\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.038517 seconds (Warm-up)\nChain 1:                0.039088 seconds (Sampling)\nChain 1:                0.077605 seconds (Total)\nChain 1: \n\nprior_summary(custom_prior_test)\n\n\nPriors for model 'custom_prior_test' \n------\nIntercept (after predictors centered)\n ~ student_t(df = 4, location = 0, scale = 10)\n\nCoefficients\n ~ normal(location = [0,0], scale = [5,6])\n\nAuxiliary (sigma)\n ~ half-cauchy(location = 0, scale = 3)\n------\nSee help('prior_summary.stanreg') for more details\n\nPor qu√™ n√£o √© interessante usar priors uniformes (flat priors)\nUma prior totalmente uniforme ou chapada (flat) √© algo que devemos evitar pelo simples motivo que ela parte da premissa de que ‚Äútudo √© poss√≠vel.‚Äù N√£o h√° limites na cren√ßa de que tamanho o valor deve ser.\nPriors chapadas e super-vagas geralmente n√£o s√£o recomendadas e algum esfor√ßo deve ser inclu√≠do para ter, pelo menos, priors um pouco informativa. Por exemplo, √© comum esperar que os tamanhos de efeito realistas sejam da ordem de magnitude \\(0.1\\) em uma escala padronizada (por exemplo, uma inova√ß√£o educacional que pode melhorar as pontua√ß√µes dos testes em \\(0.1\\) desvios padr√£o). Nesse caso, um prior de \\(N \\sim (0,1)\\) poderia ser considerado muito informativo, de uma maneira ruim, pois coloca a maior parte de sua massa em valores de par√¢metro que s√£o irrealisticamente grandes em valor absoluto. O ponto geral aqui √© que se considerarmos uma prior como ‚Äúfraca‚Äù ou ‚Äúforte,‚Äù isso √© uma propriedade n√£o apenas da prior, mas tamb√©m da pergunta que est√° sendo feita.\nQuando dizemos que a prior √© ‚Äúpouco informativa,‚Äù o que queremos dizer √© que, se houver uma quantidade razoavelmente grande de dados, a likelihood dominar√° e a prior n√£o ser√° importante. Se os dados forem fracos, por√©m, esta ‚Äúprior fracamente informativo‚Äù influenciar√° fortemente a infer√™ncia posterior.\nN√£o se esque√ßa que distribui√ß√£o normal tem suporte \\(\\mathbb{R}\\), ou seja pode acontecer qualquer n√∫mero entre \\(-\\infty\\) at√© \\(\\infty\\) independente da m√©dia \\(\\mu\\) ou desvio padr√£o \\(\\sigma\\).\nAtividade\nRegress√£o linear pensando nas priors. Usar o dataset do pacote carData chamado Salaries\n\n\nlibrary(carData)\ndata(\"Salaries\")\n?Salaries\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:31:32-03:00"
    },
    {
      "path": "5-MCMC.html",
      "title": "Markov Chain Monte Carlo -- MCMC",
      "description": "O motor por tr√°s da Estat√≠stica Bayesiana",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nPara qu√™ serve o denominador \\(P(\\text{data})\\)?\nSe removermos o denominador de Bayes o que temos?\nSimula√ß√£o Montecarlo com correntes Markov ‚Äì (MCMC)\nSimula√ß√µes ‚Äì Setup\nMetropolis e Metropolis-Hastings\nGibbs\nO que acontece quando rodamos correntes Markov em paralelo?\n\nHamiltonian Monte Carlo ‚Äì HMC\nDistribui√ß√£o dos Momentos ‚Äì \\(P(\\phi)\\)\nAlgoritmo de HMC\nHMC ‚Äì Implementa√ß√£o\n\n‚ÄúN√£o entendi nada‚Ä¶‚Äù\nImplementa√ß√£o com o rstanarm\nM√©tricas da simula√ß√£o MCMC\nO que fazer se n√£o obtermos converg√™ncia?\n\nGr√°ficos de Diagn√≥sticos do MCMC\nTraceplot\nPosterior Predictive Check\n\nO qu√™ fazer para convergir suas correntes Markov\nAmbiente\n\n\nA principal barreira computacional para estat√≠stica Bayesiana √© o denominador \\(P(\\text{data})\\) da f√≥rmula de Bayes:\n\\[P(\\theta | \\text{data})=\\frac{P(\\theta) \\cdot P(\\text{data} | \\theta)}{P(\\text{data})}\\]\nEm casos discretos podemos fazer o denominador virar a soma de todos os par√¢metros usando a regra da cadeia de probabilidade:\n\\[P(A,B|C)=P(A|B,C) \\times P(B|C)\\]\nIsto tamb√©m √© chamado de marginaliza√ß√£o:\n\\[P(\\text{data})=\\sum_{\\theta} P(\\text{data} | \\theta) \\times P(\\theta)\\]\nPor√©m no caso de valores cont√≠nuos o denominador \\(P(\\text{data})\\) vira uma integral bem grande e complicada de calcular:\n\\[P(\\text{data})=\\int_{\\theta} P(\\text{data} | \\theta) \\times P(\\theta)d \\theta\\]\nEm muitos casos essa integral vira intrat√°vel (incalcul√°vel) e portanto devemos achar outras maneiras de calcular a probabilidade posterior \\(P(\\theta | \\text{data})\\) de Bayes sem usar o denominador \\(P(\\text{data})\\).\nPara qu√™ serve o denominador \\(P(\\text{data})\\)?\nPara normalizar a posterior com o intuito de torn√°-la uma distribui√ß√£o probabil√≠stica v√°lida. Isto quer dizer que a soma de todas as probabilidades dos eventos poss√≠veis da distribui√ß√£o probabil√≠stica devem ser iguais a 1:\nno caso de distribui√ß√£o probabil√≠stica discreta: \\(\\sum_{\\theta} P(\\theta | \\text{data}) = 1\\)\nno caso de distribui√ß√£o probabil√≠stica cont√≠nua: \\(\\int_{\\theta} P(\\theta | \\text{data})d \\theta = 1\\)\nSe removermos o denominador de Bayes o que temos?\nAo removermos o denominador \\((\\text{data})\\) temos que a posterior \\(P(\\theta | \\text{data})\\) √© proporcional √† prior multiplicada pela verossimilhan√ßa \\(P(\\theta) \\cdot P(\\text{data} | \\theta)\\)1.\n\\[P(\\theta | \\text{data}) \\propto P(\\theta) \\cdot P(\\text{data} | \\theta)\\]\nEste v√≠deo do YouTube explica muito bem o problema do denominador.\n\n\nPlease use a browser that supports iframe embedding. If you are seeing this message Google ‚Äúbrowser iframe embedding not rendering.‚Äù\n\n\nSimula√ß√£o Montecarlo com correntes Markov ‚Äì (MCMC)\nA√≠ que entra simula√ß√£o Montecarlo com correntes Markov (do ingl√™s Markov Chain Monte Carlo ‚Äì MCMC). MCMC √© uma classe ampla de ferramentas computacionais para aproxima√ß√£o de integrais e gera√ß√£o de amostras de uma probabilidade posterior (S. Brooks, Gelman, Jones, & Meng, 2011). MCMC √© usada quando n√£o √© poss√≠vel coletar amostras de \\(\\theta\\) direto da distribui√ß√£o probabil√≠stica posterior \\(P(\\theta | \\text{data})\\). Ao inv√©s disso, nos coletamos amostras de maneira iterativa que a cada passo do processo n√≥s esperamos que a distribui√ß√£o da qual amostramos \\(P^*(\\theta^* | \\text{data})\\) (aqui \\(*\\) quer dizer simulado) se torna cada vez mais similar √† posterior \\(P(\\theta | \\text{data})\\). Tudo isso √© para eliminar o c√°lculo (muitas vezes imposs√≠vel) do denominador \\(P(\\text{data})\\).\nA ideia √© definir uma corrente Markov erg√≥dica (quer dizer que h√° uma distribui√ß√£o estacion√°ria √∫nica) dos quais o conjunto de estados poss√≠veis √© o espa√ßo amostral e a distribui√ß√£o estacion√°ria √© a distribui√ß√£o a ser aproximada (ou amostrada). Seja \\(X_0, X_1, \\dots, X_n\\) uma simula√ß√£o da corrente. A corrente Markov converge √† distribui√ß√£o estacion√°ria de qualquer estado inicial \\(X_0\\) ap√≥s um n√∫mero suficiente grande de itera√ß√µes \\(r\\), a distribui√ß√£o do estado \\(X_r\\) estar√° similar √† distribui√ß√£o estacion√°ria, ent√£o podemos us√°-la com amostra. As correntes Markov possuem uma propriedade que a distribui√ß√£o de probabilidade do pr√≥ximo estado depende apenas do estado atual e n√£o na sequ√™ncia de eventos que precederam: \\(P(X_{n+1}=x|X_{0},X_{1},X_{2},\\ldots ,X_{n}) = P(X_{n+1}=x|X_{n})\\). Essa propriedade √© chamada de Markoviana, em homenagem ao matem√°tico Andrei Andreyevich Markov (figura 1). Similarmente, repetindo esse argumento com \\(X_r\\) como o ponto inicial, podemos usar \\(X_{2r}\\) como amostra, e assim por diante. Podemos ent√£o usar a sequ√™ncia de estados \\(X_r, X_{2r}, X_{3r}, \\dots\\) como quase amostras independentes da distribui√ß√£o estacion√°ria da corrente Markov.\n\n\n\nFigure 1: Andrei Andreyevich Markov. Figura de https://www.wikipedia.org\n\n\n\nA efic√°cia dessa abordagem depende em:\no qu√£o grande \\(r\\) deve ser para garantir uma amostra adequadamente boa; e\npoder computacional requerido para cada itera√ß√£o da corrente Markov.\nAl√©m disso, √© costumeiro descartarmos as primeiras itera√ß√µes do algoritmo pois elas costumam n√£o ser representativas da distribui√ß√£o a ser aproximada. Nas itera√ß√µes iniciais de algoritmos MCMC geralmente a corrente Markov est√° em um processo de aquecimento2 (warm-up) e seu estado est√° bem distante do ideal para come√ßarmos uma amostragem fidedigna. Geralmente, recomenda-se que se descarte metade das itera√ß√µes (Gelman et al., 2013a). Por exemplo: se a corrente Markov possui 4.000 itera√ß√µes, descartamos as 2.000 primeiras como warm-up.\nSimula√ß√µes ‚Äì Setup\nEstou usando diversos pacotes:\nggplot2, plotly e ggforce para gr√°ficos.\ngganimate para anima√ß√µes (GIFs).\nMASS para simula√ß√µes aleat√≥rias de distribui√ß√µes multivariadas.\nrstan para fun√ß√µes de sum√°rio e m√©tricas de converg√™ncia e desempenho de simula√ß√µes MCMC.\n\n\nlibrary(ggplot2)\ntheme_set(theme_minimal())\nlibrary(plotly)\nlibrary(gganimate)\nlibrary(ggforce)\nlibrary(MASS)\nlibrary(rstan)\n\n\n\nVamos come√ßar com um problema did√°tico de uma distribui√ß√£o normal multivariada de \\(X\\) e \\(Y\\), onde\n\\[\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix} \\sim \\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}, \\mathbf{\\Sigma}\n\\right) \\\\\n\\mathbf{\\Sigma} \\sim\n\\begin{pmatrix}\n\\sigma^2_{X} & \\sigma_{X}\\sigma_{Y} \\rho \\\\\n\\sigma_{X}\\sigma_{Y} \\rho & \\sigma^2_{Y}\n\\end{pmatrix}\n\\]\nSe designarmos \\(\\mu_X = \\mu_Y = 0\\) e \\(\\sigma_X = \\sigma_Y = 1\\) (m√©dia 0 e desvio padr√£o 1 para ambos \\(X\\) e \\(Y\\)), temos a seguinte formula√ß√£o:\n\\[\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix} \\sim \\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix}, \\mathbf{\\Sigma}\n\\right), \\\\\n\\mathbf{\\Sigma} \\sim\n\\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{pmatrix}.\n\\]\nS√≥ faltando designar um valor de \\(\\rho\\) para a correla√ß√£o entre \\(X\\) e \\(Y\\). Para o nosso exemplo vamos usar correla√ß√£o de 0.8 (\\(\\rho = 0.8\\)):\n\\[\n\\mathbf{\\Sigma} \\sim\n\\begin{pmatrix}\n1 & 0.8 \\\\\n0.8 & 1\n\\end{pmatrix}.\n\\]\n\n\nmus  <- c(0, 0)\nsigmas <- c(1, 1)\nr <- 0.8\nSigma <- diag(sigmas)\nSigma[1, 2] <- r\nSigma[2, 1] <- r\ndft <- data.frame(mvrnorm(1e5, mus, Sigma))\n\n\n\nNa figura 2 √© poss√≠vel ver um gr√°fico de densidade de uma distribui√ß√£o multivariada normal de duas vari√°veis normais \\(X\\) e \\(Y\\), ambas com m√©dia 0 e desvio padr√£o 1. Sendo que a correla√ß√£o entre elas √© 0.8. E na figura 3 √© poss√≠vel ver uma imagem 3-D interativa da mesma distribui√ß√£o, fique a vontade em usar seu mouse (dedo ou caneta, dependendo do dispositivo) para movimentar a imagem.\n\n\nggplot(dft, aes(X1, X2)) +\n  geom_density2d_filled() +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(title = \"Multivariada Normal\",\n       subtitle = expression(list(mu == 0, sigma == 1, rho == 0.8)),\n       caption = \"10.000 simula√ß√µes\",\n       x = expression(X), y = expression(Y)) +\n  theme(legend.position = \"NULL\")\n\n\n\n\nFigure 2: Gr√°fico de Densidade de uma distribui√ß√£o Multivariada Normal\n\n\n\n\n\ndens <- kde2d(dft$X1, dft$X2)\nplot_ly(x = dens$x,\n        y = dens$y,\n        z = dens$z) %>% add_surface()\n\n\n\n\n{\"x\":{\"visdat\":{\"f151338bc78c\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"f151338bc78c\",\"attrs\":{\"f151338bc78c\":{\"x\":[-4.11222323909044,-3.76275945114626,-3.41329566320208,-3.0638318752579,-2.71436808731372,-2.36490429936954,-2.01544051142536,-1.66597672348118,-1.316512935537,-0.967049147592815,-0.617585359648634,-0.268121571704453,0.0813422162397277,0.430806004183909,0.780269792128089,1.12973358007227,1.47919736801645,1.82866115596063,2.17812494390481,2.52758873184899,2.87705251979317,3.22651630773736,3.57598009568154,3.92544388362572,4.2749076715699],\"y\":[-4.32381413035071,-3.95784855910568,-3.59188298786066,-3.22591741661563,-2.85995184537061,-2.49398627412558,-2.12802070288055,-1.76205513163553,-1.3960895603905,-1.03012398914547,-0.664158417900448,-0.298192846655422,0.0677727245896049,0.433738295834631,0.799703867079657,1.16566943832468,1.53163500956971,1.89760058081474,2.26356615205976,2.62953172330479,2.99549729454982,3.36146286579484,3.72742843703987,4.09339400828489,4.45935957952992],\"z\":[[3.40651108352134e-09,1.41934533126133e-08,8.54786240302624e-08,0.000130831863209747,1.94776526567504e-06,1.49783603255103e-06,1.1170089576991e-08,6.01986954373106e-12,1.16817732135535e-10,1.50245544102462e-14,4.62944974810078e-23,4.83295093316628e-35,2.28048400569722e-45,8.78734985512727e-51,2.21962779786052e-61,3.67504668072926e-77,3.98846726370328e-98,2.83738455537123e-124,1.16702068177763e-153,1.56588648116791e-183,5.9460095882168e-217,3.04648237511363e-242,1.0476778777822e-272,2.36166828733039e-308,0],[4.39462560522794e-05,6.94943900679841e-05,1.04291502349634e-05,5.75970473307081e-05,0.000176906604268204,0.000370540209494104,1.55421446807263e-05,8.30475843056959e-07,1.53069632670964e-05,7.14342142452053e-09,1.13543480848847e-15,2.9038893623358e-25,2.64561847471716e-28,1.01950388004498e-33,2.57520093025581e-44,4.26377054766638e-60,4.6281649856682e-81,2.42118913413043e-104,5.00622985414094e-129,7.25515626339583e-159,3.68875889960785e-180,1.93530271945878e-205,6.65546074071442e-236,5.25171440440826e-271,5.21884573812369e-305],[9.55774448047614e-05,0.000177213516750328,0.000316039775630404,0.000475626271585416,0.000761122825446468,0.000814816903847166,0.000578652080468251,0.000157642005236617,0.000152603615623742,3.79131793396399e-06,7.51171055499296e-11,3.17899382836251e-15,5.96544729558909e-16,2.29881835031841e-21,5.80666661659396e-32,9.62751627134557e-48,6.45601148716749e-65,2.03894997743999e-84,4.22210611955478e-109,5.68993288800346e-128,4.55423370733433e-148,2.3893730364403e-173,9.74271181456202e-204,3.23808866743861e-233,4.50467722508192e-267],[1.98596665642571e-07,4.68000995544877e-05,0.000509090380019503,0.000931319298108951,0.0012891264719117,0.00255519224543074,0.00194691180666576,0.00139812985945186,0.000387322200480871,0.000138389622336007,1.12962183753118e-05,4.51541808458966e-08,2.61431851364547e-08,1.00740783248176e-13,2.57450769746435e-24,2.19535111827777e-35,1.05790746618256e-49,3.3419444921475e-69,1.11809791229766e-85,1.36529655130866e-100,1.09278610207721e-120,5.81233702380819e-146,2.55947654424468e-171,5.43203463930242e-200,7.55679638075802e-234],[2.0971717790889e-11,2.45146152673218e-05,0.00101641626246237,0.0012023963529308,0.00431348854715178,0.00596725675304324,0.00681722538407796,0.00633846543983796,0.00269032102983323,0.00186745629509051,0.000257016293377211,6.68099172102812e-05,2.23554936688109e-05,9.43773499495428e-11,9.51160167439667e-16,6.99272785694416e-25,3.37970089697892e-39,2.8443677530022e-53,5.21415549375954e-63,6.36695308508056e-78,5.10133947007011e-98,2.57749276005559e-119,8.34462177624758e-143,1.77101149161936e-171,2.46374961111803e-205],[2.18739954530607e-13,8.27469962250889e-07,0.00068247158549802,0.00129956984621859,0.00404868380339934,0.0102606158585002,0.0148087501034371,0.0152333593380476,0.0125919827053159,0.00699472312030991,0.00241781085109058,0.000326864762138592,1.64609783322298e-05,5.34463376690191e-06,5.93226571697135e-10,5.36558733277978e-19,1.90647639364607e-28,2.53683060667389e-35,4.72578158624974e-45,5.77104072981931e-60,3.31061440322712e-77,1.63303711875425e-95,5.28749916069209e-119,1.12218648450566e-147,1.56113414734455e-181],[2.95266849169562e-16,2.85416661335113e-08,8.90740945415943e-05,0.00122580118110615,0.0046414034091961,0.0107569609558745,0.0239117407486324,0.0345320261172182,0.034266290423328,0.025908337162062,0.0111683859733492,0.00396670124627087,0.00103938025934649,0.000215424763750705,2.74471340647353e-07,1.04918685044655e-11,1.59258336217706e-17,4.46886513511614e-22,8.32438239306983e-32,5.50420318373213e-45,4.07127773250312e-58,2.01105763346145e-76,6.51146592352183e-100,1.38195370474173e-128,2.93245589422197e-157],[5.39964840899981e-19,5.73783566121195e-10,4.95237822889483e-06,0.000356036530448518,0.00364161528795007,0.0112329442468965,0.0252826501600385,0.0504539247890418,0.0644047045560643,0.0565147591080807,0.0365725560107204,0.0168018818177516,0.00511766159567819,0.00114102783214639,0.000134221597433633,3.14802960871893e-07,5.45629826051782e-09,1.53072361775968e-13,1.41245242401531e-22,1.29303645153607e-30,9.74415078109988e-44,4.81324295289534e-62,1.55844700619099e-85,4.43400824553211e-109,2.84966144455127e-127],[8.08171755432348e-23,9.9520282620662e-12,8.72151014566793e-06,0.000237656044981198,0.00242087074777216,0.0074732039291603,0.022384642393563,0.0557037311872487,0.0912557128673284,0.111817977555221,0.0876421013472796,0.0510910440320731,0.0211567749509494,0.00631968870616185,0.00125306808856439,0.000249914243381994,3.94877458670163e-05,1.74347212052722e-09,5.23218109941235e-13,6.0146277187387e-21,4.53254441838339e-34,2.23890921563308e-52,8.54657858913127e-71,8.37418644669075e-84,5.38198765821467e-102],[1.77556608338451e-26,4.68244714950935e-15,5.72836682532587e-08,5.26618049520792e-05,0.000520971666093416,0.00358304354991811,0.0146024372316178,0.0418007880446392,0.0937576081086299,0.158070989476187,0.157995985258691,0.117472705761148,0.068309234060372,0.0258290121813925,0.00614303473251792,0.00140371367897623,0.000338744362715045,2.87186516456263e-05,4.72966243885067e-08,5.43755630957161e-16,4.12176724282202e-29,8.46961929462488e-42,3.13499804301411e-50,3.07380889542921e-63,1.97549954784854e-81],[3.73881979289422e-28,4.1820126805177e-15,3.42658639075899e-07,0.000213096739325229,4.3973222484619e-05,0.00128105280915237,0.00620857351658763,0.0236609682820968,0.0673959425125629,0.142023226278179,0.202085537513268,0.206177388365867,0.142910430049431,0.0718865969637724,0.0247087506133945,0.0047660988724505,0.0012606718949474,0.000106045234620241,1.61011212783354e-07,3.22788996902712e-14,9.2123618335293e-20,1.49536811118862e-26,2.23643511113796e-34,2.19278426565827e-47,1.4092757463806e-65],[3.96017717925257e-29,3.86645225973798e-16,2.47948115467607e-08,1.04690276048912e-05,9.33739859692154e-08,0.000133111110976272,0.001505336639134,0.0100449990636382,0.0356202331310127,0.105081641208442,0.19439947445164,0.245025075795557,0.223771691425712,0.143465906225543,0.0646419650813796,0.0193175819154857,0.00436838485724641,0.000892066964783552,3.18041827586292e-05,1.69881000386951e-07,1.59463339843398e-12,2.07291272771527e-15,3.10070286927428e-23,3.04018320512746e-36,1.95388872614587e-54],[9.27339116358244e-35,9.03850964769535e-22,5.77463739774986e-14,2.41842863581794e-11,3.88926919292566e-09,0.000100098427596149,0.00050944399843051,0.00360939499961912,0.0136893590316382,0.0543814561636372,0.125589050531115,0.214168898320003,0.252702330848491,0.212680258807483,0.12368371559332,0.0571604244308721,0.0148492657453032,0.00428301746930206,0.000533538675367115,2.75095327364781e-05,2.44919907449026e-06,5.58559793676326e-09,8.35504525271334e-17,8.19197108729206e-30,5.26488017941314e-48],[4.22630081869033e-45,4.11918272416941e-32,2.63161451387286e-24,3.34109926960455e-18,4.16151119979132e-09,3.41162097549106e-05,8.99545384914694e-05,0.000537014515485226,0.00565794795375072,0.0201673293936082,0.0650968150283119,0.13459457027411,0.216709166154952,0.233737316709245,0.179942550046156,0.0954030715862073,0.0358967231160644,0.00978785209447172,0.00200550639931911,0.000394946604594165,0.000128582191560498,2.92511773464097e-07,4.37544553451306e-15,4.29280429019131e-28,6.30606127729647e-38],[3.74346108554796e-60,3.64858019840399e-47,6.70536258912719e-37,1.26938304068891e-22,1.58065154101849e-13,1.29158993030982e-09,1.61465392359957e-06,9.08018401722032e-05,0.000992221126994708,0.00512632101729583,0.0213674249325609,0.0609771333181536,0.127073954333846,0.182795988224094,0.182286136161845,0.128019925458391,0.0639035259991266,0.0214065384771248,0.00490871234422249,0.000973054976315337,4.01022180114187e-05,5.72791275188721e-09,9.59043068097496e-17,2.97039793505775e-18,6.78914545169897e-25],[6.44423115594109e-80,1.76612292089083e-65,4.93647515191911e-46,9.37777211109363e-32,1.1698365693386e-22,1.41425275871238e-13,6.22698386517161e-07,2.62951805516803e-05,0.000279527597873221,0.00157860429446793,0.00602574560623074,0.018752616701671,0.0543021443993639,0.103895733283513,0.132977779790837,0.128374603771506,0.0753939336135714,0.0355212183761688,0.0120559569069517,0.00206194514165532,0.000441096970059341,2.73484240954569e-07,1.78469373193845e-08,6.21521805008958e-10,1.42055105743125e-16],[7.68729358424888e-104,2.44560642444526e-79,7.08775698856484e-60,1.3645720073284e-45,1.85668279000812e-30,1.24717099176323e-18,5.51514928616487e-12,3.59878422919363e-08,7.11836377827706e-06,0.000206159016212575,0.000677559737625132,0.00454096610629842,0.0184411701444843,0.0423810497069583,0.0716411419156444,0.0888352172498334,0.0766481090433616,0.0426458883542985,0.0175314894554273,0.00410447707852974,0.00104229547613349,0.000236141437094331,7.29387726175071e-05,2.52746047466519e-06,5.77674769488895e-13],[1.54348158058503e-122,6.82438055278491e-98,2.1764367130903e-78,3.10524776593914e-57,3.18217737427384e-40,2.1376226328524e-28,2.96521296109842e-21,3.35038012197265e-15,1.97523298476773e-08,0.000133046448270912,0.000137471144441657,0.000903339786529172,0.00339562223667828,0.0128631814378422,0.0284907613212608,0.0459004645516009,0.0504343969800885,0.0368454386741711,0.020295131275482,0.00834622530427719,0.0025276260264015,0.000264485978452582,7.42808296982338e-05,2.97754176124738e-07,5.54723640489585e-14],[8.37072222983159e-146,6.47496277221125e-121,6.61607426510692e-94,1.03435132478381e-71,1.05997695834429e-54,7.26629325820164e-43,3.44825400430606e-33,1.16750358152517e-20,8.87833074222978e-12,6.03705086913571e-08,3.0979146677307e-07,0.000111121155685573,0.000546159808841194,0.00234731614653475,0.00811613899679378,0.01524872508983,0.0224969610775458,0.0222048655285225,0.0165153003974538,0.00796127257796058,0.00238983759119942,0.000568475706754498,0.000111703499381798,1.37762505288305e-06,7.78323799691101e-13],[5.8174658995359e-173,1.79576621930351e-140,4.28308842051526e-113,6.69614350285288e-91,6.86338053828388e-74,5.70790798535558e-61,9.45947538521377e-43,1.05207977853767e-28,8.00037164310086e-20,5.43819520219381e-16,1.1447107087918e-12,7.6069411229749e-08,1.88824406030061e-05,0.000282589139759991,0.0013566840600888,0.0043332775154336,0.00841566316788047,0.0115467512308974,0.00989353732973917,0.00702460127430449,0.00248407446365753,0.000835282652146337,0.00019689042809369,3.98684425041365e-05,1.13279935581737e-06],[6.20932658184517e-197,2.25939039635564e-164,5.3888801122331e-137,8.42494444063686e-115,9.85285809081692e-98,9.81016236238191e-75,1.65677535778078e-55,1.84264779117487e-41,1.40112690321962e-32,9.7734482363793e-29,8.84275975640076e-21,5.42678565856387e-14,3.55725848792818e-09,2.50688031203498e-05,0.00037257061396806,0.000652072224755515,0.00220596575329706,0.00390824530472625,0.00383520710782124,0.00356746514374532,0.00198660746096792,0.000847086231459706,0.000420124430561813,0.000206161951735914,0.000139297549047621],[1.51834614100114e-225,5.52481278783358e-193,1.31772509679596e-165,2.06533081524484e-143,1.29673322046541e-116,3.33931942363595e-92,5.63955821152384e-73,6.27221302516466e-59,4.76902554018659e-50,2.05571003722159e-43,6.4564082722214e-32,9.97509200941749e-22,2.50203874102328e-11,5.38184636508631e-06,2.28640215213753e-05,5.21287951737725e-05,0.000417456509937771,0.000641775046044822,0.00116730324640233,0.00158181795727826,0.00100667520344559,0.00068510033767778,0.000277599060128235,4.61300248449355e-05,4.35426880886139e-05],[7.215756742882e-259,2.6256005832423e-226,6.26274913779807e-199,2.18370540673512e-168,8.57861654858231e-139,2.20914667283235e-114,3.73088057671666e-95,4.14938831485597e-81,3.15554639168223e-72,1.05662622923261e-59,1.43227275358807e-42,5.4931932302565e-27,1.4524397658288e-16,3.79389505892861e-11,2.20909001199427e-10,3.05546580679959e-09,4.35913186858356e-06,3.31707747133925e-05,0.000430542207310076,0.000586742208506192,0.000307810148878761,0.000253683181760847,0.000409041984564786,7.35977011753656e-06,5.00851432147429e-07],[6.66465670367594e-297,2.42507689762034e-264,4.68473831753259e-230,2.80766623244054e-195,1.10298265871409e-165,2.84037672197857e-141,4.79691936622069e-122,5.33497231006159e-108,2.51418740546229e-97,2.69430717937953e-73,1.56978231833671e-52,6.05179120216435e-37,1.68469487817489e-26,5.83767588635578e-21,4.46168128161558e-20,1.47718573329587e-15,3.03760156043252e-12,1.11175862458942e-08,4.77448480759622e-07,1.37527946349091e-05,3.27369657863926e-05,4.5069603918654e-05,2.18665863961594e-05,2.45790160940866e-07,2.16743981653795e-12],[0,1.28033478220917e-301,1.17063504610327e-261,7.01587297133337e-227,2.75616313180305e-197,7.09761020470235e-173,1.1986662995098e-153,1.33312158277165e-139,6.45923897222376e-114,5.73686958901279e-88,3.34560845825795e-67,1.30290708566137e-51,3.98283248267262e-41,1.95445068709012e-35,2.5163612212255e-34,2.13073194576148e-26,2.2103622973781e-22,1.36299881951402e-16,5.4223823620928e-13,9.64890337003241e-07,4.75103088380026e-05,1.74076683148872e-08,2.3982844765835e-05,4.12736376796499e-05,4.65593304437425e-10]],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"surface\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":[]},\"yaxis\":{\"title\":[]},\"zaxis\":{\"title\":[]}},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"colorbar\":{\"title\":\"\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"x\":[-4.11222323909044,-3.76275945114626,-3.41329566320208,-3.0638318752579,-2.71436808731372,-2.36490429936954,-2.01544051142536,-1.66597672348118,-1.316512935537,-0.967049147592815,-0.617585359648634,-0.268121571704453,0.0813422162397277,0.430806004183909,0.780269792128089,1.12973358007227,1.47919736801645,1.82866115596063,2.17812494390481,2.52758873184899,2.87705251979317,3.22651630773736,3.57598009568154,3.92544388362572,4.2749076715699],\"y\":[-4.32381413035071,-3.95784855910568,-3.59188298786066,-3.22591741661563,-2.85995184537061,-2.49398627412558,-2.12802070288055,-1.76205513163553,-1.3960895603905,-1.03012398914547,-0.664158417900448,-0.298192846655422,0.0677727245896049,0.433738295834631,0.799703867079657,1.16566943832468,1.53163500956971,1.89760058081474,2.26356615205976,2.62953172330479,2.99549729454982,3.36146286579484,3.72742843703987,4.09339400828489,4.45935957952992],\"z\":[[3.40651108352134e-09,1.41934533126133e-08,8.54786240302624e-08,0.000130831863209747,1.94776526567504e-06,1.49783603255103e-06,1.1170089576991e-08,6.01986954373106e-12,1.16817732135535e-10,1.50245544102462e-14,4.62944974810078e-23,4.83295093316628e-35,2.28048400569722e-45,8.78734985512727e-51,2.21962779786052e-61,3.67504668072926e-77,3.98846726370328e-98,2.83738455537123e-124,1.16702068177763e-153,1.56588648116791e-183,5.9460095882168e-217,3.04648237511363e-242,1.0476778777822e-272,2.36166828733039e-308,0],[4.39462560522794e-05,6.94943900679841e-05,1.04291502349634e-05,5.75970473307081e-05,0.000176906604268204,0.000370540209494104,1.55421446807263e-05,8.30475843056959e-07,1.53069632670964e-05,7.14342142452053e-09,1.13543480848847e-15,2.9038893623358e-25,2.64561847471716e-28,1.01950388004498e-33,2.57520093025581e-44,4.26377054766638e-60,4.6281649856682e-81,2.42118913413043e-104,5.00622985414094e-129,7.25515626339583e-159,3.68875889960785e-180,1.93530271945878e-205,6.65546074071442e-236,5.25171440440826e-271,5.21884573812369e-305],[9.55774448047614e-05,0.000177213516750328,0.000316039775630404,0.000475626271585416,0.000761122825446468,0.000814816903847166,0.000578652080468251,0.000157642005236617,0.000152603615623742,3.79131793396399e-06,7.51171055499296e-11,3.17899382836251e-15,5.96544729558909e-16,2.29881835031841e-21,5.80666661659396e-32,9.62751627134557e-48,6.45601148716749e-65,2.03894997743999e-84,4.22210611955478e-109,5.68993288800346e-128,4.55423370733433e-148,2.3893730364403e-173,9.74271181456202e-204,3.23808866743861e-233,4.50467722508192e-267],[1.98596665642571e-07,4.68000995544877e-05,0.000509090380019503,0.000931319298108951,0.0012891264719117,0.00255519224543074,0.00194691180666576,0.00139812985945186,0.000387322200480871,0.000138389622336007,1.12962183753118e-05,4.51541808458966e-08,2.61431851364547e-08,1.00740783248176e-13,2.57450769746435e-24,2.19535111827777e-35,1.05790746618256e-49,3.3419444921475e-69,1.11809791229766e-85,1.36529655130866e-100,1.09278610207721e-120,5.81233702380819e-146,2.55947654424468e-171,5.43203463930242e-200,7.55679638075802e-234],[2.0971717790889e-11,2.45146152673218e-05,0.00101641626246237,0.0012023963529308,0.00431348854715178,0.00596725675304324,0.00681722538407796,0.00633846543983796,0.00269032102983323,0.00186745629509051,0.000257016293377211,6.68099172102812e-05,2.23554936688109e-05,9.43773499495428e-11,9.51160167439667e-16,6.99272785694416e-25,3.37970089697892e-39,2.8443677530022e-53,5.21415549375954e-63,6.36695308508056e-78,5.10133947007011e-98,2.57749276005559e-119,8.34462177624758e-143,1.77101149161936e-171,2.46374961111803e-205],[2.18739954530607e-13,8.27469962250889e-07,0.00068247158549802,0.00129956984621859,0.00404868380339934,0.0102606158585002,0.0148087501034371,0.0152333593380476,0.0125919827053159,0.00699472312030991,0.00241781085109058,0.000326864762138592,1.64609783322298e-05,5.34463376690191e-06,5.93226571697135e-10,5.36558733277978e-19,1.90647639364607e-28,2.53683060667389e-35,4.72578158624974e-45,5.77104072981931e-60,3.31061440322712e-77,1.63303711875425e-95,5.28749916069209e-119,1.12218648450566e-147,1.56113414734455e-181],[2.95266849169562e-16,2.85416661335113e-08,8.90740945415943e-05,0.00122580118110615,0.0046414034091961,0.0107569609558745,0.0239117407486324,0.0345320261172182,0.034266290423328,0.025908337162062,0.0111683859733492,0.00396670124627087,0.00103938025934649,0.000215424763750705,2.74471340647353e-07,1.04918685044655e-11,1.59258336217706e-17,4.46886513511614e-22,8.32438239306983e-32,5.50420318373213e-45,4.07127773250312e-58,2.01105763346145e-76,6.51146592352183e-100,1.38195370474173e-128,2.93245589422197e-157],[5.39964840899981e-19,5.73783566121195e-10,4.95237822889483e-06,0.000356036530448518,0.00364161528795007,0.0112329442468965,0.0252826501600385,0.0504539247890418,0.0644047045560643,0.0565147591080807,0.0365725560107204,0.0168018818177516,0.00511766159567819,0.00114102783214639,0.000134221597433633,3.14802960871893e-07,5.45629826051782e-09,1.53072361775968e-13,1.41245242401531e-22,1.29303645153607e-30,9.74415078109988e-44,4.81324295289534e-62,1.55844700619099e-85,4.43400824553211e-109,2.84966144455127e-127],[8.08171755432348e-23,9.9520282620662e-12,8.72151014566793e-06,0.000237656044981198,0.00242087074777216,0.0074732039291603,0.022384642393563,0.0557037311872487,0.0912557128673284,0.111817977555221,0.0876421013472796,0.0510910440320731,0.0211567749509494,0.00631968870616185,0.00125306808856439,0.000249914243381994,3.94877458670163e-05,1.74347212052722e-09,5.23218109941235e-13,6.0146277187387e-21,4.53254441838339e-34,2.23890921563308e-52,8.54657858913127e-71,8.37418644669075e-84,5.38198765821467e-102],[1.77556608338451e-26,4.68244714950935e-15,5.72836682532587e-08,5.26618049520792e-05,0.000520971666093416,0.00358304354991811,0.0146024372316178,0.0418007880446392,0.0937576081086299,0.158070989476187,0.157995985258691,0.117472705761148,0.068309234060372,0.0258290121813925,0.00614303473251792,0.00140371367897623,0.000338744362715045,2.87186516456263e-05,4.72966243885067e-08,5.43755630957161e-16,4.12176724282202e-29,8.46961929462488e-42,3.13499804301411e-50,3.07380889542921e-63,1.97549954784854e-81],[3.73881979289422e-28,4.1820126805177e-15,3.42658639075899e-07,0.000213096739325229,4.3973222484619e-05,0.00128105280915237,0.00620857351658763,0.0236609682820968,0.0673959425125629,0.142023226278179,0.202085537513268,0.206177388365867,0.142910430049431,0.0718865969637724,0.0247087506133945,0.0047660988724505,0.0012606718949474,0.000106045234620241,1.61011212783354e-07,3.22788996902712e-14,9.2123618335293e-20,1.49536811118862e-26,2.23643511113796e-34,2.19278426565827e-47,1.4092757463806e-65],[3.96017717925257e-29,3.86645225973798e-16,2.47948115467607e-08,1.04690276048912e-05,9.33739859692154e-08,0.000133111110976272,0.001505336639134,0.0100449990636382,0.0356202331310127,0.105081641208442,0.19439947445164,0.245025075795557,0.223771691425712,0.143465906225543,0.0646419650813796,0.0193175819154857,0.00436838485724641,0.000892066964783552,3.18041827586292e-05,1.69881000386951e-07,1.59463339843398e-12,2.07291272771527e-15,3.10070286927428e-23,3.04018320512746e-36,1.95388872614587e-54],[9.27339116358244e-35,9.03850964769535e-22,5.77463739774986e-14,2.41842863581794e-11,3.88926919292566e-09,0.000100098427596149,0.00050944399843051,0.00360939499961912,0.0136893590316382,0.0543814561636372,0.125589050531115,0.214168898320003,0.252702330848491,0.212680258807483,0.12368371559332,0.0571604244308721,0.0148492657453032,0.00428301746930206,0.000533538675367115,2.75095327364781e-05,2.44919907449026e-06,5.58559793676326e-09,8.35504525271334e-17,8.19197108729206e-30,5.26488017941314e-48],[4.22630081869033e-45,4.11918272416941e-32,2.63161451387286e-24,3.34109926960455e-18,4.16151119979132e-09,3.41162097549106e-05,8.99545384914694e-05,0.000537014515485226,0.00565794795375072,0.0201673293936082,0.0650968150283119,0.13459457027411,0.216709166154952,0.233737316709245,0.179942550046156,0.0954030715862073,0.0358967231160644,0.00978785209447172,0.00200550639931911,0.000394946604594165,0.000128582191560498,2.92511773464097e-07,4.37544553451306e-15,4.29280429019131e-28,6.30606127729647e-38],[3.74346108554796e-60,3.64858019840399e-47,6.70536258912719e-37,1.26938304068891e-22,1.58065154101849e-13,1.29158993030982e-09,1.61465392359957e-06,9.08018401722032e-05,0.000992221126994708,0.00512632101729583,0.0213674249325609,0.0609771333181536,0.127073954333846,0.182795988224094,0.182286136161845,0.128019925458391,0.0639035259991266,0.0214065384771248,0.00490871234422249,0.000973054976315337,4.01022180114187e-05,5.72791275188721e-09,9.59043068097496e-17,2.97039793505775e-18,6.78914545169897e-25],[6.44423115594109e-80,1.76612292089083e-65,4.93647515191911e-46,9.37777211109363e-32,1.1698365693386e-22,1.41425275871238e-13,6.22698386517161e-07,2.62951805516803e-05,0.000279527597873221,0.00157860429446793,0.00602574560623074,0.018752616701671,0.0543021443993639,0.103895733283513,0.132977779790837,0.128374603771506,0.0753939336135714,0.0355212183761688,0.0120559569069517,0.00206194514165532,0.000441096970059341,2.73484240954569e-07,1.78469373193845e-08,6.21521805008958e-10,1.42055105743125e-16],[7.68729358424888e-104,2.44560642444526e-79,7.08775698856484e-60,1.3645720073284e-45,1.85668279000812e-30,1.24717099176323e-18,5.51514928616487e-12,3.59878422919363e-08,7.11836377827706e-06,0.000206159016212575,0.000677559737625132,0.00454096610629842,0.0184411701444843,0.0423810497069583,0.0716411419156444,0.0888352172498334,0.0766481090433616,0.0426458883542985,0.0175314894554273,0.00410447707852974,0.00104229547613349,0.000236141437094331,7.29387726175071e-05,2.52746047466519e-06,5.77674769488895e-13],[1.54348158058503e-122,6.82438055278491e-98,2.1764367130903e-78,3.10524776593914e-57,3.18217737427384e-40,2.1376226328524e-28,2.96521296109842e-21,3.35038012197265e-15,1.97523298476773e-08,0.000133046448270912,0.000137471144441657,0.000903339786529172,0.00339562223667828,0.0128631814378422,0.0284907613212608,0.0459004645516009,0.0504343969800885,0.0368454386741711,0.020295131275482,0.00834622530427719,0.0025276260264015,0.000264485978452582,7.42808296982338e-05,2.97754176124738e-07,5.54723640489585e-14],[8.37072222983159e-146,6.47496277221125e-121,6.61607426510692e-94,1.03435132478381e-71,1.05997695834429e-54,7.26629325820164e-43,3.44825400430606e-33,1.16750358152517e-20,8.87833074222978e-12,6.03705086913571e-08,3.0979146677307e-07,0.000111121155685573,0.000546159808841194,0.00234731614653475,0.00811613899679378,0.01524872508983,0.0224969610775458,0.0222048655285225,0.0165153003974538,0.00796127257796058,0.00238983759119942,0.000568475706754498,0.000111703499381798,1.37762505288305e-06,7.78323799691101e-13],[5.8174658995359e-173,1.79576621930351e-140,4.28308842051526e-113,6.69614350285288e-91,6.86338053828388e-74,5.70790798535558e-61,9.45947538521377e-43,1.05207977853767e-28,8.00037164310086e-20,5.43819520219381e-16,1.1447107087918e-12,7.6069411229749e-08,1.88824406030061e-05,0.000282589139759991,0.0013566840600888,0.0043332775154336,0.00841566316788047,0.0115467512308974,0.00989353732973917,0.00702460127430449,0.00248407446365753,0.000835282652146337,0.00019689042809369,3.98684425041365e-05,1.13279935581737e-06],[6.20932658184517e-197,2.25939039635564e-164,5.3888801122331e-137,8.42494444063686e-115,9.85285809081692e-98,9.81016236238191e-75,1.65677535778078e-55,1.84264779117487e-41,1.40112690321962e-32,9.7734482363793e-29,8.84275975640076e-21,5.42678565856387e-14,3.55725848792818e-09,2.50688031203498e-05,0.00037257061396806,0.000652072224755515,0.00220596575329706,0.00390824530472625,0.00383520710782124,0.00356746514374532,0.00198660746096792,0.000847086231459706,0.000420124430561813,0.000206161951735914,0.000139297549047621],[1.51834614100114e-225,5.52481278783358e-193,1.31772509679596e-165,2.06533081524484e-143,1.29673322046541e-116,3.33931942363595e-92,5.63955821152384e-73,6.27221302516466e-59,4.76902554018659e-50,2.05571003722159e-43,6.4564082722214e-32,9.97509200941749e-22,2.50203874102328e-11,5.38184636508631e-06,2.28640215213753e-05,5.21287951737725e-05,0.000417456509937771,0.000641775046044822,0.00116730324640233,0.00158181795727826,0.00100667520344559,0.00068510033767778,0.000277599060128235,4.61300248449355e-05,4.35426880886139e-05],[7.215756742882e-259,2.6256005832423e-226,6.26274913779807e-199,2.18370540673512e-168,8.57861654858231e-139,2.20914667283235e-114,3.73088057671666e-95,4.14938831485597e-81,3.15554639168223e-72,1.05662622923261e-59,1.43227275358807e-42,5.4931932302565e-27,1.4524397658288e-16,3.79389505892861e-11,2.20909001199427e-10,3.05546580679959e-09,4.35913186858356e-06,3.31707747133925e-05,0.000430542207310076,0.000586742208506192,0.000307810148878761,0.000253683181760847,0.000409041984564786,7.35977011753656e-06,5.00851432147429e-07],[6.66465670367594e-297,2.42507689762034e-264,4.68473831753259e-230,2.80766623244054e-195,1.10298265871409e-165,2.84037672197857e-141,4.79691936622069e-122,5.33497231006159e-108,2.51418740546229e-97,2.69430717937953e-73,1.56978231833671e-52,6.05179120216435e-37,1.68469487817489e-26,5.83767588635578e-21,4.46168128161558e-20,1.47718573329587e-15,3.03760156043252e-12,1.11175862458942e-08,4.77448480759622e-07,1.37527946349091e-05,3.27369657863926e-05,4.5069603918654e-05,2.18665863961594e-05,2.45790160940866e-07,2.16743981653795e-12],[0,1.28033478220917e-301,1.17063504610327e-261,7.01587297133337e-227,2.75616313180305e-197,7.09761020470235e-173,1.1986662995098e-153,1.33312158277165e-139,6.45923897222376e-114,5.73686958901279e-88,3.34560845825795e-67,1.30290708566137e-51,3.98283248267262e-41,1.95445068709012e-35,2.5163612212255e-34,2.13073194576148e-26,2.2103622973781e-22,1.36299881951402e-16,5.4223823620928e-13,9.64890337003241e-07,4.75103088380026e-05,1.74076683148872e-08,2.3982844765835e-05,4.12736376796499e-05,4.65593304437425e-10]],\"type\":\"surface\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\nFigure 3: Imagem 3-D Interativa de uma distribui√ß√£o Multivariada Normal\n\n\n\nMetropolis e Metropolis-Hastings\nO primeiro algoritmo MCMC amplamente utilizado para gerar amostras de correntes Markov foi origin√°rio na f√≠sica na d√©cada de 1950 (inclusive uma rela√ß√£o muito pr√≥xima com a bomba at√¥mica no projeto Manhattan) e chama-se Metropolis (Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller, 1953) em homenagem ao primeiro autor Nicholas Metropolis (figura 4). Em s√≠ntese, o algoritmo de Metropolis √© uma adapta√ß√£o de um passeio aleat√≥rio (random walk) com uma regra de aceita√ß√£o/rejei√ß√£o para convergir √† distribui√ß√£o-alvo.\nO algorimo de Metropolis usa uma distribui√ß√£o de propostas \\(J_t(\\theta^*)\\) (\\(J\\) quer dizer jumping distribution e \\(t\\) indica em qual estado da corrente Markov estamos) para definir pr√≥ximos valores da distribui√ß√£o \\(P^*(\\theta^* | \\text{data})\\). Essa distribui√ß√£o deve ser sim√©trica:\n\\[\nJ_t (\\theta^* | \\theta^{t-1}) = J_t(\\theta^{t-1}|\\theta^*).\n\\]\nNa d√©cada de 1970, surgiu um generaliza√ß√£o do algoritmo de Metropolis que n√£o necessita que as distribui√ß√µes de proposta sejam sim√©tricas. A generaliza√ß√£o foi proposta por Wilfred Keith Hastings (Hastings, 1970) (figura 4) e chama-se algoritmo de Metropolis-Hastings.\n\n\n\nFigure 4: Da esquerda para direita: Nicholas Metropolis e Wilfred Hastings ‚Äì Figuras de https://www.wikipedia.org\n\n\n\nAlgoritmo de Metropolis\nA ess√™ncia do algoritmo √© um passeio aleat√≥rio (random walk) pelo espa√ßo amostral dos par√¢metros, onde a probabilidade da corrente Markov mudar de estado √© definida como:\n\\[\nP_{\\text{mudar}} = \\min\\left({\\frac{P (\\theta_{\\text{proposto}})}{P (\\theta_{\\text{atual}})}},1\\right).\n\\]\nIsso quer dizer a corrente Markov somente mudar√° para um novo estado em duas condi√ß√µes:\nQuando a probabilidade dos par√¢metros propostos pelo passeio aleat√≥rio \\(P(\\theta_{\\text{proposto}})\\) √© maior que a probabilidade dos par√¢metros do estado atual \\(P(\\theta_{\\text{atual}})\\), mudamos com 100% de probabilidade. Vejam que se \\(P(\\theta_{\\text{proposto}}) > P(\\theta_{\\text{atual}})\\) ent√£o a fun√ß√£o \\(\\min\\) escolhe o valor 1 que quer dizer 100%.\nQuando a probabilidade dos par√¢metros propostos pelo passeio aleat√≥rio \\(P(\\theta_{\\text{proposto}})\\) √© menor que a probabilidade dos par√¢metros do estado atual \\(P(\\theta_{\\text{atual}})\\), mudamos com probabilidade igual a propor√ß√£o dessa diferen√ßa. Vejam que se \\(P(\\theta_{\\text{proposto}}) < P(\\theta_{\\text{atual}})\\) ent√£o a fun√ß√£o \\(\\min\\) n√£o escolhe o valor 1, mas sim o valor \\(\\frac{P (\\theta_{\\text{proposto}})}{P (\\theta_{\\text{atual}})}\\) que equivale a propor√ß√£o da probabilidade dos par√¢metros propostos pela probabilidade dos par√¢metros do estado atual.\nDe qualquer maneira, a cada itera√ß√£o do algoritmo de Metropolis, mesmo que a corrente muda de estado ou n√£o, amostramos o par√¢metro \\(\\theta\\) de qualquer maneira. Ou seja, se a corrente n√£o mudar em um certo estado \\(\\theta\\) ser√° amostrado duas vezes (ou mais caso a corrente fique estacion√°ria no mesmo estado).\nO algoritmo de Metropolis-Hastings pode ser descrito na seguinte maneira3 (\\(\\theta\\) √© o par√¢metro, ou conjunto de par√¢metros, de interesse e \\(y\\) s√£o os dados):\nDefina um ponto inicial \\(\\theta^0\\) do qual \\(p(\\theta^0|y) > 0\\), ou amostre-o de uma distribui√ß√£o inicial \\(p_0 (\\theta)\\). \\(p_0(\\theta)\\) pode ser uma distribui√ß√£o normal ou uma distribui√ß√£o pr√©via de \\(\\theta\\) (\\(p(\\theta)\\)).\nPara \\(t = 1, 2, \\dots\\):\nAmostra uma proposta \\(\\theta^*\\) de uma distribui√ß√£o de propostas no tempo \\(t\\), \\(J_t (\\theta^* | \\theta^{t-1})\\).\nCalcule a propor√ß√£o das probabilidades:\nMetropolis: \\(r = \\frac{p(\\theta^* | y)}{p(\\theta^{t-1} | y)}\\)\nMetropolis-Hastings: \\(r = \\frac{\\frac{p(\\theta^* | y)}{J_t(\\theta^*|\\theta^{t-1})}}{\\frac{p(\\theta^{t-1} | y)}{J_t(\\theta^{t-1}|\\theta^*)}}\\)\n\nDesigne:\n\\[\\theta^t =\n  \\begin{cases}\n  \\theta^* & \\text{com probabilidade $\\min(r,1)$}\\\\\n  \\theta^{t-1} & \\text{caso contr√°rio}\n  \\end{cases}\\]\n\nLimita√ß√µes do Algoritmo de Metropolis\nAs limita√ß√µes do algoritmo de Metropolis-Hastings s√£o principalmente computacionais. Com propostas geradas aleatoriamente, geralmente leva um grande n√∫mero de itera√ß√µes para entrar em √°reas de densidade posterior mais alta (mais prov√°vel). Mesmo algoritmos de Metropolis-Hastings eficientes √†s vezes aceitam menos de 25% das propostas (Roberts, Gelman, & Gilks, 1997). Em situa√ß√µes dimensionais mais baixas, o poder computacional aumentado pode compensar a efici√™ncia mais baixa at√© certo ponto. Mas em situa√ß√µes de modelagem de dimens√µes mais altas e mais complexas, computadores maiores e mais r√°pidos sozinhos raramente s√£o suficientes para superar o desafio.\nMetropolis ‚Äì Implementa√ß√£o\nNo nosso exemplo did√°tico vamos partir do pressuposto que \\(J_t(\\theta^* | \\theta^{t-1})\\) √© sim√©trico √† \\(J_t (\\theta^* | \\theta^{t-1}) = J_t(\\theta^{t-1}|\\theta^*)\\), portanto vamos apenas demonstrar o algoritmo de Metropolis (e n√£o o algoritmo de Metropolis-Hastings).\nO Stan (Carpenter et al., 2017) (e consequentemente seu ecossistema inteiro de pacotes) n√£o tem implementa√ß√µes de outros algoritmos a n√£o ser o HMC (Hamiltonean Monte Carlo), portanto abaixo criei um amostrador Metropolis para o nosso exemplo did√°tico. No fim ele imprime a porcentagem total de aceita√ß√£o das propostas. Aqui estamos usando a mesma distribui√ß√£o de propostas para tanto \\(X\\) e \\(Y\\): uma distribui√ß√£o uniforme parameterizada com um par√¢metro largura width:\n\\[\nX \\sim \\text{Uniforme} \\left( X - \\frac{\\text{largura}}{2}, X + \\frac{\\text{largura}}{2} \\right) \\\\\nY \\sim \\text{Uniforme} \\left( Y - \\frac{\\text{largura}}{2}, Y + \\frac{\\text{largura}}{2} \\right)\n\\] O pacote mnormt possui algumas funcionalidades para lidar com distribui√ß√µes multivariadas, a fun√ß√£o dmnorm() em especial calcula a fun√ß√£o densidade de probabilidade (FDP)4 de uma distribui√ß√£o normal multivariada, que √© usada no c√°lculo propor√ß√£o das probabilidades \\(r\\):\n\\[\n\\begin{aligned}\nr &= \\frac{\n\\operatorname{FDP}\\left(\n\\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\nx_{\\text{proposto}} \\\\\ny_{\\text{proposto}}\n\\end{bmatrix}\n\\right)\n\\Bigg|\n\\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}, \\mathbf{\\Sigma}\n\\right)\n\\right)}\n{\n\\operatorname{FDP}\\left(\n\\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\nx_{\\text{atual}} \\\\\ny_{\\text{atual}}\n\\end{bmatrix}\n\\right)\n\\Bigg|\n\\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}, \\mathbf{\\Sigma}\n\\right)\n\\right)}\\\\\n&=\\frac{\\operatorname{FDP}_{\\text{proposto}}}{\\operatorname{FDP}_{\\text{atual}}}\\\\\n&= \\exp\\Big(\n\\log\\left(\\operatorname{FDP}_{\\text{proposto}}\\right)\n-\n\\log\\left(\\operatorname{FDP}_{\\text{atual}}\\right)\n\\Big)\n\\end{aligned}\n\\]\n\n\nmetropolis <- function(S, half_width,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho,\n                       start_x, start_y,\n                       seed = 123) {\n   set.seed(seed)\n   Sigma <- diag(2)\n   Sigma[1, 2] <- rho\n   Sigma[2, 1] <- rho\n   draws <- matrix(nrow = S, ncol = 2)\n   x <- start_x\n   y <- start_y\n   accepted <- 0\n   draws[1, 1] <- x\n   draws[1, 2] <- y\n   for (s in 2:S) {\n      x_ <- runif(1, x - half_width, x + half_width)\n      y_ <- runif(1, y - half_width, y + half_width)\n      r <- exp(mnormt::dmnorm(c(x_, y_), mean = c(mu_X, mu_Y), varcov = Sigma, log = TRUE) -\n                        mnormt::dmnorm(c(x, y), mean = c(mu_X, mu_Y), varcov = Sigma, log = TRUE))\n      if (r > runif(1, 0, 1)) {\n        x <- x_\n        y <- y_\n        accepted <- accepted + 1\n      }\n      draws[s, 1] <- x\n      draws[s, 2] <- y\n   }\n   print(paste0(\"Taxa de aceita√ß√£o \", accepted / S))\n   return(draws)\n}\n\n\n\n\n\nn_sim <- 1e4\n\n\n\nVamos executar nosso algoritmo Metropolis com 10,000 itera√ß√µes.\n\n\nX_met <- metropolis(\n  S = n_sim, half_width = 2.75,\n  mu_X = 0, mu_Y = 0,\n  sigma_X = 1, sigma_Y = 1,\n  rho = r,\n  start_x = -2.5, start_y = 2.5\n)\n\n\n[1] \"Taxa de aceita√ß√£o 0.2076\"\n\nhead(X_met, 7)\n\n\n      [,1] [,2]\n[1,] -2.50  2.5\n[2,] -2.50  2.5\n[3,] -2.50  2.5\n[4,] -2.50  2.5\n[5,] -2.50  2.5\n[6,] -1.52  2.9\n[7,]  0.68  1.5\n\nNa nossa primeira execu√ß√£o do algoritmo Metropolis temos como resultado uma matriz X_met com 10,000 linhas e 2 colunas (uma para cada valor de \\(X\\) e \\(Y\\), que passarei a chamar de \\(\\theta_1\\) e \\(\\theta_2\\), respectivamente). Vejam que a aceita√ß√£o das propostas ficou em 20.8%, o esperado para algoritmos Metropolis (em torno de 20-25%) (Roberts, Gelman, & Gilks, 1997).\nPara m√©tricas de converg√™ncia e desempenho vamos usar a fun√ß√£o rstan::monitor() que simula um print(stanfit)5mas para matrizes.\n\n\nres <- monitor(X_met, digits_summary = 1)\n\n\nInference for the input samples (2 chains: each with iter = 10000; warmup = 5000):\n\n     Q5 Q50 Q95 Mean SD  Rhat Bulk_ESS Tail_ESS\nV1 -1.6   0 1.7    0  1     1      952      909\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS > 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat <= 1.05).\n\nneff <- res[, \"n_eff\"]\nreff <- mean(neff / (nrow(X_met))) #  9.5%\n\n\n\nVejam que o n√∫mero de amostras eficientes em rela√ß√£o ao n√∫mero total de itera√ß√µes reff √© 9,5% para todas as itera√ß√µes incluindo warm-up.\nMetropolis ‚Äì Intui√ß√£o Visual\nEu acredito que uma boa intui√ß√£o visual, mesmo que voc√™ n√£o tenha entendido nenhuma f√≥rmula matem√°tica, √© a chave para voc√™ come√ßar a jornada de aprendizagem. Portanto fiz algumas anima√ß√µes com GIFs.\nA anima√ß√£o na figura 5 mostra as 100 primeiras simula√ß√µes do algoritmo Metropolis usado para gerar X_met. Vejam que em diversas itera√ß√µes a proposta √© recusada e o algoritmo amostra os par√¢metros \\(\\theta_1\\) e \\(\\theta_2\\) do estado anterior (que se torna o atual, pois a proposta √© recusada).\nObserva√ß√£o: HPD √© a sigla para Highest Probability Density (que √© o intervalo de 90% de probabilidade da posterior).\n\n\ndf100 <- data.frame(\n    id = rep(1, 100),\n    iter = 1:100,\n    th1 = X_met[1:100, 1],\n    th2 = X_met[1:100, 2],\n    th1l = c(X_met[1, 1], X_met[1:(100 - 1), 1]),\n    th2l = c(X_met[1, 2], X_met[1:(100 - 1), 2])\n)\n\nlabs1 <- c(\"Amostras\", \"Itera√ß√µes do Algoritmo\", \"90% HPD\")\n\np1 <- ggplot() +\n  geom_jitter(data = df100, width = 0.05, height = 0.05,\n             aes(th1, th2, group = id, color = \"1\"), alpha = 0.3) +\n  geom_segment(data = df100, aes(x = th1, xend = th1l, color = \"2\",\n                                 y = th2, yend = th2l)) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"3\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Metropolis\", subtitle = \"100 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"red\", \"forestgreen\", \"blue\"), labels = labs1) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA, NA), linetype = c(0, 1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\nanimate(p1 +\n  transition_reveal(along = iter) +\n  shadow_trail(0.01),\n  # animation options\n  height = 7, width = 7, units = \"in\", res = 300\n)\n\n\n\n\nFigure 5: Anima√ß√£o Metropolis\n\n\n\nNa figura 6 √© poss√≠vel ver como ficaram as primeiras 1.000 simula√ß√µes excluindo 1.000 itera√ß√µes iniciais como warmup.\n\n\n# Take all the 10,000 observations after warmup of 1,000\nwarm <- 1e3\ndfs <- data.frame(\n  th1 = X_met[(warm + 1):nrow(X_met), 1],\n  th2 = X_met[(warm + 1):nrow(X_met), 2]\n)\n\nlabs2 <- c(\"Amostras\", \"90% HPD\")\n\nggplot() +\n  geom_point(data = dfs[1:1000, ],\n             aes(th1, th2, color = \"1\"), alpha = 0.3) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"2\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Metropolis\", subtitle = \"1.000 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"steelblue\", \"blue\"), labels = labs2) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), alpha = c(1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nFigure 6: Primeiras 1.000 simula√ß√µes Metropolis ap√≥s descarte de 1.000 itera√ß√µes como warmup\n\n\n\nE na figura 7 √© poss√≠vel ver as restantes 9.000 simula√ß√µes excluindo 1.000 itera√ß√µes iniciais como warmup.\n\n\n# Show all 10,000 samples\nggplot() +\n  geom_point(data = dfs,\n             aes(th1, th2, color = \"1\"), alpha = 0.3) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"2\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Metropolis\", subtitle = \"10.000 Amostragens\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"steelblue\", \"blue\"), labels = labs2) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), alpha = c(1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nFigure 7: 9.000 simula√ß√µes Metropolis ap√≥s descarte de 1.000 itera√ß√µes como warmup\n\n\n\nGibbs\nPara contornar o problema de baixa taxa de aceita√ß√£o do algoritmo de Metropolis (e Metropolis-Hastings) foi desenvolvido o algoritmo de Gibbs que n√£o possui uma regra de aceita√ß√£o/rejei√ß√£o para a mudan√ßa de estado da corrente Markov. Todas as propostas s√£o aceitas.\nO algoritmo de Gibbs teve ideia original concebida pelo f√≠sico Josiah Willard Gibbs (figura 8), em refer√™ncia a uma analogia entre um algoritmo de amostragem e a f√≠sica estat√≠stica (statistical physics um ramo da f√≠sica que tem sua base em mec√¢nica estat√≠stica statistical mechanics). O algoritmo foi descrito pelos irm√£os Stuart e Donald Geman (figura 8) em 1984 (Geman & Geman, 1984), cerca de oito d√©cadas ap√≥s a morte de Gibbs.\n\n\n\nFigure 8: Da esquerda para direita: Josiah Gibbs,Stuart Geman e Donald Geman ‚Äì Figuras de https://www.wikipedia.org\n\n\n\nO algoritmo de Gibbs √© muito √∫til em espa√ßos amostrais multidimensionais (no qual h√° bem mais que 2 par√¢metros a serem amostrados da probabilidade posterior). Tamb√©m √© conhecido como amostragem condicional alternativa (alternating conditional sampling), pois amostramos sempre um par√¢metro condicionado √† probabilidade dos outros par√¢metros do modelo.\nO algoritmo de Gibbs pode ser visto como um caso especial do algoritmo de Metropolis-Hastings porque todas as propostas s√£o aceitas (Gelman, 1992).\nAlgoritmo de Gibbs\nA ess√™ncia do algoritmo de Gibbs √© a amostragem de par√¢metros condicionada √† outros par√¢metros \\(P(\\theta_1 | \\theta_2, \\dots \\theta_n)\\).\nO algoritmo de Gibbs pode ser descrito na seguinte maneira6 (\\(\\theta\\) √© o par√¢metro, ou conjunto de par√¢metros, de interesse e \\(y\\) s√£o os dados):\nDefina \\(p(\\theta_1), p(\\theta_2), \\dots, p(\\theta_n)\\): a probabilidade pr√©via (prior) de cada um dos par√¢metros \\(\\theta_n\\).\nAmostre um ponto inicial \\(\\theta^0_1, \\theta^0_2, \\dots, \\theta^0_n\\). Geralmente amostramos de uma distribui√ß√£o normal ou de uma distribui√ß√£o especificada como a distribui√ß√£o pr√©via (prior) de \\(\\theta_n\\).\nPara \\(t = 1,2,\\dots\\):\n\\[\\begin{aligned}\n \\theta^t_1 &\\sim p(\\theta_1 | \\theta^0_2, \\dots, \\theta^0_n) \\\\\n \\theta^t_2 &\\sim p(\\theta_2 | \\theta^{t-1}_1, \\dots, \\theta^0_n) \\\\\n &\\vdots \\\\\n \\theta^t_n &\\sim p(\\theta_n | \\theta^{t-1}_1, \\dots, \\theta^{t-1}_{n-1})\n \\end{aligned}\\]\nLimita√ß√µes do Algoritmo de Gibbs\nA principal limita√ß√£o do algoritmo de Gibbs √© com rela√ß√£o a amostragem condicional alternativa.\nSe compararmos com o algoritmo Metropolis (e consequentemente Metropolis-Hastings) temos propostas aleat√≥rias de uma distribui√ß√£o de propostas na qual amostramos cada par√¢metro incondicionalmente √† outros par√¢metros. Para que as propostas nos levem a locais corretos da probabilidade posterior para amostrarmos temos uma regra de aceita√ß√£o/rejei√ß√£o dessas propostas, se n√£o as amostras do algoritmo de Metropolis n√£o se aproximariam √† distribui√ß√£o-alvo de interesse. As mudan√ßas de estado da corrente Markov s√£o ent√£o executadas multidimensionalmente7. Como voc√™ viu nas figuras 5, 6 e 7 de intui√ß√£o visual do algoritmo de Metropolis, em um espa√ßo 2-D (como √© o nosso exemplo did√°tico bivariado normal), quando h√° uma mudan√ßa de estado na corrente Markov, o novo local de proposta considera tanto \\(\\theta_1\\) quanto \\(\\theta_2\\), provocando uma movimenta√ß√£o na diagonal no espa√ßo amostral 2-D.\nNo caso do algoritmo de Gibbs, no nosso exemplo, essa movimenta√ß√£o se d√° apenas em um √∫nico par√¢metro, pois amostramos sequencialmente e condicionalmente √† outros par√¢metros. Isto provoca movimentos horizontais (no caso de \\(\\theta_1\\)) e movimentos verticais (no caso de \\(\\theta_2\\)), mas nunca movimentos diagonais como o que vemos no algoritmo de Metropolis.\nGibbs ‚Äì Implementa√ß√£o\nO Stan (Carpenter et al., 2017) (e consequentemente seu ecossistema inteiro de pacotes) n√£o tem implementa√ß√µes de outros algoritmos a n√£o ser o HMC (Hamiltonian Monte Carlo), portanto abaixo criei um amostrador Gibbs para o nosso exemplo did√°tico.\nAqui temos algumas coisas novas comparando com a implementa√ß√£o do amostrador Metropolis. Primeiro para amostrar condicionalmente os par√¢metros \\(P(\\theta_1 | \\theta_2)\\) e \\(P(\\theta_2 | \\theta_1)\\), precisamos criar duas vari√°veis novas beta (\\(\\beta\\)) e lambda (\\(\\lambda\\)). Essas vari√°veis representam a correla√ß√£o entre \\(X\\) e \\(Y\\) (\\(\\theta_1\\) e \\(\\theta_2\\) respectivamente). E ent√£o usamos essas vari√°veis na amostragem de \\(\\theta_1\\) e \\(\\theta_2\\):\n\\[\n\\begin{aligned}\n\\beta &= \\rho \\cdot \\frac{\\sigma_Y}{\\sigma_X} = \\rho \\\\\n\\lambda &= \\rho \\cdot \\frac{\\sigma_X}{\\sigma_Y} = \\rho \\\\\n\\sigma_{YX} &= 1 - \\rho^2\\\\\n\\sigma_{XY} &= 1 - \\rho^2\\\\\n\\theta_1 &\\sim \\text{Normal} \\bigg( \\mu_X + \\lambda \\cdot (y^* - \\mu_Y), \\sigma_{XY} \\bigg) \\\\\n\\theta_2 &\\sim \\text{Normal} \\bigg( \\mu_y + \\beta \\cdot (x^* - \\mu_X), \\sigma_{YX} \\bigg).\n\\end{aligned}\n\\]\n\n\ngibbs <- function(S,\n                  mu_X = 0, mu_Y = 0,\n                  sigma_X = 1, sigma_Y = 1,\n                  rho,\n                  start_x, start_y,\n                  seed = 123) {\n   set.seed(seed)\n   Sigma <- diag(2)\n   Sigma[1, 2] <- rho\n   Sigma[2, 1] <- rho\n   draws <- matrix(nrow = S, ncol = 2)\n   x <- start_x\n   y <- start_y\n   beta <- rho * sigma_Y / sigma_X\n   lambda <- rho * sigma_X / sigma_Y\n   sqrt1mrho2 <- sqrt(1 - rho^2)\n   sigma_YX <- sigma_Y * sqrt1mrho2\n   sigma_XY <- sigma_X * sqrt1mrho2\n   draws[1, 1] <- x\n   draws[1, 2] <- y\n   for (s in 2:S) {\n     if (s %% 2 == 0) {\n        y <- rnorm(1, mu_Y + beta * (x - mu_X), sigma_YX)\n     }\n     else {\n        x <- rnorm(1, mu_X + lambda * (y - mu_Y), sigma_XY)\n     }\n     draws[s, 1] <- x\n     draws[s, 2] <- y\n   }\n   return(draws)\n}\n\n\n\nVamos executar nosso algoritmo Gibbs com 10,000 itera√ß√µes.\n\n\nX_gibbs <- gibbs(\n  S = n_sim,\n  mu_X = 0, mu_Y = 0,\n  sigma_X = 1, sigma_Y = 1,\n  rho = r,\n  start_x = -2.5, start_y = 2.5\n)\nhead(X_gibbs, 7)\n\n\n      [,1]  [,2]\n[1,] -2.50  2.50\n[2,] -2.50 -2.34\n[3,] -2.01 -2.34\n[4,] -2.01 -0.67\n[5,] -0.49 -0.67\n[6,] -0.49 -0.32\n[7,]  0.77 -0.32\n\nNa nossa primeira execu√ß√£o do algoritmo Gibbs temos como resultado uma matriz X_gibbs com 10,000 linhas e 2 colunas (as mesmas condi√ß√µes j√° mostradas no exemplo anterior com algoritmo Metropolis).\n\n\nres <- monitor(X_gibbs, digits_summary = 1)\n\n\nInference for the input samples (2 chains: each with iter = 10000; warmup = 5000):\n\n     Q5 Q50 Q95 Mean SD  Rhat Bulk_ESS Tail_ESS\nV1 -1.7   0 1.6    0  1     1     1156     1972\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS > 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat <= 1.05).\n\nneff <- res[, \"n_eff\"]\nreff <- mean(neff / (nrow(X_gibbs) / 2)) #  23.2%\n\n\n\nVejam que o n√∫mero de amostras eficientes em rela√ß√£o ao n√∫mero total de itera√ß√µes reff8 √© 23% para todas as itera√ß√µes incluindo warm-up. A efici√™ncia do algoritmo Gibbs, no nosso exemplo did√°tico, √© o mais que dobro da efici√™ncia do algoritmo de Metropolis (9,5% vs 23%).\nGibbs ‚Äì Intui√ß√£o Visual\nA anima√ß√£o na figura 9 mostra as 100 primeiras simula√ß√µes do algoritmo Gibbs usado para gerar X_gibbs. Vejam que aqui n√£o h√° movimenta√ß√£o na diagonal no espa√ßo amostral devido √† amostragem condicional alternativa dos par√¢metros \\(\\theta_1\\) e \\(\\theta_2\\). A movimenta√ß√£o do algoritmo Gibbs no espa√ßo amostral est√° condicionada a apenas um movimento por dimens√£o de par√¢metro (que no nosso exemplo did√°tico 2-D s√£o as dimens√µes horizontais \\(\\theta_1\\) e verticais \\(\\theta_2\\)).\n\n\ndf100 <- data.frame(\n    id = rep(1, 100),\n    iter = 1:100,\n    th1 = X_gibbs[1:100, 1],\n    th2 = X_gibbs[1:100, 2],\n    th1l = c(X_gibbs[1, 1], X_gibbs[1:(100 - 1), 1]),\n    th2l = c(X_gibbs[1, 2], X_gibbs[1:(100 - 1), 2])\n)\n\nlabs1 <- c(\"Amostras\", \"Itera√ß√µes do Algoritmo\", \"90% HPD\")\n\nind1 <- (1:50) * 2 - 1\ndf100s <- df100\ndf100s[ind1 + 1, 3:4] <- df100s[ind1, 3:4]\np1 <- ggplot() +\n  geom_point(data = df100s,\n             aes(th1, th2, group = id, color = \"1\")) +\n  geom_segment(data = df100, aes(x = th1, xend = th1l, color = \"2\",\n                                 y = th2, yend = th2l)) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"3\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Gibbs\", subtitle = \"100 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"red\", \"forestgreen\", \"blue\"), labels = labs1) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA, NA), linetype = c(0, 1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\nanimate(p1 +\n  transition_reveal(along = iter) +\n  shadow_trail(0.01),\n  # animation options\n  height = 7, width = 7, units = \"in\", res = 300\n)\n\n\n\n\nFigure 9: Anima√ß√£o Gibbs\n\n\n\nNa figura 10 √© poss√≠vel ver como ficaram as primeiras 1.000 simula√ß√µes excluindo 1.000 itera√ß√µes iniciais como warmup.\n\n\n# Take all the 10,000 observations after warmup of 1,000\nwarm <- 1e3\ndfs <- data.frame(\n  th1 = X_gibbs[(warm + 1):nrow(X_gibbs), 1],\n  th2 = X_gibbs[(warm + 1):nrow(X_gibbs), 2]\n)\n\nlabs2 <- c(\"Amostras\", \"90% HPD\")\n\nggplot() +\n  geom_point(data = dfs[1:1000, ],\n             aes(th1, th2, color = \"1\"), alpha = 0.3) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"2\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Gibbs\", subtitle = \"1.000 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"steelblue\", \"blue\"), labels = labs2) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), alpha = c(1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nFigure 10: Primeiras 1.000 simula√ß√µes Gibbs ap√≥s descarte de 1.000 itera√ß√µes como warmup\n\n\n\nE na figura 11 √© poss√≠vel ver as restantes 9.000 simula√ß√µes excluindo 1.000 itera√ß√µes iniciais como warmup.\n\n\n# Show all 10,000 samples\nggplot() +\n  geom_point(data = dfs,\n             aes(th1, th2, color = \"1\"), alpha = 0.3) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"2\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Gibbs\", subtitle = \"10.000 Amostragens\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"steelblue\", \"blue\"), labels = labs2) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), alpha = c(1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nFigure 11: 9.000 simula√ß√µes Gibbs ap√≥s descarte de 1.000 itera√ß√µes como warmup\n\n\n\nO que acontece quando rodamos correntes Markov em paralelo?\nComo as correntes Markov s√£o independentes, podemos execut√°-las em paralelo no computador. A chave para isso √© definir pontos iniciais diferentes de cada corrente Markov (caso voc√™ use como ponto inicial uma amostra de uma distribui√ß√£o pr√©via dos par√¢metros isto n√£o √© um problema). Vamos usar o mesmo exemplo did√°tico de uma distribui√ß√£o normal bivariada \\(X\\) e \\(Y\\) que usamos nos exemplos anteriores, mas agora com 4 correntes Markov com diferentes pontos de in√≠cio.\n\n\nstarts <- list(c(-2.5, 2.5),\n               c(2.5, -2.5),\n               c(-2.5, -2.5),\n               c(2.5, 2.5)\n               )\n\n\n\nCorrentes Markov em Paralelo ‚Äì Metropolis\nPara criar 4 correntes Markov com pontos diferentes de in√≠cio dos par√¢metros, usamos 4 vezes o amostrador Metropolis que codificamos anterior, mas agora passamos diferentes argumentos start_x e start_y, al√©m de diferentes seed do pseudogerador de n√∫mero aleat√≥rios para termos diferentes comportamentos das correntes Markov. Todo o resultado √© combinado em um dataframe com uma coluna id representando o n√∫mero de cada corrente (de 1 a 4).\n\n\nlibrary(dplyr)\nn_sim <- 100\nXs_met <- bind_rows(\n  as_tibble(metropolis(S = n_sim, half_width = 2.75,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = -2.5, start_y = 2.5,\n                       seed = 1)),\n  as_tibble(metropolis(S = n_sim, half_width = 2.75,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = 2.5, start_y = -2.5,\n                       seed = 2)),\n  as_tibble(metropolis(S = n_sim, half_width = 2.75,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = -2.5, start_y = -2.5,\n                       seed = 3)),\n  as_tibble(metropolis(S = n_sim, half_width = 2.75,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = 2.5, start_y = 2.5,\n                       seed = 4)),\n  .id = \"chain\")\n\n\n[1] \"Taxa de aceita√ß√£o 0.28\"\n[1] \"Taxa de aceita√ß√£o 0.19\"\n[1] \"Taxa de aceita√ß√£o 0.29\"\n[1] \"Taxa de aceita√ß√£o 0.15\"\n\nVejam que aqui n√£o estamos interessados em muitas itera√ß√µes, portanto cada corrente Markov amostrar√° 100 amostras dando um total de 400 amostras.\nHouveram algumas mudan√ßas significativas na taxa de aprova√ß√£o das propostas Metropolis. Todas ficaram em torno de 15%-29%, isso √© por conta do baixo n√∫mero de amostras (100), caso as amostras fosse maiores veremos esses valores convergirem para pr√≥ximo de 20% conforme o exemplo anterior de 10.000 amostras com uma √∫nica corrente. errores Na figura 12 √© poss√≠vel ver as 4 correntes Markov do algoritmo de Metropolis explorando o espa√ßo amostral.\n\n\ndfs100_met <- Xs_met %>%\n  group_by(chain) %>%\n  transmute(\n    chain,\n    iter = 1:n_sim,\n    th1 = V1,\n    th2 = V2,\n    th1l = dplyr::lag(V1, default = V1[1]),\n    th2l = dplyr::lag(V2, default = V2[1])\n  ) %>%\n  ungroup()\np1 <- ggplot(dfs100_met) +\n  geom_jitter(width = 0.05, height = 0.05,\n              aes(th1, th2, group = chain, color = chain), alpha = 0.3) +\n  geom_segment(aes(x = th1, xend = th1l, y = th2, yend = th2l,\n                   color = chain)) +\n  #geom_point(aes(x = th1, y = th2, color = chain)) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2), color = \"black\", level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Metropolis\",subtitle = \"100 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"NULL\")\n\nanimate(p1 +\n          transition_reveal(along = iter) +\n          shadow_trail(0.01),\n        # animation options\n        height = 7, width = 7, units = \"in\", res = 300\n)\n\n\n\n\nFigure 12: Anima√ß√£o Metropolis ‚Äì 4 correntes Markov em Paralelo\n\n\n\nCorrentes Markov em Paralelo ‚Äì Gibbs\nSimilar ao exemplo das correntes Markov em paralelo com o algoritmo Metropoli, para criarmos 4 correntes Markov com pontos diferentes de in√≠cio dos par√¢metros, usamos 4 vezes o amostrador Gibbs que codificamos anterior, mas agora passamos diferentes argumentos start_x e start_y, al√©m de diferentes seed do pseudogerador de n√∫mero aleat√≥rios para termos diferentes comportamentos das correntes Markov. Todo o resultado √© combinado em um dataframe com uma coluna id representando o n√∫mero de cada corrente (de 1 a 4).\n\n\nXs_gibbs <- bind_rows(\n  as_tibble(gibbs(S = n_sim,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = -2.5, start_y = 2.5,\n                       seed = 1)),\n  as_tibble(gibbs(S = n_sim,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = 2.5, start_y = -2.5,\n                       seed = 2)),\n  as_tibble(gibbs(S = n_sim,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = -2.5, start_y = -2.5,\n                       seed = 3)),\n  as_tibble(gibbs(S = n_sim,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = 2.5, start_y = 2.5,\n                       seed = 4)),\n  .id = \"chain\")\n\n\n\nVejam que aqui n√£o estamos interessados em muitas itera√ß√µes, portanto cada corrente Markov amostrar√° 100 amostras dando um total de 400 amostras.\nNa figura 13 √© poss√≠vel ver as 4 correntes Markov do algoritmo de Gibbs explorando o espa√ßo amostral.\n\n\ndfs100_gibbs <- Xs_gibbs %>%\n  group_by(chain) %>%\n  transmute(\n    chain,\n    iter = 1:n_sim,\n    th1 = V1,\n    th2 = V2,\n    th1l = dplyr::lag(V1, default = V1[1]),\n    th2l = dplyr::lag(V2, default = V2[1])\n  ) %>%\n  ungroup()\np1 <- ggplot(dfs100_gibbs) +\n  geom_point(aes(x = th1, y = th2, group = chain, color = chain)) +\n  geom_segment(aes(x = th1, xend = th1l, y = th2, yend = th2l,\n                   color = chain)) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2), color = \"black\", level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Gibbs\", subtitle = \"100 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"NULL\")\n\nanimate(p1 +\n  transition_reveal(along = iter) +\n  shadow_trail(0.01),\n  # animation options\n  height = 7, width = 7, units = \"in\", res = 300\n)\n\n\n\n\nFigure 13: Anima√ß√£o Gibbs ‚Äì 4 correntes Markov em Paralelo\n\n\n\nHamiltonian Monte Carlo ‚Äì HMC\nOs problemas de baixas taxas de aceita√ß√£o de propostas das t√©cnicas de Metropolis e do desempenho baixo do algoritmo de Gibbs em problemas multidimensionais nas quais a topologia da posterior √© complexa fizeram com que surgisse uma nova t√©cnica MCMC usando din√¢mica Hamiltoniana (em homenagem ao f√≠sico irland√™s William Rowan Hamilton (1805-1865) figura 14). O nome em ingl√™s dessa t√©cnica √© Hamiltonean Monte Carlo ‚Äì HMC.\n\n\n\nFigure 14: William Rowan Hamilton. Figura de https://www.wikipedia.org\n\n\n\nO HMC √© uma adapta√ß√£o da t√©cnica de Metropolis e emprega um esquema guiado de gera√ß√£o de novas proposta: isso melhora a taxa de aceita√ß√£o de propostas e, consequentemente, a efici√™ncia. Mais especificamente, o HMC usa o gradiente do log posterior para direcionar a cadeia de Markov para regi√µes de maior densidade posterior, onde a maioria das amostras s√£o coletadas. Como resultado, uma corrente Markov com o algoritmo HMC bem ajustada aceitar√° propostas em uma taxa muito mais alta do que o algoritmo Metropolis tradicional (Roberts, Gelman, & Gilks, 1997).\nHMC foi inicialmente descrito na literatura de f√≠sica Duane, Kennedy, Pendleton, & Roweth (1987) (que chamaram de ‚Äúhybrid‚Äù Monte Carlo ‚Äì HMC). Logo depois, HMC foi aplicado a problemas estat√≠sticos por Radford M. Neal (1994) (que chamou de Hamiltonean Monte Carlo ‚Äì HMC). Para uma discuss√£o aprofundada (que n√£o √© o foco deste conte√∫do) de HMC eu recomendo Radford M. Neal (2011) e Betancourt (2017).\nHMC usa din√¢mica Hamiltoniana aplicada para part√≠culas explorando a topologia de uma probabilidade posterior. Em algumas simula√ß√µes Metropolis possui taxa de aceita√ß√£o de aproximadamente 23%, enquanto HMC 65% (Gelman et al., 2013b). Al√©m de explorar melhor a topologia da posterior e tolerar topologias complexas, HMC √© muito mais eficiente que Metropolis e n√£o sofre do problema de correla√ß√£o dos par√¢metros que Gibbs.\nPara cada componente \\(\\theta_j\\), o HMC adiciona uma vari√°vel de momento \\(\\phi_j\\). A densidade posterior \\(P(\\theta | y)\\) √© incrementada por uma distribui√ß√£o independente \\(P(\\phi)\\) dos momentos, definindo assim uma distribui√ß√£o conjunta:\n\\[\nP(\\theta, \\phi | y) = P(\\phi) \\cdot P(\\theta|y)\n\\]\nO HMC usa uma distribui√ß√£o de propostas que muda dependendo do estado atual na corrente Markov. O HMC descobre a dire√ß√£o em que a distribui√ß√£o posterior aumenta, chamada de gradiente, e distorce a distribui√ß√£o de propostas em dire√ß√£o ao gradiente. No algoritmo de Metropolis, a distribui√ß√£o das propostas seria uma distribui√ß√£o Normal (geralmente) centrada na posi√ß√£o atual, de modo que saltos acima ou abaixo da posi√ß√£o atual teriam a mesma probabilidade de serem propostos. Mas o HMC gera propostas de maneira bem diferente.\nVoc√™ pode imaginar que para distribui√ß√µes posteriores de alta dimens√£o que t√™m vales diagonais estreitos e at√© mesmo vales curvos, a din√¢mica do HMC encontrar√° posi√ß√µes propostas que s√£o muito mais promissoras do que uma distribui√ß√£o de proposta sim√©trica ing√™nua, e mais promissoras do que a amostragem de Gibbs, que pode obter preso em paredes diagonais.\nA probabilidade da corrente Markov mudar de estado no algoritmo HMC √© definida como:\n\\[\nP_{\\text{mudar}} = \\min\\left({\\frac{P(\\theta_{\\text{proposto}}) \\cdot P(\\phi_{\\text{proposto}})}{P(\\theta_{\\text{atual}})\\cdot P(\\phi_{\\text{atual}})}}, 1\\right),\n\\]\nonde \\(\\phi\\) √© o momento.\nDistribui√ß√£o dos Momentos ‚Äì \\(P(\\phi)\\)\nNormalmente damos a \\(\\phi\\) uma distribui√ß√£o normal multivariada com m√©dia 0 e covari√¢ncia de \\(\\mathbf{M}\\), uma ‚Äúmatriz de massa.‚Äù Para mant√™r as coisas um pouco mais simples, usamos uma matriz de massa diagonal \\(\\mathbf{M}\\). Isso faz com que os componentes de \\(\\phi\\) sejam independentes com \\(\\phi_j \\sim \\text{Normal}(0, M_{jj})\\)\nAlgoritmo de HMC\nO algoritmo de HM √© bem similar ao algoritmo Metropolis mas com a inclus√£o do momento \\(\\phi\\) como uma maneira de quantificar o gradiente da posterior.\nAmostre \\(\\phi\\) de uma \\(\\text{Normal}(0,\\mathbf{M})\\)\nSimultaneamente amostre \\(\\theta\\) e \\(\\phi\\) com \\(L\\) leapfrog steps (n√£o sei como traduzir isso, talvez m√∫ltiplos passos) cada um reduzido por um fator \\(\\epsilon\\). Em um leapfrog step, tanto \\(\\theta\\) quanto \\(\\phi\\) s√£o alterados, um em rela√ß√£o ao outro. Repita os seguintes passos \\(L\\) vezes:\nUse o gradiente do log da posterior9 de \\(\\theta\\) para produzir um meio-salto(half-step) de \\(\\phi\\):\n\\[\\phi \\leftarrow \\phi + \\frac{1}{2} \\epsilon \\frac{d \\log p(\\theta | y)}{d \\theta}\\]\nUse o vetor de momentos \\(\\phi\\) para atualizar o vetor de par√¢metros \\(\\theta\\):\n\\[\\theta \\leftarrow \\theta + \\epsilon \\mathbf{M}^{-1} \\phi\\]\nNovamente use o gradiente de \\(\\theta\\) para produzir um meio-salto(half-step) de \\(\\phi\\):\n\\[\\phi \\leftarrow \\phi + \\frac{1}{2} \\epsilon \\frac{d \\log p(\\theta | y)}{d \\theta}\\]\n\nDesigne \\(\\theta^{t-1}\\) e \\(\\phi^{t-1}\\) como os valores do vetor de par√¢metros e do vetor de momentos, respectivamente, no in√≠cio do processo de leapfrog (etapa 2) e \\(\\theta^*\\) e \\(\\phi^*\\) como os valores ap√≥s \\(L\\) passos. Como regra de aceita√ß√£o/rejei√ß√£o calcule:\n\\[r = \\frac{p(\\theta^* | y) p(\\phi^*)}{p(\\theta^{t-1} | y) p(\\phi^{-1})}\\]\nDesigne:\n\\[\\theta^t\n \\begin{cases}\n \\theta^* & \\text{with probability min($r$,1)} \\\\\n \\theta^{t-1} & \\text{caso contr√°rio}\n \\end{cases}\\]\nHMC ‚Äì Implementa√ß√£o\nPara HMC, n√£o vou codificar o algoritmo na m√£o, pois envolve derivadas que n√£o vai ser muito eficiente no R. Para isso temos o Stan. O arquivo hmc.rds possui 1.000 amostragens com um leapfrog \\(L = 40\\), ent√£o no total s√£o 40.001 itera√ß√µes10. O exemplo √© o mesmo que usamos para Metropolis e Gibbs, uma distribui√ß√£o normal multivariada de \\(X\\) e \\(Y\\) (ambos com m√©dia 0 e desvio padr√£o 1), com correla√ß√£o 0.8 (\\(\\rho = 0.8\\)):\n\\[\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix} \\sim \\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix}, \\mathbf{\\Sigma}\n\\right) \\\\\n\\mathbf{\\Sigma} \\sim\n\\begin{pmatrix}\n1 & 0.8 \\\\\n0.8 & 1\n\\end{pmatrix}\n\\]\n\n\nload(here::here(\"R\", \"hmc.RData\"))\ndf <- tibble(id = rep(1, 40000),\n                 iter = rep(1:1000, each = 40),\n                 th1 = tt[1:40000, 1],\n                 th2 = tt[1:40000, 2],\n                 th1l = c(tt[1, 1], tt[1:(40000 - 1), 1]),\n                 th2l = c(tt[1, 2], tt[1:(40000 - 1), 2]))\n\n\n\n\n\nX_hmc <- tt[seq(2, 40001, by = 40), ]\nres <- monitor(X_hmc, digits_summary = 1)\n\n\nInference for the input samples (2 chains: each with iter = 1000; warmup = 500):\n\n     Q5  Q50 Q95 Mean  SD  Rhat Bulk_ESS Tail_ESS\nV1 -1.6 -0.1 1.4 -0.1 0.9     1      604      655\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS > 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat <= 1.05).\n\nneff <- res[, \"n_eff\"]\nreff <- mean(neff / (nrow(X_hmc))) #  61%!!!\n\n\n\nNa nossa execu√ß√£o do algoritmo HMC temos como resultado uma matriz X_hmc com 100 linhas e 2 colunas (as mesmas condi√ß√µes j√° mostradas nos exemplos anteriores com algoritmo Metropolis e Gibbs, por√©m agora somente com 1.000 amostras).\nVejam que o n√∫mero de amostras eficientes em rela√ß√£o ao n√∫mero total de itera√ß√µes reff √© 61% para todas as itera√ß√µes incluindo warm-up (no caso 1 leapfrog step \\(L = 1\\)). A efici√™ncia do algoritmo HMC, no nosso exemplo did√°tico, √© o mais que 6x a efici√™ncia do algoritmo de Metropolis (9,5% vs 61%) e quase 3x a efici√™ncia do Gibbs (23% vs 61%).\nHMC ‚Äì Intui√ß√£o Visual\nA anima√ß√£o na figura 15 mostra as 50 primeiras simula√ß√µes do algoritmo HMC usado para gerar X_hmc. Vejam que aqui temos em amarelo temos os leapfrog steps moldandos e distorcendo a distribui√ß√£o de propostas em dire√ß√£o ao gradiente da posterior (conduzindo-as para √°reas de maior probabilidade da posterior) e em vermelho temos as amostras ap√≥s os 40 leapfrog step \\(L = 40\\) de cada intera√ß√£o. Notem como a explora√ß√£o da posterior √© muito mais eficiente e focada em locais onde realmente a distribui√ß√£o de interesse possui maior probabilidade.\n\n\nlabs3 <- c(\"Amostras\", \"Itera√ß√µes do Algoritmo\", \"90% HPD\", \"Leapfrog\")\n# base plot\np0 <- ggplot() +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"3\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"HMC\", subtitle = \"50 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"red\", \"forestgreen\", \"blue\", \"yellow\"), labels = labs3) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA, NA, 16), linetype = c(0, 1, 1, 0)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n# first 100 iterations\ndf50 <- df %>% filter(iter <= 50)\npp <- p0 + geom_point(data = df50,\n                      aes(th1, th2, color = \"4\"), alpha = 0.3, size = 1) +\n  geom_segment(data = df50,\n               aes(x = th1, xend = th1l, color = \"2\", y = th2, yend = th2l),\n               alpha = 0.5) +\n        geom_point(data = df50[seq(1, nrow(df50), by = 40), ],\n                   aes(th1, th2, color = \"1\"), size = 2)\n\nanimate(pp +\n  transition_manual(iter, cumulative = TRUE) +\n  shadow_trail(0.05),\n  # animation options\n  height = 7, width = 7, units = \"in\", res = 300\n)\n\n\n\n\nFigure 15: Anima√ß√£o HMC\n\n\n\nNa figura 16 √© poss√≠vel ver como ficaram as 1.000 simula√ß√µes excluindo o primeiro leapfrog step \\(L = 1\\) como warmup.\n\n\n# Take all the 1,000 observations after warmup of 1,000\nwarm <- 1\ndfs <- data.frame(\n  th1 = tt[(warm + 1):nrow(tt), 1],\n  th2 = tt[(warm + 1):nrow(tt), 2]\n)\n\nggplot() +\n  geom_point(data = dfs[seq(1, nrow(dfs), by = 40), ],\n             aes(th1, th2, color = \"1\"), alpha = 0.3) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"2\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"HMC\", subtitle = \"1.000 Amostragens\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"steelblue\", \"blue\"), labels = labs2) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), alpha = c(1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nFigure 16: 1.000 simula√ß√µes HMC ap√≥s descarte da primeira itera√ß√£o como warmup\n\n\n\n‚ÄúN√£o entendi nada‚Ä¶‚Äù\nSe voc√™ n√£o entendeu nada at√© agora, n√£o se desespere. Pule todas as f√≥rmulas e pegue a intui√ß√£o visual dos algoritmos. Veja as limita√ß√µes de Metropolis e Gibbs e compare as anima√ß√µes e figuras com as do HMC. A superioridade de efici√™ncia (mais amostras com baixa autocorrela√ß√£o) e efic√°cia (mais amostras pr√≥ximas das √°reas de maior probabilidade da distribui√ß√£o-alvo) √© autoexplicativa pelas imagens.\nAl√©m disso, voc√™ provavelmente nunca ter√° que codificar o seu algoritmo HMC (Gibbs, Metropolis ou qualquer outro MCMC) na m√£o. Para isso h√° pacotes como Stan (e seu ecossistema de pacotes: rstan, PyStan, brms, rstanarm, Stan.jl etc.). Al√©m disso, Stan implementa um HMC modificado com uma t√©cnica chamada No-U-Turn Sampling (NUTS) (Hoffman & Gelman, 2011) que seleciona automaticamente os valores de \\(\\epsilon\\) (fator de redu√ß√£o) e \\(L\\) (quantidade de leapfrog steps).11 O desempenho do HMC √© altamente sens√≠vel √† esses dois ‚Äúhiperpar√¢metros‚Äù (par√¢metros que devem ser especificados pelo usu√°rio). Em particular, se \\(L\\) for muito pequeno, o algoritmo exibe comportamento indesej√°vel de um passeio aleat√≥rio, enquanto se \\(L\\) for muito grande, o algoritmo desperdi√ßa efici√™ncia computacional. NUTS usa um algoritmo recursivo para construir um conjunto de pontos candidatos prov√°veis que abrangem uma ampla faixa da distribui√ß√£o de propostas, parando automaticamente quando come√ßa a voltar e refazer seus passos (por isso que ele n√£o d√° meia-volta ‚Äì No U-turn), adicionalmente NUTS tamb√©m calibra automaticamente (e de maneira simult√¢nea) \\(L\\) e \\(\\epsilon\\).\nImplementa√ß√£o com o rstanarm\nComo configura√ß√£o padr√£o, o pacote rstanarm utiliza HMC com NUTS. Al√©m disso, os argumentos padr√µes do HMC no rstanarm s√£o:\n4 correntes Markov de amostragem (chains = 4); e\n2.000 itera√ß√µes de cada corrente (iter = 2000)12.\nRelembrando o exemplo da aula de regress√£o linear, vamos usar o mesmo dataset kidiq. S√£o dados de uma survey de mulheres adultas norte-americanas e seus respectivos filhos. Datado de 2007 possui 434 observa√ß√µes e 4 vari√°veis:\nkid_score: QI da crian√ßa;\nmom_hs: bin√°ria (0 ou 1) se a m√£e possui diploma de ensino m√©dio;\nmom_iq: QI da m√£e; e\nmom_age: idade da m√£e.\nVamos estimar um modelo de regress√£o linear Bayesiano na qual a vari√°vel dependente √© kid_score e as independentes s√£o mom_hs e mom_iq.\nO modelo √© o especificado da seguinte maneira:\n\\[\n\\begin{aligned}\n\\alpha &\\sim \\text{Normal}(\\mu_y, s_y) \\\\\n\\beta_k &\\sim \\text{Normal}(0, 2.5 \\cdot \\frac{s_y}{s_x}) \\\\\n\\sigma &\\sim \\text{Exponencial}(\\frac{1}{s_y})\\\\\ny &\\sim \\text{Normal}(\\alpha + \\beta_1 x_1 + \\dots + \\beta_K x_K, \\sigma),\n\\end{aligned}\n\\]\nonde \\(s_x = \\tt{sd(x)}\\), \\[\ns_y =\n\\begin{cases}\n\\tt{sd(y)} & \\text{se } \\tt{family = gaussian}, \\\\\n1 & \\text{caso contr√°rio}.\n\\end{cases}\n\\] e \\[\n\\mu_y =\n\\begin{cases}\n\\tt{mean(y)} & \\text{se } \\tt{family = gaussian}, \\\\\n0 & \\text{caso contr√°rio}.\n\\end{cases}\n\\]\nNo caso temos apenas duas vari√°veis independentes, ent√£o \\(K=2\\) e \\(\\beta_1 = \\tt{mom\\_hs}\\) e \\(\\beta_2 = \\tt{mom\\_iq}\\); vari√°vel dependente \\(y = \\tt{kid\\_score}\\) e o erro do modelo $= _score ~ mom_hs + mom_iq`.\n\n\noptions(mc.cores = parallel::detectCores())\noptions(Ncpus = parallel::detectCores())\n\nlibrary(rstanarm)\nmodel <- stan_glm(\n  kid_score ~ mom_hs + mom_iq,\n  data = kidiq\n)\n\n\n\nM√©tricas da simula√ß√£o MCMC\nUm modelo estimado pelo rstanarm pode ser inspecionado em rela√ß√£o ao desempenho da amostragem MCMC. Ao chamarmos a fun√ß√£o summary() no modelo estimado h√° uma parte chamada MCMC diagnostics.\n\n\nsummary(model)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs + mom_iq\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 434\n predictors:   3\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 26.0    6.0 18.4  26.0  33.4 \nmom_hs       5.9    2.2  3.1   5.9   8.8 \nmom_iq       0.6    0.1  0.5   0.6   0.6 \nsigma       18.2    0.6 17.4  18.1  19.0 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 86.8    1.3 85.2  86.8  88.4 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  5365 \nmom_hs        0.0  1.0  4079 \nmom_iq        0.0  1.0  4724 \nsigma         0.0  1.0  3809 \nmean_PPD      0.0  1.0  4289 \nlog-posterior 0.0  1.0  1801 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nA se√ß√£o MCMC diagnostics possui tr√™s colunas de valores para cada par√¢metro estimado do modelo.\nNo nosso caso, temos tr√™s par√¢metros importantes:\nvalor do coeficiente da vari√°vel mom_hs;\nvalor do coeficiente da vari√°vel mom_iq; e\nvalor do erro residual do modelo linear sigma. As tr√™s m√©tricas s√£o:\nmcse: Monte Carlo Standard Error, o erro de mensura√ß√£o da amostragem Monte Carlo do par√¢metro;\nn_eff: uma aproxima√ß√£o crua do n√∫mero de amostras efetivas amostradas pelo MCMC estimada pelo valor de Rhat; e\nRhat: uma m√©trica de converg√™ncia e estabilidade da corrente Markov.\nA m√©trica mais importante para levarmos em considera√ß√£o √© a Rhat que √© uma m√©trica que mensura se as correntes Markov s√£o est√°veis e convergiram para um valor durante o progresso total das simula√ß√µes. Ela √© basicamente a propor√ß√£o de varia√ß√£o ao compararmos duas metades das correntes ap√≥s o descarte dos warmups. Valor de 1 implica em converg√™ncia e estabilidade. Como padr√£o o Rhat deve ser menor que 1.01 para que a estima√ß√£o Bayesiana seja v√°lida (S. P. Brooks & Gelman, 1998; Gelman & Rubin, 1992).\nO que fazer se n√£o obtermos converg√™ncia?\nDependendo do modelo e dos dados √© poss√≠vel que HMC (mesmo com NUTS) n√£o atinja converg√™ncia. Nesse caso, ao rodar o modelo rstanarm dar√° diversos avisos de diverg√™ncias. Aqui vou restringir o amostrador HMC do rstanarm para apenas 200 itera√ß√µes com warmup padr√£o de metade das itera√ß√µes (100) com duas correntes em paralelo (chains). Portanto, teremos \\(2 \\cdot (200 - 100) = 200\\) amostras de MCMC. se atente as mensagens de erro.\n\n\nbad_model <- stan_glm(\n  kid_score ~ mom_hs + mom_iq,\n  data = kidiq,\n  chains = 2,\n  iter = 200\n  )\n\n\n\nEsta √© uma vantagem dos pacotes do ecossistema do Stan (incluindo o rstanarm). Quando o amostrador MCMC mostra problemas ele falha de uma maneira bem escandalosa com diversos avisos. Nunca ignore esses avisos, eles est√£o l√° para te ajudar e indicar que seu modelo possui problemas s√©rios que devem ser inspecionados e sanados.\nE vemos que o Rhat dos par√¢metros estimados do modelo est√£o bem acima do limiar de \\(1.01\\).\n\n\nsummary(bad_model)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs + mom_iq\n algorithm:    sampling\n sample:       200 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 434\n predictors:   3\n\nEstimates:\n              mean   sd    10%   50%   90%\n(Intercept)  13.6   17.3 -10.5  20.2  30.7\nmom_hs        5.9    3.3   2.3   5.6  10.2\nmom_iq        0.6    0.1   0.5   0.6   0.7\nsigma        24.6    9.5  17.9  19.1  42.1\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 75.4   15.1 48.8  82.5  88.2 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse  Rhat  n_eff\n(Intercept)    12.7   1.9   2  \nmom_hs          0.2   1.0 305  \nmom_iq          0.0   1.0 245  \nsigma           6.1   2.6   2  \nmean_PPD       12.1   3.4   2  \nlog-posterior 102.6   3.2   2  \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nGr√°ficos de Diagn√≥sticos do MCMC\nO pacote rstanarm tem diversos gr√°ficos interessantes de diagn√≥sticos de converg√™ncia das simula√ß√µes MCMC. Eu recomendo um guia de visualiza√ß√µes de modelos Bayesianos de Gabry, Simpson, Vehtari, Betancourt, & Gelman (2019).\nTraceplot\nA primeira coisa que devemos ver quando h√° mensagens de avisos sobre diverg√™ncias ou valores indesej√°veis de Rhat √© inspecionar as correntes Markov para ver se elas est√£o estacion√°rias ou se divergiram durante a amostragem do MCMC. Fazemos isso com a fun√ß√£o plot(stanreg, \"trace\"). Objetos stanreg s√£o modelos oriundos do rstanarm. No nosso caso temos dois objetos stanreg: o model e o bad_model.\nO traceplot √© a sobreposi√ß√£o das amostragens MCMC das correntes para cada par√¢metro estimado (eixo vertical). A ideia √© que as correntes se misturam e que n√£o haja nenhuma inclina√ß√£o ao longo das itera√ß√µes (eixo horizontal). Isso demonstra que elas convergiram para um certo valor do par√¢metro e se mantiveram nessa regi√£o durante boa parte (ou toda) da(a) amostragem das correntes Markov.\nDetalhe: o traceplot usa somente as itera√ß√µes v√°lidas, ap√≥s a remo√ß√£o das itera√ß√µes de warmup.\nVejam na figura 17 o traceplot do modelo que as correntes Markov convergiram e ficaram estacion√°rias durante a amostragem do MCMC (afinal esse √© o modelo model que designamos iter = 2.000 e chains = 4, ambos padr√µes do rstanarm). O ideal √© sempre esse padr√£o no qual as correntes n√£o apresentam uma tend√™ncia espec√≠fica, ou seja, elas ficam geralmente ‚Äúplanas‚Äù na horizontal e n√£o h√° uma grande varia√ß√£o de valores no eixo vertical (valor dos par√¢metros). Esse padr√£o √© muito parecido com ‚Äútaturana.‚Äù\n\n\nplot(model, \"trace\")\n\n\n\n\nFigure 17: Traceplot do model\n\n\n\nNa figura 18 temos o traceplot do modelo que as correntes Markov n√£o convergiram, o bad_model (designamos iter = 200 e chains = 2). Aqui voc√™ v√™ que se aumentarmos o per√≠odo de warmup e o n√∫mero de itera√ß√µes, provavelmente as correntes Markov convergiriam e ficariam estacion√°rias na regi√£o de maior probabilidade da posterior (e, consequentemente, dos par√¢metros de interesse).\n\n\nplot(bad_model, \"trace\")\n\n\n\n\nFigure 18: Traceplot do bad_model\n\n\n\nPosterior Predictive Check\nUm bom gr√°fico de diagn√≥stico √© o posterior predictive check (PPC) que compara o histograma da vari√°vel dependente \\(y\\) contra o histograma vari√°veis dependentes simuladas pelo modelo \\(y_{\\text{rep}}\\) ap√≥s a estima√ß√£o dos par√¢metros. A ideia √© que os histogramas reais e simulados se misturem e n√£o haja diverg√™ncias. Fazemos isso com a fun√ß√£o pp_check(stanreg).\nVejam na figura 19 o PPC do modelo que as correntes Markov convergiram e ficaram estacion√°rias durante a amostragem do MCMC (model). Podemos ver que as simula√ß√µes \\(y_{\\text{rep}}\\) realmente capturaram a natureza da vari√°vel dependente \\(y\\).\n\n\npp_check(model)\n\n\n\n\nFigure 19: Posterior Preditive Check do model\n\n\n\nJ√° na na figura 20 temos o PPC do modelo que as correntes Markov n√£o convergiram, o bad_model. Aqui vemos que as simula√ß√µes \\(y_{\\text{rep}}\\) falharam em capturar a natureza da vari√°vel dependente \\(y\\). O PPC do bad_model tamb√©m indica que se mantiv√©ssemos um periodo maior de warmup e mais itera√ß√µes das correntes Markov, provavelmente conseguir√≠amos ter um modelo que representasse muito bem o processo de gera√ß√£o de dados da nossa vari√°vel dependente \\(y\\).\n\n\npp_check(bad_model)\n\n\n\n\nFigure 20: Posterior Preditive Check do bad_model\n\n\n\nO qu√™ fazer para convergir suas correntes Markov\nPrimeiro: Antes de fazer ajustes finos no n√∫mero de correntes chains ou no n√∫mero de itera√ß√µes iter (entre outros ‚Ä¶) saiba que o amostrador HMC-NUTS do Stan e seu ecossistema de pacotes (rstanarm incluso) √© muito eficiente e eficaz em explorar as mais diversas complexas e ‚Äúmalucas‚Äù topologias de distribui√ß√µes-alvo posterior. Os argumentos padr√µes (iter = 2000, chains = 4 e warmup = floor(iter/2)) funcionam perfeitamente para 99% dos casos (mesmo em modelos complexos). Dito isto, na maioria das vezes quando voc√™ possui problemas de amostragem e computacionais no seu modelo Bayesiano, o problema est√° na especifica√ß√£o do modelo e n√£o no algoritmo de amostragem MCMC. Esta frase foi dita por Andrew Gelman (o ‚Äúpai‚Äù do Stan) e √© conhecido como o Folk Theorem (Gelman, 2008): ‚ÄúWhen you have computational problems, often there‚Äôs a problem with your model‚Äù.\nSe o seu modelo Bayesiano est√° com problemas de converg√™ncia h√° alguns passos que podem ser tentados13. Aqui listados do mais simples para o mais complexo:\nAumentar o n√∫mero de itera√ß√µes e correntes: primeira op√ß√£o √© aumentar o n√∫mero de itera√ß√µes do MCMC com o argumento iter = XXX e tamb√©m √© poss√≠vel aumentar o n√∫mero de correntes com o argumento chains = X. Lembrando que o padr√£o √© iter = 2000 e chains = 4.\nAlterar a rotina de adapta√ß√£o do HMC: a segunda op√ß√£o √© fazer com que o algoritmo de amostragem HMC fique mais conservador (com proposi√ß√µes de pulos menores). Isto pode ser alterado com o argumento adapt_delta da lista de op√ß√µes control. control=list(adapt_delta=0.9). O padr√£o do adapt_delta √© control=list(adapt_delta=0.8). Ent√£o qualquer valor entre \\(0.8\\) e \\(1.0\\) o torna mais conservador.\nReparametriza√ß√£o do Modelo: a terceira op√ß√£o √© reparametrizar o modelo. H√° duas maneiras de parametrizar o modelo: a primeira com parametriza√ß√£o centrada (centered parameterization) e a segunda com parametriza√ß√£o n√£o-centrada (non-centered parameterization). N√£o s√£o assuntos que vamos cobrir aqui no curso. Recomendo o material de um dos desenvolvedores da linguagem Stan, Michael Betancourt.\nColetar mais dados: √†s vezes o modelo √© complexo demais e precisamos de uma amostragem maior para conseguirmos estimativas est√°veis.\nRepensar o modelo: falha de converg√™ncia quando temos uma amostragem adequada geralmente √© por conta de uma especifica√ß√£o de priors e verossimilhan√ßa que n√£o s√£o compat√≠veis com os dados. Nesse caso, √© preciso repensar o processo generativo de dados no qual os pressupostos do modelo est√£o ancorados.\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] rstanarm_2.21.1      Rcpp_1.0.6           dplyr_1.0.4         \n [4] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n [7] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[10] gganimate_1.0.7      plotly_4.9.3         ggplot2_3.3.3       \n\nloaded via a namespace (and not attached):\n  [1] minqa_1.2.4        colorspace_2.0-0   ellipsis_0.3.1    \n  [4] ggridges_0.5.3     rsconnect_0.8.16   rprojroot_2.0.2   \n  [7] markdown_1.1       base64enc_0.1-3    farver_2.0.3      \n [10] DT_0.17            fansi_0.4.2        splines_4.0.3     \n [13] codetools_0.2-18   downlit_0.2.1      mnormt_2.0.2      \n [16] knitr_1.31         shinythemes_1.2.0  polyclip_1.10-0   \n [19] bayesplot_1.8.0    jsonlite_1.7.2     nloptr_1.2.2.2    \n [22] png_0.1-7          shiny_1.6.0        compiler_4.0.3    \n [25] httr_1.4.2         Matrix_1.3-2       assertthat_0.2.1  \n [28] fastmap_1.1.0      lazyeval_0.2.2     cli_2.3.0         \n [31] later_1.1.0.1      tweenr_1.0.1       htmltools_0.5.1.1 \n [34] prettyunits_1.1.1  tools_4.0.3        igraph_1.2.6      \n [37] gtable_0.3.0       glue_1.4.2         reshape2_1.4.4    \n [40] V8_3.4.0           vctrs_0.3.6        nlme_3.1-152      \n [43] crosstalk_1.1.1    xfun_0.21          stringr_1.4.0     \n [46] ps_1.5.0           lme4_1.1-26        mime_0.9          \n [49] miniUI_0.1.1.1     lifecycle_0.2.0    gtools_3.8.2      \n [52] statmod_1.4.35     zoo_1.8-8          scales_1.1.1      \n [55] colourpicker_1.1.0 hms_1.0.0          promises_1.2.0.1  \n [58] parallel_4.0.3     inline_0.3.17      shinystan_2.5.0   \n [61] RColorBrewer_1.1-2 yaml_2.2.1         curl_4.3          \n [64] gridExtra_2.3      loo_2.4.1          distill_1.2       \n [67] stringi_1.5.3      highr_0.8          dygraphs_1.1.1.6  \n [70] gifski_0.8.6       boot_1.3-27        pkgbuild_1.2.0    \n [73] rlang_0.4.10       pkgconfig_2.0.3    matrixStats_0.58.0\n [76] evaluate_0.14      lattice_0.20-41    purrr_0.3.4       \n [79] rstantools_2.1.1   htmlwidgets_1.5.3  labeling_0.4.2    \n [82] processx_3.4.5     tidyselect_1.1.0   here_1.0.1        \n [85] plyr_1.8.6         magrittr_2.0.1     R6_2.5.0          \n [88] magick_2.6.0       generics_0.1.0     DBI_1.1.1         \n [91] pillar_1.4.7       withr_2.4.1        xts_0.12.1        \n [94] survival_3.2-7     tibble_3.0.6       crayon_1.4.1      \n [97] tmvnsim_1.0-2      rmarkdown_2.6      jpeg_0.1-8.1      \n[100] progress_1.2.2     grid_4.0.3         isoband_0.2.3     \n[103] data.table_1.13.6  callr_3.5.1        threejs_0.3.3     \n[106] digest_0.6.27      xtable_1.8-4       tidyr_1.1.2       \n[109] httpuv_1.5.5       RcppParallel_5.0.2 stats4_4.0.3      \n[112] munsell_0.5.0      viridisLite_0.3.0  shinyjs_2.0.0     \n\n\n\n\nBetancourt, M. (2017, January 9). A Conceptual Introduction to Hamiltonian Monte Carlo. Retrieved November 6, 2019, from http://arxiv.org/abs/1701.02434\n\n\nBrooks, S., Gelman, A., Jones, G., & Meng, X.-L. (2011). Handbook of Markov Chain Monte Carlo. Retrieved from http://books.google.com?id=qfRsAIKZ4rIC\n\n\nBrooks, S. P., & Gelman, A. (1998). General Methods for Monitoring Convergence of Iterative Simulations. Journal of Computational and Graphical Statistics, 7(4), 434‚Äì455. https://doi.org/10.1080/10618600.1998.10474787\n\n\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., ‚Ä¶ Riddell, A. (2017). Stan : A Probabilistic Programming Language. Journal of Statistical Software, 76(1). https://doi.org/10.18637/jss.v076.i01\n\n\nCasella, G., & George, E. I. (1992). Explaining the gibbs sampler. The American Statistician, 46(3), 167‚Äì174. https://doi.org/10.1080/00031305.1992.10475878\n\n\nChib, S., & Greenberg, E. (1995). Understanding the Metropolis-Hastings Algorithm. The American Statistician, 49(4), 327‚Äì335. https://doi.org/10.1080/00031305.1995.10476177\n\n\nDuane, S., Kennedy, A. D., Pendleton, B. J., & Roweth, D. (1987). Hybrid Monte Carlo. Physics Letters B, 195(2), 216‚Äì222. https://doi.org/10.1016/0370-2693(87)91197-X\n\n\nGabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389‚Äì402. https://doi.org/10.1111/rssa.12378\n\n\nGelman, A. (1992). Iterative and Non-Iterative Simulation Algorithms. Computing Science and Statistics (Interface Proceedings), 24, 457‚Äì511. PROCEEDINGS PUBLISHED BY VARIOUS PUBLISHERS.\n\n\nGelman, A. (2008). The folk theorem of statistical computing. Retrieved from https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013a). Basics of Markov Chain Simulation. In Bayesian Data Analysis. Chapman and Hall/CRC.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013b). Bayesian Data Analysis. Chapman and Hall/CRC.\n\n\nGelman, A., & Rubin, D. B. (1992). Inference from Iterative Simulation Using Multiple Sequences. Statistical Science, 7(4), 457‚Äì472. https://doi.org/10.1214/ss/1177011136\n\n\nGeman, S., & Geman, D. (1984). Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-6(6), 721‚Äì741. https://doi.org/10.1109/TPAMI.1984.4767596\n\n\nHastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1), 97‚Äì109. https://doi.org/10.1093/biomet/57.1.97\n\n\nHoffman, M. D., & Gelman, A. (2011). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1), 1593‚Äì1623. Retrieved from http://arxiv.org/abs/1111.4246\n\n\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953). Equation of State Calculations by Fast Computing Machines. The Journal of Chemical Physics, 21(6), 1087‚Äì1092. https://doi.org/10.1063/1.1699114\n\n\nNeal, Radford M. (1994). An Improved Acceptance Procedure for the Hybrid Monte Carlo Algorithm. Journal of Computational Physics, 111(1), 194‚Äì203. https://doi.org/10.1006/jcph.1994.1054\n\n\nNeal, Radford M. (2011). MCMC using Hamiltonian dynamics. In S. Brooks, A. Gelman, G. L. Jones, & X.-L. Meng (Eds.), Handbook of markov chain monte carlo.\n\n\nRoberts, G. O., Gelman, A., & Gilks, W. R. (1997). Weak convergence and optimal scaling of random walk Metropolis algorithms. Annals of Applied Probability, 7(1), 110‚Äì120. https://doi.org/10.1214/aoap/1034625254\n\n\no s√≠mbolo \\(\\propto\\) (\\propto) deve ser lido como ‚Äúproporcional √†.‚Äù‚Ü©Ô∏é\nAlgumas refer√™ncias chamam esse processo de burnin.‚Ü©Ô∏é\nCaso queira uma melhor explana√ß√£o do algoritmo de Metropolis e Metropolis-Hastings sugiro ver Chib & Greenberg (1995)‚Ü©Ô∏é\ndo ingles probability density function (PDF).‚Ü©Ô∏é\nobjetos stanfit s√£o objetos resultantes de modelos rstan ou rstanarm.‚Ü©Ô∏é\nCaso queira uma melhor explana√ß√£o do algoritmo de Gibbs sugiro ver Casella & George (1992).‚Ü©Ô∏é\nisto ficar√° claro nas imagens e anima√ß√µes.‚Ü©Ô∏é\nVejam que aqui eu propositalmente dividi a neff por nrow(X_gibbs) / 2 (metade do n√∫mero de itera√ß√µes). Isso foi necess√°rio, pois da maneira que eu codifiquei o algoritmo Gibbs ele amostra um par√¢metro a cada intera√ß√£o e geralmente n√£o se implementa um amostrador Gibbs dessa maneira (amostra-se todos os par√¢metros por itera√ß√£o). Eu fiz de prop√≥sito pois quero gerar nos GIFs animados na figura 9 a real trajet√≥ria do amostrador Gibbs no espa√ßo amostral (vertical e horizontal, e n√£o diagonal).‚Ü©Ô∏é\npor quest√µes de transbordamento num√©rico (numeric overflow) sempre trabalhamos com log de probabilidades.‚Ü©Ô∏é\n1.000 * 40 = 40.000. Esse 1 a mais √© que usei a primeira itera√ß√£o com Leapfrog \\(L = 1\\) como warmup.‚Ü©Ô∏é\nal√©m disso, todos os pacotes do ecossistema Stan aplicam uma decomposi√ß√£o QR na matriz \\(X\\) de dados, criando uma base ortogonal (n√£o correlacionada) para amostragem. Isso faz com a distribui√ß√£o-alvo (posterior) fique muito mais amig√°vel do ponto de vista topol√≥gico/geom√©trico para o amostrador MCMC explor√°-la de maneira mais eficiente e eficaz.‚Ü©Ô∏é\nSendo que, por padr√£o, Stan e rstanarm descartam a primeira metade (1.000) das itera√ß√µes como aquecimento (warmup = floor(iter/2)).‚Ü©Ô∏é\nal√©m disso, vale a pena ativar a decomposi√ß√£o QR na matriz \\(X\\) de dados, criando uma base ortogonal (n√£o correlacionada) para amostragem. Isso faz com a distribui√ß√£o-alvo (posterior) fique muito mais amig√°vel do ponto de vista topol√≥gico/geom√©trico para o amostrador MCMC explor√°-la de maneira mais eficiente e eficaz. S√≥ voc√™ especificar o argumento QR = TRUE dentro da fun√ß√µes do rstanarm, exemplo stan_glm(..., QR = TRUE).‚Ü©Ô∏é\n",
      "last_modified": "2021-02-12T09:06:47-03:00"
    },
    {
      "path": "6-Regressao_Binomial.html",
      "title": "Regress√£o Binomial",
      "description": "Modelos Lineares Generalizados -- Binomial",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nComparativo com a Regress√£o Linear\nExemplo\n\nRegress√£o log√≠stica com o rstanarm\nInterpreta√ß√£o dos coeficientes\nPriors\nAtividade Pr√°tica\nAmbiente\n\n\nSaindo do universo dos modelos lineares, come√ßamos a nos aventurar nos modelos linares generalizados (generalized linear models - GLM). O primeiro deles √© a regress√£o log√≠stica (tamb√©m chamada de regress√£o binomial).\nUma regress√£o log√≠stica se comporta exatamente como um modelo linear: faz uma predi√ß√£o simplesmente computando uma soma ponderada das vari√°veis independentes, mais uma constante. Por√©m ao inv√©s de retornar um valor cont√≠nuo, como a regress√£o linear, retorna a fun√ß√£o log√≠stica desse valor.\n\\[\\operatorname{Log√≠stica}(x) = \\frac{1}{1 + e^{(-x)}}\\]\nUsamos regress√£o log√≠stica quando a nossa vari√°vel dependente √© bin√°ria. Ela possui apenas dois valores distintos, geralmente codificados como \\(0\\) ou \\(1\\).\n\n\nx <- seq(-10, 10, length.out = 100)\nsig <- 1 / (1 + exp(-x))\nplot(x, sig, type = \"l\", lwd = 2, ylab = \"Log√≠stica(x)\")\n\n\n\n\nComparativo com a Regress√£o Linear\n\\[ \\operatorname{Linear} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\\]\n\\(\\operatorname{Linear}\\) - regress√£o linear\n\\(\\theta\\) - par√¢metro do modelo\n\\(n\\) - n√∫mero de atributos (features)\n\\(x_i\\) - o valor do in√©simo atributo (feature)\n\\(\\hat{p} = \\sigma(\\operatorname{Linear}) = \\frac{1}{1 + e^{-\\operatorname{Linear}}}\\)\n\\(\\hat{p}\\) - probabilidade prevista da observa√ß√£o ser 1\n\\(\\hat{y}=\\left\\{\\begin{array}{ll} 0 & \\text { se } \\hat{p} < 0.5 \\\\ 1 & \\text { se } \\hat{p} \\geq 0.5 \\end{array}\\right.\\)\nExemplo\n\\[\\mathrm{Previs√£o~de~Morte} = \\sigma \\big(-10 + 10\\times \\mathrm{cancer} + 12 \\times \\mathrm{diabetes} + 8 \\times \\mathrm{obesidade} \\big)\\]\nRegress√£o log√≠stica com o rstanarm\nO rstanarm pode tolerar qualquer modelo linear generalizado e regress√£o log√≠stica n√£o √© uma exce√ß√£o. Para rodar um modelo binomial no rstanarm √© preciso simplesmente alterar o argumento family da fun√ß√£o stan_glm.\nPara exemplo, usaremos um dataset chamado wells do pacote rstanarm. √â uma survey com 3200 residentes de uma pequena √°rea de Bangladesh na qual os len√ß√≥is fre√°ticos est√£o contaminados por ars√™nico. Respondentes com altos n√≠veis de ars√™nico nos seus po√ßos foram encorajados para trocar a sua fonte de √°gua para uma n√≠veis seguros de ars√™nico.\nPossui as seguintes vari√°veis:\nswitch: dependente indicando se o respondente trocou ou n√£o de po√ßo\narsenic: n√≠vel de ars√™nico do po√ßo do respondente\ndist: dist√¢ncia em metros da casa do respondente at√© o po√ßo seguro mais pr√≥ximo\nassociation: dummy se os membros da casa do respondente fazem parte de alguma organiza√ß√£o da comunidade\neduc: quantidade de anos de educa√ß√£o que o chefe da fam√≠lia respondente possui\n\n\noptions(mc.cores = parallel::detectCores())\noptions(Ncpus = parallel::detectCores())\n\nlibrary(rstanarm)\ndata(wells)\n\nmodel_binomial <- stan_glm(\n  switch ~ dist + arsenic + assoc + educ,\n  data = wells,\n  family = binomial()\n    )\n\n\n\n\n\nsummary(model_binomial)\n\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      switch ~ dist + arsenic + assoc + educ\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 3020\n predictors:   5\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) -0.2    0.1 -0.3  -0.2   0.0 \ndist         0.0    0.0  0.0   0.0   0.0 \narsenic      0.5    0.0  0.4   0.5   0.5 \nassoc       -0.1    0.1 -0.2  -0.1   0.0 \neduc         0.0    0.0  0.0   0.0   0.1 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.6    0.0  0.6   0.6   0.6  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  5643 \ndist          0.0  1.0  4302 \narsenic       0.0  1.0  4433 \nassoc         0.0  1.0  4583 \neduc          0.0  1.0  4852 \nmean_PPD      0.0  1.0  4369 \nlog-posterior 0.0  1.0  1947 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nInterpreta√ß√£o dos coeficientes\nAo vermos a f√≥rmula de regress√£o binomial vemos que para analisarmos o efeito de um preditor na vari√°vel dependente temos que calcular o valor log√≠stico dos coeficientes do preditor. E interpretamos como chances (odds ratio) na qual 1 √© neutro e qualquer valor abaixo de 1 tende a respostas codificadas como 0 e qualquer valor acima de 1 tende a respostas codificadas como 1.\n\\[\\text{odds ratio} = e^{(x)}\\]\n\n\ncoeff <- exp(model_binomial$coefficients)\ncoeff\n\n\n(Intercept)        dist     arsenic       assoc        educ \n       0.85        0.99        1.60        0.88        1.04 \n\n(Intercept): a chance basal de respondentes mudarem de po√ßo (15% de n√£o mudarem)\ndist: a cada metro de dist√¢ncia diminui a chance de troca de po√ßo em 1%\narsenic: a cada incremento do n√≠vel de ars√™nico aumenta a chance de troca de po√ßo em 60%\nassoc: resid√™ncias com membros que fazem parte de alguma organiza√ß√£o da comunidade diminui a chance de troca de po√ßo em 12%\neduc: a cada incremento dos anos de estudo aumenta a chance de troca de po√ßo em 4%\nPriors\nrstanarm possui as seguintes configura√ß√µes como padr√£o de priors para regress√£o binomial:\nConstante (Intercept): centralizada com m√©dia \\(\\mu = 0\\) e desvio padr√£o de \\(2.5 \\sigma_y\\) - prior_intercept = normal(0, 2.5 * sd_y)\nCoeficientes: para cada coeficiente m√©dia \\(\\mu = 0\\) and standard deviation of \\(2.5\\times\\frac{1}{\\sigma_{x_k}}\\) - prior = normal(0, 2.5 * 1/sd_xk)\nErro residual (prior_aux): uma distribui√ß√£o exponencial com taxa \\(\\frac{1}{\\sigma_y}\\): prior_aux = exponential(1/sd_y)\nAtividade Pr√°tica\nDois datasets est√£o dispon√≠veis na pasta datasets/:\nTitanic Survival: datasets/Titanic_Survival.csv\nIBM HR Analytics Employee Attrition & Performance: datasets/IBM_HR_Attrition.csv\n\n\n###\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:37:35-03:00"
    },
    {
      "path": "7-Regressao_Poisson.html",
      "title": "Regress√£o de Poisson",
      "description": "Modelos Lineares Generalizados -- Poisson",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nRegress√£o de Poisson com o rstanarm\nInterpreta√ß√£o dos coeficientes\nPriors\nAtividade Pr√°tica\nAmbiente\n\n\nSaindo do universo dos modelos lineares, come√ßamos a nos aventurar nos modelos linares generalizados (generalized linear models - GLM). O segundo deles √© a regress√£o de Poisson.\nUma regress√£o de Poisson se comporta exatamente como um modelo linear: faz uma predi√ß√£o simplesmente computando uma soma ponderada das vari√°veis independentes, mais uma constante. Por√©m ao inv√©s de retornar um valor cont√≠nuo, como a regress√£o linear, retorna o logar√≠tmo natural desse valor.\n\\[\\log(y)= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\\] que √© o mesmo que\n\\[y = e^{(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n)}\\] Regress√£o de Poisson √© usada quando a nossa vari√°vel dependente s√≥ pode tomar valores positivos e discretos (n√∫mero inteiros), geralmente em contextos de dados de contagem.\n\n\nx <- seq(-5, 5, length.out = 100)\nplot(x, exp(x), type = \"l\", lwd = 2, ylab = \"Exponencial(x)\")\n\n\n\n\nRegress√£o de Poisson com o rstanarm\nO rstanarm pode tolerar qualquer modelo linear generalizado e regress√£o de Poisson n√£o √© uma exce√ß√£o. Para rodar um modelo de Poisson no rstanarm √© preciso simplesmente alterar o argumento family da fun√ß√£o stan_glm.\nPara exemplo, usaremos um dataset chamado roaches do pacote rstanarm. √â uma base de dados com 262 observa√ß√µes sobre a efic√°cia de um sistema de controle de pragas em reduzir o n√∫mero de baratas (roaches) em apartamentos urbanos.\nPossui as seguintes vari√°veis:\ny: vari√°vel dependente - n√∫mero de baratas mortas\nroach1: n√∫mero de baratas antes da dedetiza√ß√£o\ntreatment: dummy para indicar se o apartamento foi dedetizado ou n√£o\nsenior: dummy para indicar se h√° apenas idosos no apartamento\nexposure2: n√∫mero de dias que as armadilhas de baratas foram usadas\n\n\noptions(mc.cores = parallel::detectCores())\noptions(Ncpus = parallel::detectCores())\n\nlibrary(rstanarm)\ndata(roaches)\n\nmodel_poisson <- stan_glm(\n  y ~ roach1 + treatment + senior,\n  data = roaches,\n  family = poisson()\n    )\n\n\n\n\n\nsummary(model_poisson)\n\n\n\nModel Info:\n function:     stan_glm\n family:       poisson [log]\n formula:      y ~ roach1 + treatment + senior\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 262\n predictors:   4\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept)  3.1    0.0  3.1   3.1   3.2 \nroach1       0.0    0.0  0.0   0.0   0.0 \ntreatment   -0.5    0.0 -0.5  -0.5  -0.5 \nsenior      -0.4    0.0 -0.4  -0.4  -0.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 25.6    0.4 25.1  25.6  26.2 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3726 \nroach1        0.0  1.0  3599 \ntreatment     0.0  1.0  3655 \nsenior        0.0  1.0  2811 \nmean_PPD      0.0  1.0  4134 \nlog-posterior 0.0  1.0  1811 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nInterpreta√ß√£o dos coeficientes\nAo vermos a f√≥rmula de regress√£o de Poisson vemos que para analisarmos o efeito de um preditor na vari√°vel dependente temos que calcular o valor \\(e\\) elevado ao coeficiente do preditor\n\\[y = e^{(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n)}\\]\n\n\ncoeff <- exp(model_poisson$coefficients)\ncoeff\n\n\n(Intercept)      roach1   treatment      senior \n      23.02        1.01        0.60        0.69 \n\n(Intercept): a taxa basal de extermina√ß√£o das baratas \\(y\\)\nroach1: a cada uma barata antes da extermina√ß√£o h√° um aumento de 1.01 barata exterminada a mais\ntreatment: se o apartamento foi dedetizado h√° um aumento de 0.6 barata exterminada a mais\nsenior: se o apartamento possui somente idoso h√° um aumento de 0.69 barata exterminada a mais\nPriors\nrstanarm possui as seguintes configura√ß√µes como padr√£o de priors para regress√£o de Poisson:\nConstante (Intercept): centralizada com m√©dia \\(\\mu = 0\\) e desvio padr√£o de \\(2.5 \\sigma_y\\) - prior_intercept = normal(0, 2.5 * sd_y)\nCoeficientes: para cada coeficiente m√©dia \\(\\mu = 0\\) and standard deviation of \\(2.5\\times\\frac{1}{\\sigma_{x_k}}\\) - prior = normal(0, 2.5 * 1/sd_xk)\nErro residual (prior_aux): uma distribui√ß√£o exponencial com taxa \\(\\frac{1}{\\sigma_y}\\): prior_aux = exponential(1/sd_y)\nAtividade Pr√°tica\nUm datasets est√° dispon√≠vel na pasta datasets/:\nNew York City - East River Bicycle Crossings: datasets/NYC_bicycle.csv\n\n\n###\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:37:50-03:00"
    },
    {
      "path": "8-Regressao_Robusta.html",
      "title": "Regress√£o Robusta",
      "description": "Modelos Lineares Generalizados -- $t$ de Student",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nComparativo Normal vs Student\nModelos Lineares Robustos com o pacote brms\nExemplo com os dados de Prest√≠gio de Duncan (1961)\nPriors do brms\n\nAtividade Pr√°tica\nAmbiente\n\n\nLembrando da curva normal gaussiana que possui um formato de sino. Ela n√£o √© muito alongada nas ‚Äúpontas.‚Äù Ou seja, as observa√ß√µes n√£o fogem muito da m√©dia. Quando usamos essa distribui√ß√£o como verossimilhan√ßa na infer√™ncia modelos Bayesianos, for√ßamos a que todas as estimativas sejam condicionadas √† uma distribui√ß√£o normal da vari√°vel dependente. Se nos dados houverem muitas observa√ß√µes com valores discrepantes (bem diferentes da m√©dia - outliers), isso faz com que as estimativas dos coeficientes das vari√°veis independentes fiquem inst√°veis. Isso ocorre porqu√™ a distribui√ß√£o normal n√£o consegue contemplar observa√ß√µes muito divergentes da m√©dia sem mudar a m√©dia de local.\n\n\nx <- seq(-4, 4, length = 100)\nplot(x, dnorm(x),\n     type = \"l\",\n     col = \"red\",\n     lwd = 3,\n     xlab = \"valor de x\",\n     ylab = \"Densidade\",\n     main = \"Distribui√ß√£o Normal\",\n     sub = \"M√©dia 0 e Desvio Padr√£o 1\",\n     xlim = c(-4, 4),\n     ylim = c(0, 0.4))\n\n\n\n\nEnt√£o precisamos de uma distribui√ß√£o mais ‚Äúmale√°vel‚Äù como verossimilhan√ßa. Precisamos de uma distribui√ß√£o que seja mais robusta √† observa√ß√µes discrepantes (outliers). Precisamos de uma distribui√ß√£o similar √† Normal mas que possua caudas mais longas para justamente contemplar observa√ß√µes muito longe da m√©dia sem ter que mudar a m√©dia de local. Para isso temos a distribui√ß√£o t de Student. Lembrando o formato dela:\n\n\nx <- seq(-4, 4, length = 100)\nplot(x, dt(x, 2),\n     type = \"l\",\n     col = \"blue\",\n     lwd = 3,\n     xlab = \"valor de x\",\n     ylab = \"Densidade\",\n     main = \"Distribui√ß√£o t de Student\",\n     sub = \"M√©dia 0 e Graus de Liberdade 2\",\n     xlim = c(-4, 4),\n     ylim = c(0, 0.4))\n\n\n\n\nComparativo Normal vs Student\nReparem nas caudas:\n\n\nplot(NA, xlab = \"valor de x\",\n  ylab = \"Densidade\",\n  main = \"Comparativo de Distribui√ß√µes\",\n  sub = \"Normal vs t de Student\",\n  xlim = c(-4, 4),\n  ylim = c(0, 0.4))\nlines(x, dnorm(x), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 2), lwd = 2, col = \"blue\")\nlegend(\"topright\", legend = c(\"Normal\", \"Student\"),\n       col = c(\"red\", \"blue\"), title = \"Distribui√ß√µes\", lty = 1)\n\n\n\n\nModelos Lineares Robustos com o pacote brms\nO rstanarm n√£o possui a possibilidade de usar distribui√ß√µes t de Student como verossimilhan√ßa do modelo Bayesiano. Para usarmos distribui√ß√µes t de Student, precisamos do pacote brms. O brms usa a mesma s√≠ntaxe que o rstanarm e a √∫nica diferen√ßa √© que o brms n√£o possui os modelos pr√©-compilados ent√£o os modelos devem ser todos compilados antes de serem rodados. A diferen√ßa pr√°tica √© que voc√™ ir√° esperar alguns instantes antes do R come√ßar a simular MCMC e amostrar do modelo.\nA fun√ß√£o que usa-se para designar modelos lineares no brms √© a brm():\n\n\nbrm(y ~ x1 + x2 + x3,\n    data = df,\n    family = student)\n\n\n\nExemplo com os dados de Prest√≠gio de Duncan (1961)\nPara exemplicar regress√£o robusta vamos usar um dataset que tem muitas observa√ß√µes discrepantes (outliers) chamado Duncan. Ele possui 45 observa√ß√µes sobre ocupa√ß√µes nos EUA e 4 vari√°veis:\ntype: Tipo de ocupa√ß√£o. Uma vari√°vel qualitativa:\nprof - profissional ou de gest√£o\nwc - white-collar (colarinho branco)\nbc - blue-collar (colarinho azul)\n\nincome: Porcentagem de pessoas da ocupa√ß√£o que ganham acima $ 3.500 por ano em 1950 (mais ou menos $36.000 em 2017);\neducation: Porcentagem de pessoas da ocupa√ß√£o que possuem diploma de ensino m√©dio em 1949 (que, sendo c√≠nicos, podemos dizer que √© de certa maneira equivalente com diploma de Doutorado em 2017); e\nprestige:Porcentagem de respondentes na pesquisa que classificam a sua ocupa√ß√£o como no m√≠nimo ‚Äúboa‚Äù em respeito √† prest√≠gio.\n\n\nduncan <- read.csv2(\"datasets/Duncan.csv\", row.names = 1, stringsAsFactors = TRUE)\n\nhist(duncan$prestige,\n     main = \"Histograma do Prest√≠gio\",\n     xlab = \"Prest√≠gio\",\n     ylab = \"Frequ√™ncia\")\n\n\n\n\nPrimeiro modelo: Regress√£o Linear\nVamos estimar primeiramente uma regress√£o linear usando a distribui√ß√£o Normal como verossimilhan√ßa:\n\n\nlibrary(rstanarm)\nmodel_1 <- stan_glm(\n  prestige ~ income + education,\n  data = duncan,\n  family = gaussian\n)\n\n\n\nE na sequ√™ncia o sum√°rio das estimativas do modelo, assim como os diagn√≥sticos da MCMC:\n\n\nsummary(model_1)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      prestige ~ income + education\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 45\n predictors:   3\n\nEstimates:\n              mean   sd    10%   50%   90%\n(Intercept)  -6.1    4.3 -11.7  -6.1  -0.6\nincome        0.6    0.1   0.4   0.6   0.8\neducation     0.5    0.1   0.4   0.5   0.7\nsigma        13.7    1.5  11.8  13.6  15.7\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 47.6    2.9 43.9  47.6  51.3 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  5171 \nincome        0.0  1.0  1988 \neducation     0.0  1.0  2026 \nsigma         0.0  1.0  2501 \nmean_PPD      0.0  1.0  3393 \nlog-posterior 0.0  1.0  1338 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nAparentemente parece que o modelo possui boas m√©tricas mas quando olhamos o posterior predictive check, vemos uma bagun√ßa:\n\n\npp_check(model_1, nsamples = 45)\n\n\n\n\nSegundo modelo: Regress√£o Robusta\nPara rodar um modelo Bayesiano que usa como verossimilhan√ßa a distribui√ß√£o t de Student √© somente usar a mesma s√≠ntaxe que o stan_glm mas colocando argumento family = student:\n\n\nlibrary(brms)\nmodel_2 <- brm(\n  prestige ~ income + education,\n  data = duncan,\n  family = student)\n\n\n\nE na sequ√™ncia o sum√°rio das estimativas do modelo, assim como os diagn√≥sticos da MCMC. Vemos que as estimativas n√£o alteraram muito. Al√©m disso temos um novo par√¢metro estimado pelo modelo que √© o par√¢metro nu (\\(\\nu\\)), que √© os graus de liberdade da distribui√ß√£o t de Student usada como verossimilhan√ßa:\n\n\nsummary(model_2, prob =  0.9)\n\n\n Family: student \n  Links: mu = identity; sigma = identity; nu = identity \nFormula: prestige ~ income + education \n   Data: duncan (Number of observations: 45) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -6.65      4.19   -13.40     0.26 1.00     4079     3241\nincome        0.66      0.14     0.43     0.89 1.00     1695     1996\neducation     0.51      0.11     0.33     0.70 1.00     1767     1867\n\nFamily Specific Parameters: \n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nsigma    12.42      1.88     9.39    15.56 1.00     1985     1670\nnu       18.39     13.65     3.64    45.00 1.00     2117     1708\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nMas a posterior predictive check ficou com um aspecto muito melhor que o modelo linear:\n\n\npp_check(model_2, nsamples = 45)\n\n\n\n\nPriors do brms\nbrms possui as seguintes configura√ß√µes como padr√£o de priors para regress√£o robusta usando t de Student:\nConstante (Intercept): t de Student com m√©dia \\(\\mu = \\text{median}_y\\), desvio padr√£o de \\(\\max(2.5, MAD(y)\\) e graus de liberdade \\(3\\) - prior = student_t(3, median_y, mad_y), class = intercept\nCoeficientes: para cada coeficiente m√©dia \\(\\mu = 0\\) e desvio padr√£o de \\(2.5\\times\\frac{1}{\\sigma_{x_k}}\\) - prior = normal(0, 2.5 * 1/sd_xk)\nErro residual (sigma): t de Student com m√©dia \\(\\mu = 0\\), desvio padr√£o de \\(\\max(2.5, MAD(y)\\) e graus de liberdade \\(3\\) - prior = student_t(3, 0, mad_y), class = sigma\nGraus de liberdade (nu): distribui√ß√£o gamma com \\(\\alpha = 2\\) e \\(\\beta = 0.1\\) - prior = gamma(2, 0.1), class = nu\nAtividade Pr√°tica\nO dataset Boston Housing est√° dispon√≠vel em datasets/Boston_Housing.csv. Possui 506 observa√ß√µes e possui 14 vari√°veis:\nCRIM - per capita crime rate by town\nZN - proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS - proportion of non-retail business acres per town.\nCHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\nNOX - nitric oxides concentration (parts per 10 million)\nRM - average number of rooms per dwelling\nAGE - proportion of owner-occupied units built prior to 1940\nDIS - weighted distances to five Boston employment centres\nRAD - index of accessibility to radial highways\nTAX - full-value property-tax rate per $10,000\nPTRATIO - pupil-teacher ratio by town\nB - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\nLSTAT - % lower status of the population\nMEDV - Median value of owner-occupied homes in $1000‚Äôs\n\n\n###\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:38:19-03:00"
    },
    {
      "path": "9-Regressao_Multinivel.html",
      "title": "Modelos Multiniveis",
      "description": "Modelos Multiniveis ou Modelos Hier√°rquicos",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nQuando usar Modelos Multin√≠veis?\nHyperprior\nTr√™s abordagens\nRandom Intercept Model\nRandom Slope Model\nRandom Intercept-Slope Model\nExemplo com o dataset cheese\n\nPriors de Modelos Multin√≠veis\nAtividade Pr√°tica\nAmbiente\n\n\nModelos hier√°rquicos Bayesianos (tamb√©m chamados de modelos multin√≠veis) s√£o um modelo estat√≠stico escrito em n√≠veis m√∫ltiplos (forma hier√°rquica) que estima os par√¢metros da distribui√ß√£o posterior usando o m√©todo Bayesiano. Os submodelos se combinam para formar o modelo hier√°rquico, e o teorema de Bayes √© usado para integr√°-los aos dados observados e contabilizar toda a incerteza que est√° presente. O resultado dessa integra√ß√£o √© a distribui√ß√£o posterior, tamb√©m conhecida como estimativa de probabilidade atualizada, √† medida que evid√™ncias adicionais sobre a distribui√ß√£o anterior s√£o adquiridas.\nA modelagem hier√°rquica √© usada quando as informa√ß√µes est√£o dispon√≠veis em v√°rios n√≠veis diferentes de unidades de observa√ß√£o. A forma hier√°rquica de an√°lise e organiza√ß√£o auxilia no entendimento de problemas multipar√¢metros e tamb√©m desempenha um papel importante no desenvolvimento de estrat√©gias computacionais.\nOs modelos hier√°rquicos s√£o descri√ß√µes matem√°ticas que envolvem v√°rios par√¢metros, de modo que as estimativas de alguns par√¢metros dependem significativamente dos valores de outros par√¢metros.\nModelo Hier√°rquicoQuando usar Modelos Multin√≠veis?\nModelos multin√≠veis s√£o particularmente apropriados para projetos de pesquisa onde os dados dos participantes s√£o organizados em mais de um n√≠vel (ou seja, dados aninhados - nested data). As unidades de an√°lise geralmente s√£o indiv√≠duos (em um n√≠vel inferior) que est√£o aninhados em unidades contextuais/agregadas (em um n√≠vel superior).\nH√° um pressuposto principal que n√£o pode ser violado em modelos multin√≠veis que √© o de permutabilidade. Esse pressuposto parte do princ√≠pio que os grupos s√£o permut√°veis. Se esse pressuposto √© violado na sua infer√™ncia, ent√£o modelos multin√≠veis n√£o s√£o apropriados.\nHyperprior\nComo as priors dos par√¢metros s√£o amostradas de uma outra prior do hiperpar√¢metro (par√¢metro do n√≠vel superior), as priors do n√≠vel superior s√£o chamadas de hyperpriors. Isso faz com que estimativas de um grupo ajudem o modelo a estimar melhor os outros grupos e dando estimativas mais robustas e est√°veis.\nTr√™s abordagens\nModelos multin√≠veis geralmente se dividem em tr√™s abordagens:\nRandom intercept model: Modelo no qual cada grupo recebe uma constante (intercept) diferente\nRandom slope model: Modelo no qual cada grupo recebe um coeficiente diferente para cada vari√°vel independente\nRandom intercept-slope model: Modelo no qual cada grupo recebe tanto uma constante (intercept) quanto um coeficiente diferente para cada vari√°vel independente\nRandom Intercept Model\nA primeira abordagem √© o random intercept model na qual especificamos para cada grupo uma constante diferente. Essas constantes s√£o amostrada de uma hyperprior.\nO pacote rstanarm tem as funcionalidades completas para rodar modelos multin√≠veis e a √∫nica coisa a se fazer √© alterar a formula. H√° uma segunda mudan√ßa tamb√©m que n√£o usamos mais a fun√ß√£o stan_glm() mas sim a fun√ß√£o stan_glmer().\nNo caso de random intercept model, a formula a ser usada segue este padr√£o:\ny ~ (1 | group) + x1 + x2\nRandom Slope Model\nA segunda abordagem √© o random slope model na qual especificamos para cada grupo um coeficiente diferente para cada vari√°vel independente. Esses coeficientes s√£o amostrada de uma hyperprior.\nNo caso de random slope model, a formula a ser usada segue este padr√£o:\ny ~ (0 + x1 | group) + (0 + x2 | group)\nRandom Intercept-Slope Model\nA terceira abordagem √© o random intercept-slope model na qual especificamos para cada grupo uma constante diferente al√©m de coeficientes diferentes para cada vari√°vel independente. Essas constantes e coeficientes s√£o amostrados de duas ou mais hyperpriors.\nNo caso de random intercept-slope model, a formula a ser usada segue este padr√£o:\ny ~ (1 + x1 | group) + (1 + x2 | group)\nExemplo com o dataset cheese\nO dataset cheese possui 160 observa√ß√µes de avalia√ß√µes de queijo. Um grupo de 10 avaliadores ‚Äúrurais‚Äù e 10 ‚Äúurbanos‚Äù avaliaram 4 queijos diferentes \\((A,B,C,D)\\) em duas amostras. Portanto \\(4 \\cdot 20 \\cdot 2 = 160\\). Possui 4 vari√°veis:\ncheese: tipo do queijo \\((A,B,C,D)\\)\nrater: avaliador \\((1,\\dots, 10)\\)\nbackground: origem do avaliador em ‚Äúurbano‚Äù ou ‚Äúrural‚Äù\ny: vari√°vel dependente - nota da avalia√ß√£o\n\n\ncheese <- read.csv2(\"datasets/cheese.csv\", stringsAsFactors = TRUE, row.names = 1)\n\n\n\nRandom Intercept Model\nNo primeiro exemplo vamos usar um modelo que cada grupo de cheese recebe uma constante diferente:\n\n\nlibrary(rstanarm)\nrandom_intercept <- stan_glmer(\n  y ~ (1 | cheese) + background,\n  data = cheese\n)\n\n\n\nNo sum√°rio do modelo vemos que os avaliadores urbanos avaliam melhor os queijos que os avaliadores rurais, mas tamb√©m observamos que cada queijo possui uma ‚Äútaxa basal‚Äù de avalia√ß√£o. Sendo \\(B\\) o pior queijo e \\(C\\) o melhor queijo:\n\n\nsummary(random_intercept)\n\n\n\nModel Info:\n function:     stan_glmer\n family:       gaussian [identity]\n formula:      y ~ (1 | cheese) + background\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 160\n groups:       cheese (4)\n\nEstimates:\n                                        mean   sd    10%   50%   90%\n(Intercept)                            67.2    5.9  60.3  67.2  74.1\nbackgroundurban                         7.4    1.1   5.9   7.4   8.7\nb[(Intercept) cheese:A]                 3.8    5.9  -3.1   3.7  10.8\nb[(Intercept) cheese:B]               -14.1    5.9 -21.2 -13.9  -7.1\nb[(Intercept) cheese:C]                 8.6    5.9   1.7   8.6  15.7\nb[(Intercept) cheese:D]                 1.4    5.9  -5.6   1.4   8.3\nsigma                                   7.1    0.4   6.6   7.1   7.6\nSigma[cheese:(Intercept),(Intercept)] 138.9  131.0  43.1  98.7 270.7\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 70.8    0.8 69.9  70.8  71.8 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                      mcse Rhat n_eff\n(Intercept)                           0.2  1.0  1003 \nbackgroundurban                       0.0  1.0  3702 \nb[(Intercept) cheese:A]               0.2  1.0   998 \nb[(Intercept) cheese:B]               0.2  1.0  1013 \nb[(Intercept) cheese:C]               0.2  1.0  1009 \nb[(Intercept) cheese:D]               0.2  1.0  1014 \nsigma                                 0.0  1.0  3482 \nSigma[cheese:(Intercept),(Intercept)] 3.5  1.0  1365 \nmean_PPD                              0.0  1.0  4036 \nlog-posterior                         0.1  1.0  1307 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nRandom Slope Model\nNo segundo exemplo vamos usar um modelo que cada grupo de cheese recebe um coeficiente diferente para background:\n\n\nrandom_slope <- stan_glmer(\n  y ~ (0 + background | cheese),\n  data = cheese\n)\n\n\n\nAqui vemos que todos os queijos recebem a mesma constante mas cada queijo possui um coeficiente diferente para background do avaliador:\n\n\nsummary(random_slope)\n\n\n\nModel Info:\n function:     stan_glmer\n family:       gaussian [identity]\n formula:      y ~ (0 + background | cheese)\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 160\n groups:       cheese (4)\n\nEstimates:\n                                                mean   sd    10%\n(Intercept)                                    70.0    5.8  63.0\nb[backgroundrural cheese:A]                     0.7    5.9  -6.7\nb[backgroundurban cheese:A]                     8.6    6.0   1.1\nb[backgroundrural cheese:B]                   -15.8    6.2 -23.4\nb[backgroundurban cheese:B]                   -10.3    5.8 -17.3\nb[backgroundrural cheese:C]                     5.3    5.9  -1.9\nb[backgroundurban cheese:C]                    13.5    6.1   5.9\nb[backgroundrural cheese:D]                    -0.7    5.9  -8.0\nb[backgroundurban cheese:D]                     5.3    6.0  -2.2\nsigma                                           7.1    0.4   6.6\nSigma[cheese:backgroundrural,backgroundrural] 136.3  131.4  40.9\nSigma[cheese:backgroundurban,backgroundrural]  75.5   92.7   5.7\nSigma[cheese:backgroundurban,backgroundurban] 160.7  144.9  49.9\n                                                50%   90%\n(Intercept)                                    70.0  77.2\nb[backgroundrural cheese:A]                     0.6   7.8\nb[backgroundurban cheese:A]                     8.7  15.9\nb[backgroundrural cheese:B]                   -15.7  -8.3\nb[backgroundurban cheese:B]                   -10.4  -3.2\nb[backgroundrural cheese:C]                     5.3  12.5\nb[backgroundurban cheese:C]                    13.5  21.1\nb[backgroundrural cheese:D]                    -0.7   6.3\nb[backgroundurban cheese:D]                     5.3  12.5\nsigma                                           7.1   7.7\nSigma[cheese:backgroundrural,backgroundrural]  96.6 274.3\nSigma[cheese:backgroundurban,backgroundrural]  53.6 170.5\nSigma[cheese:backgroundurban,backgroundurban] 117.4 311.3\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 70.8    0.8 69.8  70.8  71.9 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                              mcse Rhat n_eff\n(Intercept)                                   0.2  1.0   662 \nb[backgroundrural cheese:A]                   0.2  1.0   674 \nb[backgroundurban cheese:A]                   0.2  1.0   716 \nb[backgroundrural cheese:B]                   0.2  1.0   689 \nb[backgroundurban cheese:B]                   0.2  1.0   716 \nb[backgroundrural cheese:C]                   0.2  1.0   683 \nb[backgroundurban cheese:C]                   0.2  1.0   680 \nb[backgroundrural cheese:D]                   0.2  1.0   673 \nb[backgroundurban cheese:D]                   0.2  1.0   682 \nsigma                                         0.0  1.0  4728 \nSigma[cheese:backgroundrural,backgroundrural] 3.1  1.0  1742 \nSigma[cheese:backgroundurban,backgroundrural] 2.5  1.0  1362 \nSigma[cheese:backgroundurban,backgroundurban] 4.7  1.0   942 \nmean_PPD                                      0.0  1.0  3721 \nlog-posterior                                 0.1  1.0  1050 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nRandom Intercept-Slope Model\nNo terceiro exemplo vamos usar um modelo que cada grupo de cheese recebe uma constante diferente e um coeficiente diferente para background:\n\n\nrandom_intercept_slope <- stan_glmer(\n  y ~ (1 + background | cheese),\n  data = cheese\n)\n\n\n\nAqui vemos que os queijos recebem a constantes diferentes e que cada queijo possui um coeficiente diferente para background do avaliador:\n\n\nsummary(random_intercept_slope)\n\n\n\nModel Info:\n function:     stan_glmer\n family:       gaussian [identity]\n formula:      y ~ (1 + background | cheese)\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 160\n groups:       cheese (4)\n\nEstimates:\n                                                mean   sd    10%\n(Intercept)                                    65.8    7.8  56.3\nb[(Intercept) cheese:A]                         5.1    7.8  -4.3\nb[backgroundurban cheese:A]                     7.8    2.2   5.0\nb[(Intercept) cheese:B]                       -11.2    8.1 -21.0\nb[backgroundurban cheese:B]                     4.7    2.3   1.7\nb[(Intercept) cheese:C]                         9.5    7.8   0.4\nb[backgroundurban cheese:C]                     8.3    2.2   5.4\nb[(Intercept) cheese:D]                         3.5    7.8  -5.8\nb[backgroundurban cheese:D]                     6.0    2.1   3.3\nsigma                                           7.1    0.4   6.6\nSigma[cheese:(Intercept),(Intercept)]         149.5  144.5  43.3\nSigma[cheese:backgroundurban,(Intercept)]      16.8   75.6 -55.4\nSigma[cheese:backgroundurban,backgroundurban]  85.6   82.5  24.2\n                                                50%   90%\n(Intercept)                                    65.7  75.0\nb[(Intercept) cheese:A]                         5.1  14.6\nb[backgroundurban cheese:A]                     7.8  10.5\nb[(Intercept) cheese:B]                       -11.1  -1.4\nb[backgroundurban cheese:B]                     4.7   7.6\nb[(Intercept) cheese:C]                         9.5  18.9\nb[backgroundurban cheese:C]                     8.3  11.2\nb[(Intercept) cheese:D]                         3.7  12.9\nb[backgroundurban cheese:D]                     6.0   8.7\nsigma                                           7.1   7.7\nSigma[cheese:(Intercept),(Intercept)]         104.7 295.0\nSigma[cheese:backgroundurban,(Intercept)]      13.7  92.4\nSigma[cheese:backgroundurban,backgroundurban]  61.7 168.3\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 70.8    0.8 69.8  70.8  71.9 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                              mcse Rhat n_eff\n(Intercept)                                   0.3  1.0   664 \nb[(Intercept) cheese:A]                       0.3  1.0   675 \nb[backgroundurban cheese:A]                   0.0  1.0  4607 \nb[(Intercept) cheese:B]                       0.3  1.0   670 \nb[backgroundurban cheese:B]                   0.0  1.0  2970 \nb[(Intercept) cheese:C]                       0.3  1.0   684 \nb[backgroundurban cheese:C]                   0.0  1.0  3896 \nb[(Intercept) cheese:D]                       0.3  1.0   664 \nb[backgroundurban cheese:D]                   0.0  1.0  5162 \nsigma                                         0.0  1.0  3677 \nSigma[cheese:(Intercept),(Intercept)]         3.9  1.0  1384 \nSigma[cheese:backgroundurban,(Intercept)]     2.6  1.0   836 \nSigma[cheese:backgroundurban,backgroundurban] 1.9  1.0  1944 \nmean_PPD                                      0.0  1.0  3871 \nlog-posterior                                 0.1  1.0  1126 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nPriors de Modelos Multin√≠veis\nRelembrando a tabela de priors da Aula 4:\nArgumento\nUsado em\nAplica-se √†\nprior_intercept\nTodas fun√ß√µes de modelagem exceto stan_polr and stan_nlmer\nConstante (intercept) do modelo, ap√≥s centraliza√ß√£o dos preditores\nprior\nTodas fun√ß√µes de modelagem\nCoeficientes de Regress√£o, n√£o inclui coeficientes que variam por grupo em modelos multin√≠veis (veja prior_covariance)\nprior_aux\nstan_glm, stan_glmer, stan_gamm4, stan_nlmer\nPar√¢metro auxiliar (ex: desvio padr√£o (standard error - DP), interpreta√ß√£o depende do modelo\nprior_covariance\nstan_glmer, stan_gamm4, stan_nlmer\nMatrizes de covari√¢ncia em modelos multin√≠veis\nConstante(Intercept): centralizada com m√©dia \\(\\mu_{y_{group}}\\) para cada grupo e desvio padr√£o de \\(2.5 \\sigma_{y_{group}}\\) para cada grupo - prior_intercept = normal(mean_y_group, 2.5 * sd_y_group)\nCoeficientes: aqui n√£o especifica-se uma prior para cada coeficiente, mas sim uma prior para a matriz de correla√ß√£o das vari√°veis independentes usando uma distribui√ß√£o LKJ - prior_covariance = lkj(regularization = 1, concentration = 1, shape = 1, scale = 1)\nAtividade Pr√°tica\nPara atividade pr√°tica, temos o dataset rikz em datasets/rikz.csv.\nFor each of 9 intertidal areas (denoted ‚ÄòBeaches‚Äô), the researchers sampled five sites (denoted ‚ÄòSites‚Äô) and at each site they measured abiotic variables and the diversity of macro-fauna (e.g.¬†aquatic invertebrates). Here, species richness refers to the total number of species found at a given site while NAP ( i.e.¬†Normal Amsterdams Peil) refers to the height of the sampling location relative to the mean sea level and represents a measure of the amount of food available for birds, etc. For our purpose, the main question is:\nWhat is the influence of NAP on species richness?\nRikz Dataset\n\nrikz <- read.csv2(\"datasets/rikz.csv\", row.names = 1)\nrikz$Beach <- as.factor(rikz$Beach)\nrikz$Site <- as.factor(rikz$Site)\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:39:07-03:00"
    },
    {
      "path": "aux-Dados_Faltantes.html",
      "title": "Dados Faltantes",
      "description": "Dados Faltantes",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nRemover dados faltantes\nImputar valores nos dados faltantes\nImputar a m√©dia\nImputar a mediana\nImputar o √∫ltimo valor ocorrido\n\nCompara√ß√£o dos resultados\nAmbiente\n\n\nDados faltantes s√£o um problema comum em qualquer an√°lise de dados. Tanto o rstan, quanto brms e rstanarm usam observa√ß√µes completas nas suas infer√™ncias. Ent√£o, toma observa√ß√£o que contiver qualquer dado faltante ser√° removida por completa. Temos duas abordagens b√°sicas para lidar com dados faltantes1:\nremover os dados faltantes\nimputar valores nos dados faltantes\nRemover dados faltantes\nA remo√ß√£o de dados faltantes se divide em duas principais abordagens usando a fun√ß√£o na.omit() do pacote base stats:\nremo√ß√£o de observa√ß√µes com dados faltantes: aqui removemos as linhas com dados faltantes df <- na.omit(df)\nremo√ß√£o de vari√°veis com dados faltantes: aqui removemos as colunas com dados faltantes df <- t(na.omit(t(df)))\nImputar valores nos dados faltantes\nDentre as diversas maneiras de imputar valores ao dados faltantes, as mais comuns s√£o tr√™s:\nimputar a m√©dia\nimputar a mediana\nimputar o √∫ltimo valor ocorrido (muito usada em s√©ries temporais)\nMas ainda h√° maneiras mais avan√ßadas e que desempenham melhor em certas condi√ß√µes (n√£o cobriremos essas t√©cnicas nesse curso):\nk-nearest neighbors imputation\nrandom forest imputation\nH√° um pacote de R chamado DescTools que √© uma cole√ß√£o de fun√ß√µes focadas especialmente na parte descritiva de an√°lise de um dataset.\nPara mostrar as abordagens, geramos um dataset de uma s√©rie temporal de uma semana com dados faltantes:\n\n\nlibrary(DescTools)\nset.seed(123)\ndf <- data.frame(\n  dia = c(\"seg\", \"ter\", \"qua\", \"qui\", \"sex\", \"sab\", \"dom\"),\n  valor = runif(7))\nindices_aleatorios <- sample(1:nrow(df), 2)\ndf[indices_aleatorios[1], 2] <- NA\ndf[indices_aleatorios[2], 2] <- NA\n\n\n\nImputar a m√©dia\n\n\ndf$media <- Impute(df$valor, FUN = mean(df$valor, na.rm = T))\n\n\n\nImputar a mediana\n\n\ndf$mediana <- Impute(df$valor, FUN = median(df$valor, na.rm = T))\n\n\n\nImputar o √∫ltimo valor ocorrido\n\n\ndf$ultimo <- LOCF(df$valor)\n\n\n\nCompara√ß√£o dos resultados\n\n\ndf\n\n\n  dia valor media mediana ultimo\n1 seg  0.29  0.29    0.29   0.29\n2 ter  0.79  0.79    0.79   0.79\n3 qua    NA  0.69    0.79   0.79\n4 qui  0.88  0.88    0.88   0.88\n5 sex  0.94  0.94    0.94   0.94\n6 sab    NA  0.69    0.79   0.94\n7 dom  0.53  0.53    0.53   0.53\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\nh√° uma terceira que √© modelar os dados faltantes, veja a vinheta do brms para mais detalhes‚Ü©Ô∏é\n",
      "last_modified": "2021-02-12T06:39:10-03:00"
    },
    {
      "path": "aux-Regressao_Coeficientes.html",
      "title": "Coeficientes de uma Regress√£o",
      "description": "Diferen√ßas entre Coeficientes Padronizados vs Brutos",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nSimula√ß√£o\nM√©dia e Desvio Padr√µes\nCoeficientes Brutos vs Padronizados\n\nAmbiente\n\n\nEm tabelas de regress√£o temos geralmente temos duas op√ß√µes de reportar os coeficientes:\nCoeficientes Brutos: n√£o h√° transforma√ß√µes e as associa√ß√µes das vari√°veis independentes/controles (covari√°veis) com a dependente s√£o reportadas em suas medidas originais. Exemplo: A cada 1 unidades de aumento de \\(x\\), \\(y\\) aumenta 0.45.\nCoeficientes Padronizados: os coeficientes s√£o transformados para expressarem as associa√ß√µes das vari√°veis independentes/controles (covari√°veis) com a dependente em rela√ß√£o √† varia√ß√£o dos seus desvios padr√µes. Exemplo: A cada 1 desvio padr√£o de varia√ß√£o positiva de \\(x\\), \\(y\\) possui varia√ß√£o de 0.1 desvio padr√£o.\nSimula√ß√£o\nPara explicar melhor esses conceitos, simularemos alguns dados:\n\\(x\\): 1,000 observa√ß√µes amostradas de uma distribui√ß√£o normal com m√©dia 1 e desvio padr√£o 0.1. \\(x \\sim \\mathcal{N} (1, 0.1)\\)\n\\(y\\): uma combina√ß√£o linear de \\(100x\\) com uma constante e um erro pequeno normalmente distribu√≠do. \\(y = 10 + 100x + \\epsilon\\) e \\(\\epsilon \\sim \\mathcal{N} (0, 1)\\).\n\n\nN <- 1000\nx <- rnorm(N, 1, 0.1)\nerror <- rnorm(N, 0, 1)\ny <- rep(10, N) + 100*x + error\n\ndf <- data.frame(x, y)\n\n\n\n\n\nlibrary(skimr)\nskim(df)\n\n\nTable 1: Data summary\nName\ndf\nNumber of rows\n1000\nNumber of columns\n2\n_______________________\n\nColumn type frequency:\n\nnumeric\n2\n________________________\n\nGroup variables\nNone\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nx\n0\n1\n1\n0.1\n0.67\n0.93\n1\n1.1\n1.3\n‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ\ny\n0\n1\n110\n10.1\n75.94\n103.01\n109\n116.5\n142.7\n‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ\n\nM√©dia e Desvio Padr√µes\nPrestem aten√ß√£o:\n\\(x\\): m√©dia 1, desvio padr√£o 0.1\n\\(y\\): m√©dia 109.6, desvio padr√£o 10.05\nCoeficientes Brutos vs Padronizados\nAgora vamos rodar uma regress√£o e mostrar coeficientes tanto os coeficientes brutos e os padronizados\n\n\nlibrary(lm.beta)\nmodel <- lm.beta(lm(y ~ x, df))\nsummary(model)\n\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.701 -0.727 -0.009  0.683  2.728 \n\nCoefficients:\n            Estimate Standardized Std. Error t value\n(Intercept)    9.460        0.000      0.324    29.2\nx            100.506        0.995      0.324   310.5\n                       Pr(>|t|)    \n(Intercept) <0.0000000000000002 ***\nx           <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 998 degrees of freedom\nMultiple R-squared:  0.99,  Adjusted R-squared:  0.99 \nF-statistic: 9.64e+04 on 1 and 998 DF,  p-value: <0.0000000000000002\n\nPor fim, ambas colunas mostram a mesma coisa\nColuna n√£o padronizada Estimate: a cada 1 unidade que \\(x\\) aumenta, \\(y\\) aumenta 100.51\nColuna padronizada Standardized: a cada 1 desvio padr√£o de \\(x\\) de incremento (dp = 0.1), h√° um aumento de 0.99 desvio padr√£o de \\(y\\) (10). Um total de 100.51. \\(\\big( \\frac{0.955 * \\operatorname{sd}_y}{\\operatorname{sd}_x}\\big)\\)\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:39:11-03:00"
    },
    {
      "path": "aux-Tabelas_para_Publicacao.html",
      "title": "Tabelas para Publica√ß√£o",
      "description": "Como montar tabelas de modelos Bayesianos prontas para publica√ß√£o",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nEstat√≠sticas Descritivas\nTabela de Correla√ß√µes\nRegress√£o Linear Bayesiana\nTabela de Regress√£o Linear\n\nModelo de Regress√£o Binomial/Log√≠stica\nTabela de Regress√£o Binomial/Log√≠stica\n\nAmbiente\n\n\n\nknitr::opts_chunk$set(echo = TRUE)\n# Detectar quantos cores/processadores\noptions(mc.cores = parallel::detectCores())\noptions(Ncpus = parallel::detectCores())\n\nlibrary(dplyr)\nlibrary(rstanarm)\nlibrary(gtsummary)\nlibrary(sjPlot)\n\n# algumas modifica√ß√µes no datadset kidiq\nkidiq <- kidiq %>%\n  mutate(mom_hs = factor(mom_hs, labels = c(\"no\", \"yes\")))\n\n\n\n\nAo inv√©s de ser obrigado a passar horas a fio formatando tabelas em Excel softwares pagos, voc√™ pode usar pacotes gratuitos do R para formatar automaticamente suas tabelas:\nEstat√≠sticas Descritivas: gtsummary::tbl_summary()\nCorrela√ß√µes: sjPlot::tab_corr()\nRegress√µes: sjPlot::tab_model()\nEstat√≠sticas Descritivas\nO pacote gtsummary possui um conjunto de fun√ß√µes para sumarizar dados e tabelas. Eu particularmente gosto da fun√ß√£o gtsummary::tbl_summary(). Ela formata uma tabela de Estat√≠stica Descritiva de maneira bem conveniente.\n\n\ngtsummary::tbl_summary(\n  kidiq,\n  by = mom_hs,\n  type = all_continuous() ~ \"continuous2\",\n  statistic = list(\n    all_continuous() ~ c(\"{N_nonmiss}\",\n                         \"{median} ({p25}, {p75})\",\n                         \"{min}, {max}\"),\n    all_categorical() ~ \"{n} ({p}%)\"),\n  missing = \"no\",\n  digits = all_continuous() ~ 2) %>%\n  # add p value and overall\n  add_p(pvalue_fun = ~style_pvalue(.x, digits = 2)) %>%\n  add_overall() %>%\n  # bold variable labels, italicize levels\n  bold_labels() %>%\n  italicize_levels() %>%\n  # change stuff\n  modify_header(label ~ \"**Variable**\") %>%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Mom High School**\") %>%\n  add_n()\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#rjepkiiayr .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#rjepkiiayr .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#rjepkiiayr .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#rjepkiiayr .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#rjepkiiayr .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#rjepkiiayr .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#rjepkiiayr .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#rjepkiiayr .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#rjepkiiayr .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#rjepkiiayr .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#rjepkiiayr .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#rjepkiiayr .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#rjepkiiayr .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#rjepkiiayr .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rjepkiiayr .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rjepkiiayr .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#rjepkiiayr .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#rjepkiiayr .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#rjepkiiayr .gt_left {\n  text-align: left;\n}\n\n#rjepkiiayr .gt_center {\n  text-align: center;\n}\n\n#rjepkiiayr .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#rjepkiiayr .gt_font_normal {\n  font-weight: normal;\n}\n\n#rjepkiiayr .gt_font_bold {\n  font-weight: bold;\n}\n\n#rjepkiiayr .gt_font_italic {\n  font-style: italic;\n}\n\n#rjepkiiayr .gt_super {\n  font-size: 65%;\n}\n\n#rjepkiiayr .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nVariable\n      N\n      Overall, N = 434\n      \n        Mom High School\n      \n      p-value1\n    no, N = 93\n      yes, N = 341\n    kid_score\n      434.00\n      \n      \n      \n      <0.001\n    N\n      \n      434.00\n      93.00\n      341.00\n      \n    Median (IQR)\n      \n      90.00 (74.00, 102.00)\n      80.00 (58.00, 95.00)\n      92.00 (77.00, 103.00)\n      \n    Range\n      \n      20.00, 144.00\n      20.00, 136.00\n      38.00, 144.00\n      \n    mom_iq\n      434.00\n      \n      \n      \n      <0.001\n    N\n      \n      434.00\n      93.00\n      341.00\n      \n    Median (IQR)\n      \n      97.92 (88.66, 110.27)\n      88.66 (81.83, 99.16)\n      100.24 (90.45, 113.17)\n      \n    Range\n      \n      71.04, 138.89\n      74.23, 127.54\n      71.04, 138.89\n      \n    mom_age\n      434.00\n      \n      \n      \n      <0.001\n    N\n      \n      434.00\n      93.00\n      341.00\n      \n    Median (IQR)\n      \n      23.00 (21.00, 25.00)\n      21.00 (20.00, 24.00)\n      23.00 (21.00, 25.00)\n      \n    Range\n      \n      17.00, 29.00\n      17.00, 28.00\n      17.00, 29.00\n      \n    \n        \n          1\n          \n           \n          Wilcoxon rank sum test\n          \n      \n    \n\nTabela de Correla√ß√µes\nPara as tabelas de correla√ß√µes, eu uso o pacote sjPlot com a fun√ß√£o sjPlot::tab_cor()\nOs astericos significam:\n* - \\(p < 0.05\\)\n** - \\(p < 0.01\\)\n*** - \\(p < 0.001\\)\n\n\nsjPlot::tab_corr(\n  kidiq %>% mutate(mom_hs = as.integer(mom_hs)),\n  digits = 2,\n  triangle = \"lower\"\n)\n\n\n\n¬†\n\n\nkid_score\n\n\nmom_hs\n\n\nmom_iq\n\n\nmom_age\n\n\nkid_score\n\n\n¬†\n\n\n¬†\n\n\n¬†\n\n\n¬†\n\n\nmom_hs\n\n\n0.24***\n\n\n¬†\n\n\n¬†\n\n\n¬†\n\n\nmom_iq\n\n\n0.45***\n\n\n0.28***\n\n\n¬†\n\n\n¬†\n\n\nmom_age\n\n\n0.09\n\n\n0.21***\n\n\n0.09\n\n\n¬†\n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\nRegress√£o Linear Bayesiana\nVamos come√ßar com o caso simples da Aula 2 - Regress√£o Linear\n\n\nmodel <- stan_glm(\n  kid_score ~ mom_hs + mom_iq,\n  data = kidiq\n  )\n\n\n\nTabela de Regress√£o Linear\nPara as tabelas de regress√µes eu geralmente uso o mesmo pacote sjPlot, mas agora com a fun√ß√£o sjPlot::tab_model() que aceita um modelo bayesiano.\n\n\ntab_model(model, show.reflvl = TRUE)\n\n\n\n¬†\n\n\nkid_score\n\n\nPredictors\n\n\nEstimates\n\n\nCI (95%)\n\n\n(Intercept)\n\n\n25.43\n\n\n13.84¬†‚Äì¬†37.36\n\n\nno\n\n\nReference\n\n\n\n\nyes\n\n\n5.99\n\n\n1.72¬†‚Äì¬†10.31\n\n\nmom_iq\n\n\n0.57\n\n\n0.45¬†‚Äì¬†0.69\n\n\nObservations\n\n\n434\n\n\nR2 Bayes\n\n\n0.216\n\n\nModelo de Regress√£o Binomial/Log√≠stica\nVamos utilizar o caso da Aula 6 - Regress√£o Binomial\n\n\nmodel_binomial <- stan_glm(\n  switch ~ dist + arsenic + assoc + educ,\n  data = wells,\n  family = binomial()\n)\n\n\n\nTabela de Regress√£o Binomial/Log√≠stica\nA fun√ß√£o sjPlot::tab_model() quando aplicada √† um modelo bayesiano linear generalizado (binomial, Poisson etc.) j√° faz a transforma√ß√£o necess√°ria para uma melhor interpreta√ß√£o dos coeficientes.\nNo caso de modelos binomiais/log√≠sticos geralmente √© aplicada uma exponencia√ß√£o (exp()) dos coeficientes para transform√°-los em raz√µes de probabilidades (odds ratio)\nCaso queira deixar os coeficientes brutos (raw coefficients) use transform = NULL\n\n\ntab_model(model_binomial, show.reflvl = TRUE)\n\n\n\n¬†\n\n\nswitch\n\n\nPredictors\n\n\nOdds Ratios\n\n\nCI (95%)\n\n\n(Intercept)\n\n\n0.85\n\n\n0.70¬†‚Äì¬†1.04\n\n\narsenic\n\n\n1.60\n\n\n1.47¬†‚Äì¬†1.73\n\n\nassoc\n\n\n0.88\n\n\n0.76¬†‚Äì¬†1.03\n\n\ndist\n\n\n0.99\n\n\n0.99¬†‚Äì¬†0.99\n\n\neduc\n\n\n1.04\n\n\n1.02¬†‚Äì¬†1.06\n\n\nObservations\n\n\n3020\n\n\nR2 Bayes\n\n\n0.067\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:39:38-03:00"
    },
    {
      "path": "index.html",
      "title": "Estat√≠stica Bayesiana com R e Stan",
      "description": "Companion para a disciplina de Estat√≠stica Bayesiana para alunos de Mestrado e Doutorado da UNINOVE",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nStan\nComo usar esse conte√∫do?\nAulas\nO que esta disciplina n√£o √©\nRStudio na N√∫vem Gratuito\nProfessor\nComo usar esse conte√∫do?\nRefer√™ncias\nLivros\nArtigos\n\nConte√∫dos Similares\nComo citar esse conte√∫do\nLicen√ßa\n\n\n\nA Estat√≠stica Bayesiana √© uma abordagem de Estat√≠stica inferencial que n√£o usa hip√≥teses nulas (\\(H_0\\)) e \\(p\\)-valores. Se voc√™ n√£o sabe o que √© um \\(p\\)-valor, recomendo olhar o tutorial sobre o que s√£o \\(p\\)-valores. Muitos cientistas e pesquisadores acreditam que sabe o que √© um \\(p\\)-valor, mas sua compreens√£o √© falha e imperfeita, por isso, mesmo que voc√™ acredite que saiba o que √© um \\(p\\)-valor, eu ainda recomendo que veja o tutorial sobre o que s√£o \\(p\\)-valores.\nStan\nStan (Carpenter et al., 2017) √© uma plataforma para modelagem estat√≠stica e computa√ß√£o estat√≠stica de alto desempenho. Milhares de usu√°rios contam com Stan para modelagem estat√≠stica, an√°lise de dados e previs√£o nas ci√™ncias sociais, biol√≥gicas e f√≠sicas, engenharia e neg√≥cios. Al√©m disso Stan tem o suporte financeiro da NumFOCUS, uma funda√ß√£o sem fins lucrativos que d√° apoio financeiro √† projetos de softwares opensource. Dentre os patrocinadores da NumFOCUS podemos citar AWS Amazon, Bloomberg, Microsoft, IBM, RStudio, Facebook, NVIDIA, Netflix, entre outras.\nOs modelos em Stan s√£o especificados pela sua pr√≥pria linguagem (similar √† C++) e s√£o compilados em um arquivo execut√°vel que gera infer√™ncias estat√≠sticas Bayesiana com amostragem MCMC de alto desempenho. Stan possui interfaces para as seguintes linguagens de programa√ß√£o1:\nR: RStan e CmdStanR\nPython: PyStan e CmdStanPy\nShell (Linha de Comando): CmdStan\nJulia: Stan.jl\nScala: ScalaStan\nMatlab: MatlabStan\nStata: StataStan\nMathematica: MathematicaStan\nA linguagem Stan possui uma curva de aprendizagem bem desafiadora, por isso Stan possui um ecossitemas de pacotes de interfaces que muitas vezes ajudam e simplificam a sua utiliza√ß√£o:\nrstanarm: ajuda o usu√°rio a especificar modelos usando a s√≠ntaxe familiar de f√≥rmulas do R.\nbrms: similar ao rstanarm pois usa a s√≠ntaxe familiar de f√≥rmulas do R, mas d√° maior flexibilidade na especifica√ß√£o de modelos mais complexos2.\nStan3 usa um amostrador Monte Carlo de correntes Markov que utiliza din√¢mica Hamiltoniana (Hamiltonian Monte Carlo ‚Äì HMC) para guiar as propostas de amostragem de novos par√¢metros no sentido do gradiente da densidade de probabilidade da posterior. Isto implica em um amostrador mais eficiente e que consegue explorar todo o espa√ßo amostral da posterior com menos itera√ß√µes; e tamb√©m mais eficaz que consegue tolerar diferentes topologias de espa√ßos amostrais da posterior. Em outras palavras, Stan usa t√©cnicas de amostragem avan√ßadas que permite com que modelos complexos Bayesianos atinjam converg√™ncia de maneira r√°pida. No Stan, raramente deve-se ajustar os par√¢metros do algoritmo HMC, pois geralmente os par√¢metros padr√µes (out-of-the-box) funcionam muito bem. Assim, o usu√°rio foca no que √© importante: a especifica√ß√£o dos componentes probabil√≠sticos do seu modelo Bayesiano.\nComo usar esse conte√∫do?\nEste conte√∫do possui licen√ßa livre para uso (CC BY-SA). Caso queira utilizar o conte√∫do para um curso ou estudos, por favor colabore nesse reposit√≥rio quaisquer aprimora√ß√µes que foram realizadas. O prop√≥sito do conte√∫do n√£o √© o rigor matem√°tico geralmente adotado em disciplinas e tutoriais de estat√≠stica Bayesiana, mas gerar uma forte intui√ß√£o deixando de lado o rigor matem√°tico e focar no ferramental (primariamente rstanarm e um pouco de brms).\nPara configurar um ambiente local:\nClone o reposit√≥rio do GitHub: git clone https://github.com/storopoli/Estatistica-Bayesiana.git\nAcesse o diret√≥rio: cd Estatistica-Bayesiana\nInstale os pacotes necess√°rios: Rscript .binder/install.R\nAulas\nO que s√£o p-valores?\nO que √© Estat√≠stica Bayesiana\nConte√∫dos Prim√°rios:\nComandos B√°sicos de R\nRegress√£o Linear\nDistribui√ß√µes Estat√≠sticas\nPriors\nMarkov Chain Montecarlo (MCMC)\nRegress√£o Binomial\nRegress√£o de Poisson\nRegress√£o Robusta\nModelos Multin√≠veis\nConte√∫dos Auxiliares:\nDados Faltantes\nCoeficientes de uma Regress√£o\nTabelas para Publica√ß√£o\nO que esta disciplina n√£o √©\nN√£o ser√° coberto conte√∫dos sobre leitura, manipula√ß√£o e exporta√ß√£o de dados com R. Para isso recomendo fortemente o livro R para Data Science (Figura 1) que pode ser encontrado gratuitamente aqui e possui uma vers√£o impressa em portugu√™s4.\n\n\n\nFigure 1: R for Data Science\n\n\n\nRStudio na N√∫vem Gratuito\nClique no √≠cone abaixo para abrir uma sess√£o do RStudio no Projeto Binder.\n\nProfessor\nProf.¬†Dr.¬†Jos√© Eduardo Storopoli    \nComo usar esse conte√∫do?\nEste conte√∫do possui licen√ßa livre para uso (CC BY-SA). Caso queira utilizar o conte√∫do para um curso ou estudos, por favor colabore nesse reposit√≥rio quaisquer aprimora√ß√µes que foram realizadas.\nPara configurar um ambiente local:\nClone o reposit√≥rio do GitHub: git clone https://github.com/storopoli/Estatistica-Bayesiana.git\nAcesse o diret√≥rio: cd Estatistica-Bayesiana\nInstale os pacotes necess√°rios: Rscript .binder/install.R\nRefer√™ncias\nLivros\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis. Chapman and Hall/CRC.\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press.\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press.\nBrooks, S., Gelman, A., Jones, G., & Meng, X.-L. (2011). Handbook of Markov Chain Monte Carlo. CRC Press. http://books.google.com?id=qfRsAIKZ4rIC\nGeyer, C. J. (2011). Introduction to markov chain monte carlo. In S. Brooks, A. Gelman, G. L. Jones, & X.-L. Meng (Eds.), Handbook of markov chain monte carlo.\n\nArtigos\nB√°sicos\nGelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., Kennedy, L., Gabry, J., B√ºrkner, P.-C., & Modr‚Äôak, M. (2020, November 3). Bayesian Workflow. http://arxiv.org/abs/2011.01808\nGabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389‚Äì402. https://doi.org/10.1111/rssa.12378\nBenjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E.-J., Berk, R., Bollen, K. A., Brembs, B., Brown, L., Camerer, C., Cesarini, D., Chambers, C. D., Clyde, M., Cook, T. D., De Boeck, P., Dienes, Z., Dreber, A., Easwaran, K., Efferson, C., ‚Ä¶ Johnson, V. E. (2018). Redefine statistical significance. Nature Human Behaviour, 2(1), 6‚Äì10. https://doi.org/10.1038/s41562-017-0189-z\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., & Riddell, A. (2017). Stan : A Probabilistic Programming Language. Journal of Statistical Software, 76(1). https://doi.org/10.18637/jss.v076.i01\nEtz, A. (2018). Introduction to the Concept of Likelihood and Its Applications. Advances in Methods and Practices in Psychological Science, 1(1), 60‚Äì69. https://doi.org/10.1177/2515245917744314\nEtz, A., Gronau, Q. F., Dablander, F., Edelsbrunner, P. A., & Baribault, B. (2018). How to become a Bayesian in eight easy steps: An annotated reading list. Psychonomic Bulletin & Review, 25(1), 219‚Äì234. https://doi.org/10.3758/s13423-017-1317-5\nMcShane, B. B., Gal, D., Gelman, A., Robert, C., & Tackett, J. L. (2019). Abandon Statistical Significance. American Statistician, 73, 235‚Äì245. https://doi.org/10.1080/00031305.2018.1527253\nAmrhein, V., Greenland, S., & McShane, B. (2019). Scientists rise up against statistical significance. Nature, 567(7748), 305‚Äì307. https://doi.org/10.1038/d41586-019-00857-9\nvan Ravenzwaaij, D., Cassey, P., & Brown, S. D. (2018). A simple introduction to Markov Chain Monte‚ÄìCarlo sampling. Psychonomic Bulletin and Review, 25(1), 143‚Äì154. https://doi.org/10.3758/s13423-016-1015-8\nVandekerckhove, J., Matzke, D., Wagenmakers, E.-J., & others. (2015). Model comparison and the principle of parsimony. In J. R. Busemeyer, Z. Wang, J. T. Townsend, & A. Eidels (Eds.), Oxford handbook of computational and mathematical psychology (pp. 300‚Äì319). Oxford University Press Oxford.\nvan de Schoot, R., Kaplan, D., Denissen, J., Asendorpf, J. B., Neyer, F. J., & van Aken, M. A. G. (2014). A Gentle Introduction to Bayesian Analysis: Applications to Developmental Research. Child Development, 85(3), 842‚Äì860. https://doi.org/10.1111/cdev.12169_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/cdev.12169\nWagenmakers, E.-J. (2007). A practical solution to the pervasive problems of p values. Psychonomic Bulletin & Review, 14(5), 779‚Äì804. https://doi.org/10.3758/BF03194105\nComplementares\nCohen, J. (1994). The earth is round (p \\(<\\) .05). American Psychologist, 49(12), 997‚Äì1003. https://doi.org/10.1037/0003-066X.49.12.997\nDienes, Z. (2011). Bayesian Versus Orthodox Statistics: Which Side Are You On? Perspectives on Psychological Science, 6(3), 274‚Äì290. https://doi.org/10.1177/1745691611406920\nEtz, A., & Vandekerckhove, J. (2018). Introduction to Bayesian Inference for Psychology. Psychonomic Bulletin & Review, 25(1), 5‚Äì34. https://doi.org/10.3758/s13423-017-1262-3\nJ‚Äôunior, C. A. M. (2020). Quanto vale o valor-p? Arquivos de Ci√™ncias Do Esporte, 7(2).\nKerr, N. L. (1998). HARKing: Hypothesizing after the results are known. Personality and Social Psychology Review, 2(3), 196‚Äì217. https://doi.org/10.1207/s15327957pspr0203_4\nKruschke, J. K., & Vanpaemel, W. (2015). Bayesian estimation in hierarchical models. In J. R. Busemeyer, Z. Wang, J. T. Townsend, & A. Eidels (Eds.), The Oxford handbook of computational and mathematical psychology (pp. 279‚Äì299). Oxford University Press Oxford, UK.\nKruschke, J. K., & Liddell, T. M. (2018). Bayesian data analysis for newcomers. Psychonomic Bulletin & Review, 25(1), 155‚Äì177. https://doi.org/10.3758/s13423-017-1272-1\nKruschke, J. K., & Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin & Review, 25(1), 178‚Äì206. https://doi.org/10.3758/s13423-016-1221-4\nLakens, D., Adolfi, F. G., Albers, C. J., Anvari, F., Apps, M. A. J., Argamon, S. E., Baguley, T., Becker, R. B., Benning, S. D., Bradford, D. E., Buchanan, E. M., Caldwell, A. R., Van Calster, B., Carlsson, R., Chen, S. C., Chung, B., Colling, L. J., Collins, G. S., Crook, Z., ‚Ä¶ Zwaan, R. A. (2018). Justify your alpha. Nature Human Behaviour, 2(3), 168‚Äì171. https://doi.org/10.1038/s41562-018-0311-x\nMorey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The fallacy of placing confidence in confidence intervals. Psychonomic Bulletin & Review, 23(1), 103‚Äì123. https://doi.org/10.3758/s13423-015-0947-8\nMurphy, K. R., & Aguinis, H. (2019). HARKing: How Badly Can Cherry-Picking and Question Trolling Produce Bias in Published Results? Journal of Business and Psychology, 34(1). https://doi.org/10.1007/s10869-017-9524-7\nStark, P. B., & Saltelli, A. (2018). Cargo-cult statistics and scientific crisis. Significance, 15(4), 40‚Äì43. https://doi.org/10.1111/j.1740-9713.2018.01174.x\nConte√∫dos Similares\nExistem alguns conte√∫dos em portugu√™s similares que eu indico:\nMarco In√°cio ‚Äî Apostila de Stan\nUm dos desenvolvedores da equipe do Stan. A apostila est√° um pouco desatualizada (2018). O foco √© o rigor matem√°tico e a linguagem Stan. Muito bem escrita e com bons exemplos.\nRicardo Ehlers (USP) ‚Äî Infer√™ncia Bayesiana (Notas de Aula)\nNotas de uma disciplina da USP pelo professor Ricardo Ehlers. O foco √© o rigor matem√°tica e as ferramentas utilizadas s√£o desatualizadas (BUGS e JAGS). Tamb√©m muito bem escrita e com bons exemplos.\nLu√≠s Gustavo Esteves, Rafael Izbicki e Rafael Bassi Stern (UFSCar) ‚Äî Infer√™ncia Bayesiana (Notas de Aula)\nNotas de uma disciplina da UFSCar pelos professores Lu√≠s Gustavo Esteves, Rafael Izbicki e Rafael Bassi Stern. O foco √© o rigor matem√°tico, mas o conte√∫do √© um pouco mais acess√≠vel com uma forte introdu√ß√£o √† l√≥gica Bayesiana. Fala um pouco da linguagem Stan e sua interface do R (rstan) no finalzinho.\nComo citar esse conte√∫do\nPara citar o conte√∫do use:\nStoropoli (2021). Estat√≠stica Bayesiana com R e Stan. Dispon√≠vel em: https://storopoli.io/Estatistica-Bayesiana.\nOu em formato BibTeX para LaTeX:\n@misc{storopoli2021estatisticabayesianaR,\n  author = {Storopoli, Jose},\n  title = {Estat√≠stica Bayesiana com R e Stan},\n  url = {https://storopoli.io/Estatistica-Bayesiana},\n  year = {2021}\n}\nLicen√ßa\nEste obra est√° licenciado com uma Licen√ßa Creative Commons Atribui√ß√£o-CompartilhaIgual 4.0 Internacional.\n\n\nestou riscando as linguagens que n√£o s√£o opensource por uma quest√£o de princ√≠pios.‚Ü©Ô∏é\ne geralmente a amostragem √© um pouco mais r√°pida que o rstanarm.‚Ü©Ô∏é\ne consequentemente todas suas interfaces com diversas linguagens de programa√ß√£o e todos os pacotes do seu ecossistema.‚Ü©Ô∏é\nN√£o temos nada a ver com a Amazon. Caso queira comprar em qualquer outra loja fique √† vontade, ou algum sebo‚Ä¶ Jeff Bezos nem sabe que eu existo‚Ä¶‚Ü©Ô∏é\n",
      "last_modified": "2021-02-12T15:44:20-03:00"
    },
    {
      "path": "pvalores.html",
      "title": "O que s√£o p-valores?",
      "description": "Porque $p$-valor, hip√≥tese nula ($H_0$) e pressupostos s√£o importantes",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nEstat√≠stica Inferencial\n\\(p\\)-valor e Hip√≥tese Nula \\(H_0\\)\nAlgumas quest√µes hist√≥ricas\nO que o \\(p\\)-valor n√£o √©\nIntervalos de Confian√ßa\nSignific√¢ncia Estat√≠stica vs Signific√¢ncia Pr√°tica\n\nErro Tipo I e Erro Tipo II\nTamanho da Amostra\nE aonde entra a Estat√≠stica Bayesiana?\nComent√°rios Finais\nAmbiente\n\n\nEsse conte√∫do foi criado com o intuito de despertar o leitor para a import√¢ncia da Estat√≠stica para a ci√™ncia e gera√ß√£o de conhecimento. Nossa ideia √© apresentar conceitos da maneira que gostar√≠amos de ter sido apresentados quando alunos prestes a serem iniciados na ci√™ncia. Nossa abordagem √© simplificar os conceitos o m√°ximo poss√≠vel sem perder a sua ess√™ncia. E, quando necess√°rio, aliando-os com sua trajet√≥ria hist√≥rica para compreens√£o do ‚Äúporque as coisas s√£o como s√£o.‚Äù N√£o estamos atr√°s de formalismo matem√°tico, mas sim de conseguir desenvolver uma intui√ß√£o clara do que √© cada conceito, quando se deve us√°-lo e quais s√£o os principais cuidados que se deve ter.\nA estat√≠stica √© dividida em duas partes:\nEstat√≠stica Descritiva: Sumariza e quantifica as caracter√≠sticas de uma amostra de dados observados. M√©tricas comuns s√£o: m√©dia, mediana, moda, desvio padr√£o, vari√¢ncia, correla√ß√£o, percentis.\nEstat√≠stica Inferencial: Permite gerar infer√™ncias (afirma√ß√µes) a partir de um conjunto de uma amostra de dados observados sobre real processo de gera√ß√£o de dados (popula√ß√£o). H√° diversas maneiras de se gerar tais infer√™ncias, mas os principais s√£o os testes de hip√≥teses cl√°ssicos que usam uma hip√≥tese nula \\(H_0\\) pr√©-especificada. A figura 1 mostra a rela√ß√£o entre dados observados e o processo de gera√ß√£o de dados sob a √≥tica da probabilidade e da estat√≠stica.\n\n\n\n{\"x\":{\"diagram\":\"\\n digraph estatistica_inferencial {\\n  forcelabels = true;\\n  graph [overlap = false,\\n         fontsize = 12,\\n         rankdir = TD]\\n  node [shape = oval,\\n        fontname = Helvetica]\\n  A [label = \\\"Processo de\\nGera√ß√£o de Dados\\\"]\\n  B [label = \\\"Dados\\nObservados\\\"]\\n  A -> B [dir = forward,\\n          xlabel = \\\"  Probabilidade  \\\",\\n          tailport = \\\"e\\\",\\n          headport = \\\"e\\\"]\\n  B -> A [dir = backward,\\n          label = \\\"  Infer√™ncia  \\\",\\n          tailport = \\\"w\\\",\\n          headport = \\\"w\\\"]\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\nFigure 1: Estat√≠stica Inferencial\n\n\n\nEstat√≠stica Inferencial\nO nosso intuito nesse conjunto de tutoriais √© focar na Estat√≠stica inferencial, porque, ao contr√°rio da Estat√≠stica descritiva, a Estat√≠stica inferencial √© raramente compreendida ao ponto do usu√°rio e consumidor estarem aptos √† realizar e consumir an√°lises, respectivamente.\nA Estat√≠stica inferencial t√™m suas origens no final do s√©culo XIX, especialmente no trabalho de Karl Pearson1 e se baseia em um conjunto de t√©cnicas e procedimentos para testar hip√≥teses sobre uma amostra generalizando para uma popula√ß√£o-alvo.\n\n\n\nFigure 2: Karl Pearson. Figura de https://www.wikipedia.org\n\n\n\nA chave para comprees√£o da Estat√≠stica inferencial se baseia em entender os testes de hip√≥teses, tamb√©m chamado de testes estat√≠sticos. Todos testes estat√≠sticos2 segue o mesmo padr√£o universal (Downey, 2016):\nCalculamos uma estat√≠stica da amostra. Aqui estat√≠stica (em letras min√∫sculas) significa uma medida dos dados. Para fins de exemplo vamos chamar essa medida de \\(\\delta\\) (letra gregra delta). Essa √© a medida que mais nos importamos: uma diferen√ßa de m√©dia, mediana ou propor√ß√µes, entre outras‚Ä¶\nContrastamos essa estat√≠stica observada com uma estat√≠stica computada se o efeito fosse nulo. Em outras palavras, o que observamos √© comparado com o resultado que esperar√≠amos caso est√≠vessemos vivendo em um mundo no qual essa medida (diferen√ßa de m√©dia, mediana ou propor√ß√µes, ‚Ä¶) fosse nula (zero). Geralmente esse universo paralelo no qual o efeito observado √© zero ou nulo √© chamado de Hip√≥tese Nula e √© representada com o seguinte s√≠mbolo \\(H_0\\). A estat√≠stica \\(\\delta\\) no mundo da \\(H_0\\) n√£o √© calculada, mas sim dada por um valor que fora matematicamente provado como o valor de \\(\\delta\\) no mundo da \\(H_0\\). Vamos chamar esse valor de \\(\\delta_0\\)\nCalculamos a probabidalide de obtermos algo como \\(\\delta\\) no mundo da \\(H_0\\): chamamos isso de \\(p\\)-valor. O \\(p\\)-valor √© a probabilidade de observarmos um \\(\\delta\\) no m√≠nimo t√£o grande quanto o observado num mundo no qual n√£o h√° o efeito \\(\\delta\\). Ou seja \\(\\delta = 0\\), e consequentemente \\(\\delta = \\delta_0\\). Como sabemos do valor \\(\\delta_0\\) de antem√£o, basta compararmos o nosso \\(\\delta\\) com \\(\\delta_0\\) para gerar o \\(p\\)-valor. Por isso que muitos livros de Estat√≠stica possuem um vasto arsenal de tabelas. O leitor pode facilmente ver o seu \\(\\delta\\) e com alguns dados sobre a amostra, em especial o n√∫mero da amostra, obter o \\(\\delta_0\\) e \\(p\\)-valor respectivos.\nDecidimos se \\(\\delta\\) possui signific√¢ncia estat√≠stica. Escolhemos um limiar de rejei√ß√£o da \\(H_0\\), muitas vezes chamado de \\(\\alpha\\) (letra gregra alpha). Esse limiar ser√° o nosso crit√©rio de decis√£o se h√° evid√™ncias suficientes para rejeitarmos o mundo da \\(H_0\\).\nEste paradigma descrito nos quatro passos acima √© chamado de Null Hypothesis Significance Testing ‚Äì NHST (tradu√ß√£o: teste de signific√¢ncia de hip√≥tese nula) e √© o que predomina em grande parte da ci√™ncia do passado e atual.\nUma segunda chave para a compreens√£o da Estat√≠stica inferencial possui raz√µes hist√≥ricas. As t√©cnicas de Estat√≠stica inferencial cl√°ssicas s√£o em grande parte um mecanismo t√©cnico de aproxima√ß√µes num√©ricas baseadas na distribui√ß√£o Normal e suas muitas engrenagens subsidi√°rias. Essa m√°quina j√° foi necess√°ria, porque a alternativa conceitualmente mais simples baseada em permuta√ß√µes estava computacionalmente al√©m de nosso alcance3. Antes dos computadores, os estat√≠sticos n√£o tinham escolha (Cobb, 2007).\n\nQuem ficou curioso com a hist√≥ria da Estat√≠stica. Recomendo um livro de Stephen Stigler intitulado Statistics on the Table: The History of Statistical Concepts and Methods. O primeiro autor comprou uma c√≥pia em um sebo online.\n\\(p\\)-valor e Hip√≥tese Nula \\(H_0\\)\n\n\\(p\\)-valores s√£o de dif√≠cil entendimento, \\(p < 0.05\\).\n\n\n\n\nSem d√∫vida, esta parte da Estat√≠stica inferencial √© a mais complicada e menos intuitiva. Parafraseando Andrew Gelman, estat√≠stico da Columbia University, ‚ÄúPara definir \\(p\\)-valores, escolha uma das duas caracter√≠sticas: intuitiva ou precisa. Ou sua defini√ß√£o √© intuitiva mas imprecisa, ou sua defini√ß√£o √© precisa mas n√£o intuitiva.‚Äù A grande maioria dos pesquisadores4 possui uma defini√ß√£o incorreta do que √© um \\(p\\)-valor (Cumming, 2009). E quando vemos evid√™ncias do campo da medicina, que talvez seja o campo com maior quantidade de recursos dispon√≠veis para pesquisa e avan√ßo do conhecimento, tamb√©m encontramos muitos problemas no uso dos \\(p\\)-valores (Ioannidis, 2019). Antes de entrarmos nas defini√ß√µes de \\(p\\)-valores, vale a pena tranquiliz√°-los: \\(p\\)-valores s√£o uma coisa complicada e se voc√™ n√£o entender na primeira vez que ler as defini√ß√µes abaixo, n√£o se preocupe, voc√™ n√£o estar√° em m√° companhia; respire fundo e tente ler mais uma vez.\nPrimeiramente a defini√ß√£o estat√≠stica:\n\n\\(p\\)-valor √© a probabilidade de obter resultados no m√≠nimo t√£o extremos quanto os que foram observados, dado que a hip√≥tese nula \\(H_0\\) √© verdadeira.\n\nSe voc√™ escrever essa defini√ß√£o em qualquer prova, livro ou artigo cient√≠fico, voc√™ estar√° 100% preciso e correto na defini√ß√£o do que √© um \\(p\\)-valor. Agora, a compreens√£o dessa defini√ß√£o √© algo complicado. Para isso, vamos quebrar essa defini√ß√£o em algumas partes para melhor compreens√£o:\n‚Äúprobabilidade de obter resultados‚Ä¶‚Äù: vejam que \\(p\\)-valores s√£o uma caracter√≠stica dos seus dados e n√£o da sua teoria ou hip√≥tese.\n‚Äú‚Ä¶no m√≠nimo t√£o extremos quanto os que foram observados‚Ä¶‚Äù: ‚Äúno minimo t√£o‚Äù implica em definir um limiar para a caracteriza√ß√£o de algum achado relevante, que √© comumente chamado de \\(\\alpha\\). Geralmente estipulamos alpha em 5% (\\(\\alpha = 0.05\\)) e qualquer coisa mais extrema que alpha (ou seja menor que 5%) caracterizamos como significante5.\n‚Äú..dado que a hip√≥tese nula √© verdadeira‚Ä¶‚Äù: Todo teste estat√≠stico que possui um \\(p\\)-valor possui uma Hip√≥tese Nula (geralmente escrita como \\(H_0\\)). Hip√≥tese nula, sempre tem a ver com algum efeito nulo. Por exemplo, a hip√≥tese nula do teste Shapiro-Wilk e Komolgorov-Smirnov √© ‚Äúos dados s√£o distribu√≠dos conforme uma distribui√ß√£o Normal‚Äù e a do teste de Levene √© ‚Äúas vari√¢ncias dos dados s√£o iguais.‚Äù Sempre que ver um \\(p\\)-valor, se pergunte: ‚ÄúQual a hip√≥tese nula que este teste presup√µe correta?6‚Äù\nPara entender o \\(p\\)-valor qualquer teste estat√≠stico primeiro descubra qual √© a hip√≥tese nula por tr√°s daquele teste. A defini√ß√£o do \\(p\\)-valor n√£o mudar√°. Em todo teste ela √© sempre a mesma. O que muda com o teste √© a hip√≥tese nula. Cada teste possui sua \\(H_0\\).\n\n\n\n\\(p\\)-valor √© a probabilidade dos dados que voc√™ obteve dado que a hip√≥tese nula √© verdadeira. Para os que gostam do formalismo matem√°tico: \\(p = P(D|H_0)\\). Em portugu√™s, essa express√£o significa ‚Äúa probabilidade de \\(D\\) condicionado √† \\(H_0\\).‚Äù Antes de avan√ßarmos para alguns exemplos e tentativas de formalizar uma intui√ß√£o sobre os \\(p\\)-valores, √© importante ressaltar que \\(p\\)-valores dizem algo √† respeito dos dados e n√£o de hip√≥teses. Para o \\(p\\)-valor, a hip√≥tese nula √© verdadeira, e estamos apenas avaliando se os dados se conformam √† essa hip√≥tese nula ou n√£o. Se voc√™s sa√≠rem desse tutorial munidos com essa intui√ß√£o, o mundo ser√° agraciado com pesquisadores mais preparados para qualificar e interpretar evid√™ncias (\\(p < 0.05\\)).\nExemplo intuitivo:\n\nImagine que voc√™ tem uma moeda que suspeita ser enviesada para uma probabilidade maior de dar cara. (Sua hip√≥tese nula √© ent√£o que a moeda √© justa.) Voc√™ joga a moeda 100 vezes e obt√©m mais cara do que coroa. O \\(p\\)-valor n√£o dir√° se a moeda √© justa, mas dir√° a probabilidade de voc√™ obter pelo menos tantas caras quanto se a moeda fosse justa. √â isso - nada mais.\n\n\nApesar de termos falado anterior que defini√ß√µes intuitivas n√£o s√£o precisas, elas sem d√∫vida facilitam o entendimento do \\(p\\)-valor.\nAlgumas quest√µes hist√≥ricas\nN√£o tem como entendermos \\(p\\)-valores se n√£o compreendermos as suas origens e trajet√≥ria hist√≥rica. A primeira men√ß√£o do termo foi feita pelo estat√≠stico Ronald Fisher7 em 1925 (Fisher, 1925) que define o \\(p\\)-valor como um ‚Äú√≠ndice que mede a for√ßa da evid√™ncia contra a hip√≥tese nula.‚Äù Para quantificar a for√ßa da evid√™ncia contra a hip√≥tese nula, Fisher defendeu ‚Äú\\(p<0.05\\) (5% de signific√¢ncia) como um n√≠vel padr√£o para concluir que h√° evid√™ncia contra a hip√≥tese testada, embora n√£o como uma regra absoluta.‚Äù Fisher n√£o parou por a√≠ mas classificou a for√ßa da evid√™ncia contra a hip√≥tese nula. Ele prop√¥s ‚Äúse \\(p\\) est√° entre 0.1 e 0.9, certamente n√£o h√° raz√£o para suspeitar da hip√≥tese testada. Se estiver abaixo de 0.02, √© fortemente indicado que a hip√≥tese falha em explicar o conjunto dos fatos. N√£o seremos frequentemente perdidos se tra√ßarmos uma linha convencional de 0.05‚Äù Desde que Fisher fez esta declara√ß√£o h√° quase 100 anos, o limiar de 0.05 foi usado por pesquisadores e cientistas em todo o mundo e tornou-se ritual√≠stico usar 0.05 como limiar como se outros limiares n√£o pudessem ser usados.\n\n\n\nFigure 3: Ronald Fisher. Figura de https://www.wikipedia.org\n\n\n\nAp√≥s isso, o limiar de 0.05 agora instaurado como inquestion√°vel influenciou fortemente a estat√≠stica e a ci√™ncia. Mas n√£o h√° nenhuma raz√£o contra a ado√ß√£o de outros limiares (\\(\\alpha\\)) como 0.1 ou 0.01. Se bem argumentados, a escolha de limiares diferentes de 0.05 pode ser bem-vista por editores, revisores e orientadores. Como o \\(p\\)-valor √© uma probabilidade, ele n√£o √© um quantidade cont√≠nua. N√£o h√° raz√£o para diferenciarmos um \\(p\\) de 0.049 contra um \\(p\\) de 0.051. Robert Rosenthal, um psic√≥logo j√° dizia ‚ÄúDeus ama \\(p\\) de 0.06 tanto quanto um \\(p\\) de 0.05‚Äù (Rosnow & Rosenthal, 1989).\n\n\n\nO que o \\(p\\)-valor n√£o √©\nCom a defini√ß√£o e intui√ß√£o do que √© um \\(p\\)-valor bem ancoradas, podemos avan√ßar para o que o \\(p\\)-valor n√£o √©!\n\n\n\n\\(p\\)-valor n√£o √© a probabilidade da Hip√≥tese nula - Famosa confus√£o entre \\(P(D|H_0)\\) e \\(P(H_0|D)\\). \\(p\\)-valor n√£o √© a probabilidade da hip√≥tese nula, mas sim a probabilidade dos dados que voc√™ obteve. Por exemplo: a probabilidade de voc√™ tossir dado que voc√™ est√° com COVID √© diferente da probabilidade de voc√™ estar com COVID dado que voc√™ tossiu: \\(P(\\text{tosse} | \\text{COVID}) \\neq P(\\text{COVID} | \\text{tosse})\\). Acredito que a primeira, \\(P(\\text{tosse} | \\text{COVID})\\) √© bem alta, enquanto a segunda, \\(P(\\text{COVID} | \\text{tosse})\\) deve ser bem baixa (afinal tossimos a todo momento).\n\nO primeiro autor tentou explicar essa diferen√ßa para uma senhora que o viu tossir na fila do mercado, mas os seus esfor√ßos foram em v√£o‚Ä¶\n\\(p\\)-valor n√£o √© a probabilidade dos dados serem produzidos pelo acaso - N√£o! Ningu√©m falou nada de acaso. Mais uma vez: \\(p\\)-valor √© probabilidade de obter resultados no m√≠nimo t√£o extremos quanto os que foram observados, dado que a hip√≥tese nula √© verdadeira.\n\\(p\\)-valor mensura o tamanho do efeito de um teste estat√≠stico - Tamb√©m n√£o‚Ä¶ \\(p\\)-valor n√£o diz nada sobre o tamanho do efeito. Apenas sobre se o quanto os dados observados divergem do esperado sob a hip√≥tese nula. √â claro que efeitos grandes s√£o mais prov√°veis de serem estatisticamente significantes que efeitos pequenos. Mas isto n√£o √© via de regra e nunca julguem um achado pelo seu \\(p\\)-valor, mas sim pelo seu tamanho de efeito. Al√©m disso, \\(p\\)-valores podem ser ‚Äúhackeados‚Äù de diversas maneiras (Head, Holman, Lanfear, Kahn, & Jennions, 2015) e muitas vezes seu valor √© uma consequ√™ncia direta do tamanho da amostra. Mais sobre isso no conte√∫do auxiliar sobre tamanho de amostra.\nIntervalos de Confian√ßa\nIntervalos de confian√ßa foram criados como uma solu√ß√£o para os problemas de m√°-interpreta√ß√£o dos \\(p\\)-valores e sua aplica√ß√£o se destina ao tamanho do efeito. Se voc√™ achou \\(p\\)-valor confuso, se prepare! Intervalos de confian√ßa s√£o ainda mais confusos e muitos pesquisadores e cientistas tamb√©m n√£o possuem a compreens√£o correta (Hoekstra, Morey, Rouder, & Wagenmakers, 2014)8‚Ä¶Vamos para a defini√ß√£o estat√≠stica do idealizador dos intervalos de confian√ßa, Jerzy Neyman, em 1937 (Neyman, 1937):\n\n‚ÄúUm intervalo de confian√ßa de X% para um par√¢metro √© um intervalo (a, b) gerado por um procedimento que em amostragem repetida tem uma probabilidade de X% de conter o valor verdadeiro do par√¢metro, para todos os valores poss√≠veis do par√¢metro.‚Äù9 (Neyman, 1937)\n\nMais uma vez vamos quebrar essa defini√ß√£o em em algumas partes para melhor compreens√£o:\n‚Äú‚Ä¶ intervalo (a,b) ‚Ä¶‚Äù: intervalo de confian√ßa sempre ser√£o expressados como um intervalo \\(a\\) - \\(b\\), onde \\(a\\) √© menor que \\(b\\) (\\(a < b\\)).\n‚Äú‚Ä¶ gerado por um procedimento que em amostragem repetida‚Ä¶‚Äù: aqui estamos falando de popula√ß√£o. E o que voc√™ geralmente tem nas suas m√£os quando est√° fazendo uma an√°lise estat√≠stica √© uma amostra. Uma popula√ß√£o √© um conjunto de pessoas, itens ou eventos sobre os quais voc√™ quer fazer infer√™ncias. Uma amostra √© um √© um subconjunto de pessoas, itens ou eventos de uma popula√ß√£o maior que voc√™ coleta e analisa para fazer infer√™ncias. Geralmente o tamanho da amostra √© bem menor que o tamanho da popula√ß√£o. Ent√£o, intervalos de confian√ßa expressam a frequ√™ncia de longo-prazo que voc√™s esperaria obter de um tamanho de efeito caso replicasse o teste estat√≠stico para diversas amostras da MESMA popula√ß√£o.\n‚Äú‚Ä¶ tem uma probabilidade de X% de conter o valor verdadeiro do par√¢metro, para todos os valores poss√≠veis do par√¢metro.‚Äù: os intervalos de confian√ßa sempre ser√£o expressados acompanhados de uma probabilidade (algo entre 0.001% e 99.999%) que quantifica a certeza de encontrar o intervalo em uma replica√ß√µes do teste estat√≠stico para diversas amostras da MESMA popula√ß√£o.\nPor exemplo: digamos que voc√™ executou uma an√°lise estat√≠stica para comparar efic√°cia de uma pol√≠tica p√∫blica em dois grupos e voc√™ obteve a diferen√ßa entre a m√©dia desses grupos. Voc√™ pode expressar essa diferen√ßa como um intervalo de confian√ßa. Geralmente escolhemos a confian√ßa de 95% (sim, est√° relacionado com o 0.05 do \\(p\\)-valor). Voc√™ ent√£o escreve no seu artigo que a ‚Äúdiferen√ßa entre grupos observada √© de 10.5 - 23.5 (95% IC).‚Äù Isso quer dizer que 95 estudos de 100, que usem o mesmo tamanho de amostra e popula√ß√£o-alvo, aplicando o mesmo teste estat√≠stico, esperar√£o encontrar um resultado de diferen√ßas de m√©dia entre grupos entre 10.5 e 23.5. Aqui as unidades s√£o arbitr√°rias, mas para continuar o exemplo vamos supor que sejam espectativa de vida.\nFal√°cias\nEm um artigo bem controverso, Morey, Hoekstra, Rouder, Lee, & Wagenmakers (2016) mostram as tr√™s grandes fal√°cias (qualquer enunciado ou racioc√≠nio falso que entretanto simula a veracidade) dos intervalos de confian√ßa (a tradu√ß√£o √© livre e feita por n√≥s):\nA fal√°cia fundamental dos intervalos de confian√ßa: Um intervalo de confian√ßa de X% para um par√¢metro √© um intervalo (a, b) gerado por um procedimento que na amostragem repetida tem uma probabilidade de X% de conter o valor verdadeiro do par√¢metro, para todos os valores poss√≠veis do par√¢metro. probabilidade de que um intervalo aleat√≥rio cont√©m o valor verdadeiro √© X%, ent√£o a plausibilidade ou probabilidade de que um determinado intervalo observado cont√©m o valor verdadeiro tamb√©m √© X%; ou, alternativamente, podemos ter X% de confian√ßa de que o intervalo observado cont√©m o valor real10.\nA fal√°cia da precis√£o: A largura de um intervalo de confian√ßa indica a precis√£o de nosso conhecimento sobre o par√¢metro. Intervalos de confian√ßa estreitos correspondem a conhecimentos precisos, enquanto erros de confian√ßa amplos correspondem a conhecimentos imprecisos11.\nA fal√°cia da probabilidade: Um intervalo de confian√ßa cont√©m os valores prov√°veis para o par√¢metro. Os valores dentro do intervalo de confian√ßa s√£o mais prov√°veis do que os externos. Essa fal√°cia existe em v√°rias variedades, √†s vezes envolvendo plausibilidade, credibilidade ou razoabilidade de cren√ßas sobre o par√¢metro12.\nNote que todas essas tr√™s fal√°cias est√£o erradas e s√£o uma compreens√£o err√¥nea ou incompleta de intervalos de confian√ßa.\nRela√ß√£o entre intervalos de confian√ßa e \\(p\\)-valores\nIntervalos de confian√ßa est√£o profundamente relacionados com \\(p\\)-valores. Primeiro, para que uma estimativa tenha um \\(p\\)-valor menor que 0.05, seu intervalo de confian√ßa 95% n√£o pode capturar o zero. Ou seja, o intervalo n√£o pode compreender o efeito nulo (Hip√≥tese Nula - \\(H_0\\)). Isso segue para outros valores de \\(p\\) correspondentes com outros n√≠veis de confian√ßa dos intervalos. Por exemplo, para uma estimativa com \\(p\\)-valor menor que 0.01, seu intervalo de confian√ßa 99% n√£o pode capturar o 0. Al√©m disso, intervalos de confian√ßa (assim como \\(p\\)-valores) est√£o intrinsicamente conectados com o tamanho da amostra. Quanto maior o tamanho de amostra, mais estreito ser√° o intervalo de confian√ßa. A intui√ß√£o por tr√°s disso √© que conforme a sua amostra aumenta, tamb√©m aumentar√£o a sua confian√ßa e precis√£o em infer√™ncias sobre a popula√ß√£o-alvo. Por fim, intervalos de confian√ßa (assim como \\(p\\)-valores) n√£o falam nada sobre a sua teoria ou hip√≥tese, mas sobre a rela√ß√£o dos seus dados (amostra) com a popula√ß√£o-alvo. Eles n√£o s√£o a probabilidade do par√¢metro estimado (\\(P(\\text{par√¢metro} | D)\\), no nosso exemplo diferen√ßa entre m√©dias de grupos), mas sim a probabilidade de amostras com o mesmo par√¢metro estimado (\\(P(D | \\text{par√¢metro})\\)).\nUma boa maneira de resumir \\(p\\)-valores e intervalos de confian√ßa √© a seguinte:\n\nConsidere \\(p\\)-valores algo que mensura a possibilidade de existir um efeito ou n√£o e intervalos de confian√ßa quantificam o tamanho desse efeito.\n\n\nMas sempre se atente nas defini√ß√µes. Lembre-se que se tentarmos ser intuitivos com \\(p\\)-valores e intervalos de confian√ßa n√£o seremos precisos nas defini√ß√µes.\nSignific√¢ncia Estat√≠stica vs Signific√¢ncia Pr√°tica\n\nConsidere isso uma introdu√ß√£o r√°pida √† \\(p\\)-hacking.\nPara encerrar esse tour de \\(p\\)-valores e intervalos de confian√ßa, temos que nos atentar que signific√¢ncia estat√≠stica n√£o √© a mesma coisa que signific√¢ncia pr√°tica. Signific√¢ncia estat√≠stica √© se algum achado de um teste/modelo estat√≠stico diverge o suficiente da hip√≥tese nula e, sendo que hip√≥tese nula sempre s√£o sobre efeitos ou diferen√ßas nulas, podemos afirmar que signific√¢ncia estat√≠stica quer dizer um achado √© diferente de um efeito nulo. Diversos testes da Estat√≠stica inferencial cl√°ssica quando submetidos √† amostras grandes13 v√£o detectar uma diferen√ßa significante, mesmo que praticamente insignificante. Com uma amostra suficientemente grande n√≥s conseguimos gerar \\(p\\)-valores significantes para diferen√ßas min√∫sculas, como por exemplo uma diferen√ßa de 0.01cm altura entre dois grupos de uma amostra.\nPor isso que defendemos que nunca se interprete an√°lises estat√≠sticas somente com \\(p\\)-valores, mas sempre em conjunto com os intervalos de confian√ßa que quantificam o tamanho do efeito. Nunca gere argumentos sobre evid√™ncias somente a partir de signific√¢ncia estat√≠stica, sempre inclua tamanho do efeito.\nErro Tipo I e Erro Tipo II\nNa Estat√≠stica inferencial temos dois erros poss√≠veis quando estamos realizando um teste estat√≠stico contra uma hip√≥tese nula.\nErro tipo I, tamb√©m chamado de ‚Äúfalso positivo‚Äù, √© a chance de rejeitarmos a hip√≥tese nula quando ela √© verdadeira. Esse erro √© o alpha \\(\\alpha\\) que √© usado como limiar de signific√¢ncia do \\(p\\)-valor.\nErro tipo II, tamb√©m chamado de ‚Äúfalso negativo‚Äù, √© a chance de n√£o rejeitarmos a hip√≥tese nula quando ela √© falsa. Esse erro √© identificado como a letra grega beta \\(\\beta\\). Al√©m disso, o poder de um teste estat√≠stico √© mensurado como \\(1 - \\beta\\). O poder de um teste estat√≠stico aumenta proporcionalmente ao tamanho amostral. Quanto maior a amostra, maior o poder do teste.\n\nEsses conceitos foram criados por matem√°ticos, ent√£o a nomenclatura erro tipo I e erro tipo II √© perfeita matematicamente, pois no contexto de testes estat√≠sticos contra uma hip√≥tese nula s√≥ existem dois tipos de erros. Mas para o ensino da Estat√≠stica e comunica√ß√£o de incertezas √© p√©ssima. Sempre que poss√≠vel optamos por usar termos como ‚Äúfalso positivo‚Äù e ‚Äúfalso negativo‚Äù ao inv√©s de erro tipo I e erro tipo II.\n\n\n\nPor quest√µes hist√≥ricas, o erro tipo I14 foi considerado mais importante de ser controlado do que o erro tipo II. Portanto, quase todos os testes de hip√≥tese nula focam no controle dos ‚Äúfalsos positivos‚Äù enquanto o controle dos ‚Äúfalsos negativos‚Äù s√£o colocados em segundo plano. No mundo ideal, tanto \\(\\alpha\\) quando \\(\\beta\\) devem ser reduzidos o m√°ximo poss√≠vel. Isto requer um tamanho amostral frequentemente maior do que os recursos dispon√≠veis para o pesquisador, portanto √© comum pesquisadores usarem um \\(\\alpha\\) de 5% e um \\(\\beta\\) de 20% (poder de 80%).\nTamanho da Amostra\nA maioria dos testes estat√≠sticos que computam um \\(p\\)-valor s√£o extremamente sens√≠veis a tamanho da amostra. A hip√≥tese nula sempre representa a aus√™ncia de qualquer efeito e nunca a diferen√ßa observada na amostra √© igual a zero. Sempre h√° algum digito, menor que seja, que faz com que a diferen√ßa seja diferente de zero, ex: 0.00001. Quanto maior o tamanho da amostra maior a probabilidade de obtermos um \\(p\\)-valor significante, pois ele indica que o efeito √© diferente de zero, mesmo que essa diferen√ßa seja insignificante do ponto de vista pr√°tico. Em certos contextos, defendemos que o \\(p\\)-valor √© uma aproxima√ß√£o (proxy) de tamanho da amostra.\n\n\n\nE aonde entra a Estat√≠stica Bayesiana?\nA Estat√≠stica Frequentista15 se baseia em realizarmos testes de hip√≥teses sempre comparando os dados dispon√≠veis com um cen√°rio hip√≥tetico de efeito nulo ‚Äì \\(H_0\\):\n\\[P(D | H_0)\\]\nO resultado dessa probabilidade √© o \\(p\\)-valor: a probabilidade dos dados obtidos condicionado √† hip√≥tese nula ser verdadeira. E se quisermos a probabilidade da hip√≥tese nula16 e n√£o dos dados obtidos?\nPara isso temos que ‚Äúinverter‚Äù a probabilidade. Estamos interessados em:\n\\[P(H_0 | D)\\] Isso somente pode ser feito com o teorema de Bayes. Generalizando de \\(H_0\\) para qualquer \\(H\\), o teorema de Bayes nos permite ‚Äúinverter‚Äù a probabilidade condicional:\n\\[P(H | D)=\\frac{P(H) \\cdot P(D | H)}{P(D)}\\] Aqui temos as seguintes probabilidades (note que podemos trocar \\(H\\) aqui para qualquer par√¢metro \\(\\theta\\)):\n\\(P(H|D)\\) ‚Äì probabilidade posterior de \\(H\\) depois de observamos os dados \\(D\\).\n\\(P(H)\\) ‚Äì probabilidade pr√©via de \\(H\\) antes de observarmos os dados \\(D\\).\n\\(P(D|H)\\) ‚Äì probabilidade dos dados obtidos sob a hip√≥tese \\(H\\), tamb√©m chamada de verossimilhan√ßa (do ingl√™s likelihood).\n\\(P(D)\\) ‚Äì chamada de evid√™ncia ou verossimilhan√ßa marginal (do ingl√™s marginal likelihood), √© a probabilidade geral dos dados de acordo com o modelo, determinada pela m√©dia de todos os valores de hip√≥teses ou param√™tros poss√≠veis ponderados pela for√ßa da cren√ßa nesses valores de hip√≥teses ou par√¢metros. Para hip√≥teses valores discretos de par√¢metros: \\(P(D) = \\sum_\\theta P(D|H_0) P(H_0)\\). J√° para valores cont√≠nuos de par√¢metros: \\(P(D) = \\int_\\theta P(D|\\theta) P(\\theta) d \\theta\\). Em outras palavras, tome a probabilidade m√©dia \\(P(D|\\theta)\\) em todos os valores de \\(\\theta\\), ponderada pela probabilidade anterior de \\(\\theta\\) - \\(P(\\theta)\\). A √∫nica fun√ß√£o de \\(P(D)\\) √© garantir que a probabilidade posterior \\(P(H|D)\\) seja v√°lida (algo entre 0 e 1).\nPortanto, a Estat√≠stica Bayesiana √© qualquer t√©cnica inferencial caracterizada pelo uso de informa√ß√£o pr√©via embutida como probabilidade pr√©via \\(P(H)\\). N√≥s n√£o usamos \\(p\\)-valores nem intervalo de confian√ßa, pois o conceito de hip√≥tese nula √© inexistente. Voc√™ pode especificar qualquer hip√≥tese que queria, n√£o necessariamente uma hip√≥tese nula. Aqui temos o conceitos de probabilidade posterior de uma hip√≥tese ou par√¢metro ao inv√©s de \\(p\\)-valores e tamb√©m o conceito de intervalos de credibilidade que, ao inv√©s de intervalos de confian√ßa, nos d√£o a probabilidade de um par√¢metro estar entre um intervalo de valores (muito mais intuitivo e simples de usar que um intervalo de confian√ßa).\nComent√°rios Finais\nSim, \\(p\\)-valores, intervalos de confian√ßa, hip√≥teses nulas s√£o conceitos complexos e muitos pesquisadores e cientistas n√£o possuem a compreens√£o m√≠nima necess√°ria para a pr√°tica de Estat√≠stica inferencial. Acreditamos que a ci√™ncia (e a sociedade como um todo) se beneficiar√° de um maior n√∫mero de cidad√£os e pesquisadores que consigam avaliar, quantificar e qualificar evid√™ncias cient√≠ficas. O paradigma da evid√™ncia cient√≠fica atual (e, acreditamos que perdurar√° assim por bastante tempo) √© o NHST e, apesar de termos algumas alternativas ‚Äì como a Estat√≠stica Bayesiana ‚Äì NHST ir√° predominar em boa parte da ci√™ncia pelas pr√≥ximas d√©cadas. Por isso, caro leitor, saiba que com ‚Äúgrandes poderes, v√™m grandes responsabilidades.‚Äù N√£o deixe algu√©m torturar dados em pr√°ticas anti-√©ticas de \\(p\\)-hacking ou fundamentarem seus argumentos em compreens√µes incorretas de \\(p\\)-valor e \\(H_0\\).\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] DiagrammeR_1.0.6.1\n\nloaded via a namespace (and not attached):\n [1] visNetwork_2.0.9   fansi_0.4.2        digest_0.6.27     \n [4] jsonlite_1.7.2     magrittr_2.0.1     evaluate_0.14     \n [7] highr_0.8          rlang_0.4.10       stringi_1.5.3     \n[10] rstudioapi_0.13    vctrs_0.3.6        rmarkdown_2.6     \n[13] distill_1.2        RColorBrewer_1.1-2 tools_4.0.3       \n[16] stringr_1.4.0      htmlwidgets_1.5.3  glue_1.4.2        \n[19] xfun_0.21          yaml_2.2.1         parallel_4.0.3    \n[22] compiler_4.0.3     htmltools_0.5.1.1  knitr_1.31        \n[25] downlit_0.2.1     \n\n\n\n\nBaird, D. (1983). The fisher/pearson chi-squared controversy: A turning point for inductive inference. The British Journal for the Philosophy of Science, 34(2), 105‚Äì118. Retrieved from http://www.jstor.org/stable/687444\n\n\nCobb, G. W. (2007). The introductory statistics course: A ptolemaic curriculum? Technology Innovations in Statistics Education, 1(1).\n\n\nCumming, G. (2009). Inference by eye: Reading the overlap of independent confidence intervals. Statistics in Medicine, 28(2), 205‚Äì220.\n\n\nDowney, A. (2016). Probably overthinking it: There is still only one test. Retrieved from http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html\n\n\nFisher, R. A. (1925). Statistical methods for research workers. Oliver; Boyd.\n\n\nHead, M. L., Holman, L., Lanfear, R., Kahn, A. T., & Jennions, M. D. (2015). The extent and consequences of p-hacking in science. PLoS Biol, 13(3), e1002106.\n\n\nHoekstra, R., Morey, R. D., Rouder, J. N., & Wagenmakers, E.-J. (2014). Robust misinterpretation of confidence intervals. Psychonomic Bulletin & Review, 21(5), 1157‚Äì1164. https://doi.org/10.3758/s13423-013-0572-3\n\n\nIoannidis, J. P. A. (2019). What Have We (Not) Learnt from Millions of Scientific Papers with <i>P<\/i> Values? The American Statistician, 73(sup1), 20‚Äì25. https://doi.org/10.1080/00031305.2018.1447512\n\n\nMorey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The fallacy of placing confidence in confidence intervals. Psychonomic Bulletin & Review, 23(1), 103‚Äì123. https://doi.org/10.3758/s13423-015-0947-8\n\n\nNeyman, J. (1937). Outline of a theory of statistical estimation based on the classical theory of probability. Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences, 236(767), 333‚Äì380.\n\n\nNeyman, J., & Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706), 289‚Äì337.\n\n\nRosnow, R. L., & Rosenthal, R. (1989). Statistical procedures and the justification of knowledge in psychological science. American Psychologist, 44, 1276‚Äì1284.\n\n\nStigler, S. M., & others. (2007). The epic story of maximum likelihood. Statistical Science, 22(4), 598‚Äì620.\n\n\nMatem√°tico ingl√™s que viveu entre 1857-1936. Considerado o fundador do campo da Estat√≠stica.‚Ü©Ô∏é\nem especial as t√©cnicas cl√°ssicas/frequentistas de Estat√≠stica inferencial.‚Ü©Ô∏é\nTeoricamente n√£o precisamos da hip√≥tese nula se, no passo 2, simul√°ssemos e permut√°ssemos valores da amostra para calcular um \\(\\delta_0\\) (√© provado matematicamente que se gerarmos amostras e permuta√ß√µes simuladas o suficiente, conseguiremos ter um \\(\\delta_0\\) no m√≠nimo t√£o ver√≠dico que a abordagem cl√°ssica) ao inv√©s de nos embasarmos em uma aproxima√ß√£o num√©rica pr√©-estabelecida de \\(\\delta_0\\). √â claro que todas essas permuta√ß√µes e simula√ß√µes s√£o computacionalmente intensas.‚Ü©Ô∏é\nInclusive muitos renomados e citados em abund√¢ncia em suas √°reas.‚Ü©Ô∏é\nCuidado com essa palavra. Ela √© precisa e somente deve ser usada em contextos estat√≠sticos. Signific√¢ncia estat√≠stica quer dizer que os dados observados s√£o mais extremos que um alpha pr√©definido de que a hip√≥tese nula √© verdadeira.‚Ü©Ô∏é\nEsse conselho √© extremamente √∫til. Por diversas vezes temos alunos que nos procuram com uma pergunta mais ou menos assim: ‚ÄúProfessor, o que √© o teste de Sobrenome que nunca ouvi falar na minha vida h√≠fen outro sobrenome ainda mais estranho?‚Äù Gra√ßas a Wikipedia e Google, n√≥s simplesmente vamos atr√°s da \\(H_0\\) desse teste (busca Google: ‚Äúsobrenome1-sobrenome2 null hypothesis‚Äù) e com isso conseguimos responder ao aluno.‚Ü©Ô∏é\nA controv√©rsia da personalidade e vida de Ronald Fisher merece uma nota de rodap√©. Suas contribui√ß√µes, sem d√∫vida, foram cruciais para o avan√ßo da ci√™ncia e da estat√≠stica. Seu intelecto era brilhante e seu talento j√° floresceu jovem: antes de completar 33 anos de idade ele tinha proposto o m√©todo de estima√ß√£o por m√°xima verossimilhan√ßa (maximum likelihood estimation) (Stigler & others, 2007) e tamb√©m criou o conceito de graus de liberdade (degrees of freedom) ao propor uma corre√ß√£o no teste de chi-quadrado de Pearson (Baird, 1983). Tamb√©m inventou a An√°lise de Vari√¢ncia (ANOVA) e foi o primeiro a propor randomiza√ß√£o como uma maneira de realizar experimentos, sendo considerado o ‚Äúpai‚Äù dos ensaios cl√≠nicos randomizados. Nem tudo √© florido na vida de Fisher, ele foi um eugenista e possu√≠a uma vis√£o muito forte sobre etnia e ra√ßa preconizando a superioridade de certas etnias. Al√©m disso, era extremamente invariante, perseguindo, prejudicando e debochando qualquer cr√≠tico √† suas teorias e publica√ß√µes. O que vemos hoje no monop√≥lio do paradigma Neyman-Pearson (Neyman & Pearson, 1933) com \\(p\\)-valores e hip√≥teses nulas √© resultado desse esfor√ßo Fisheriano em calar os cr√≠ticos e deixar apenas sua voz ecoar.‚Ü©Ô∏é\ninclusive muitos professores de estat√≠stica, veja a refer√™ncia‚Ü©Ô∏é\nOriginal em ingles: ‚ÄúAn X% confidence interval for a parameter is an interval (a, b) generated by a procedure that in repeated sampling has an X% probability of containing the true value of the parameter, for all possible values of the parameter.‚Äù‚Ü©Ô∏é\nOriginal em ingl√™s: If the probability that a random interval contains the true value is X%, then the plausibility or probability that a particular observed interval contains the true value is also X%;or, alternatively, we can have X% confidence that the observed interval contains the true value.‚Ü©Ô∏é\nOriginal em ingl√™s: The width of a confidence interval indicates the precision ofour knowledge about the parameter. Narrow confidence intervals correspond to precise knowledge, while wide confidence errors correspond to imprecise knowledge.‚Ü©Ô∏é\nOriginal em ingl√™s: A confidence interval contains the likely values for the parameter. Values inside the confidence interval are more likely than those outside. This fallacy exists in several varieties, sometimes involving plausibility, credibility, or reasonableness of beliefs about the parameter.‚Ü©Ô∏é\nO que √© muito comum em 2020s com o advento de Big Data e facilidade de obten√ß√£o de dados.‚Ü©Ô∏é\nJerzy Newman, fundador do paradigma NHST, e criador dos erros tipo I e tipo II defendia a ideia de que √© melhor absolver um culpado (erro tipo II) do que culpar um inocente (erro tipo I).‚Ü©Ô∏é\ntamb√©m chamada de Estat√≠stica Cl√°ssica‚Ü©Ô∏é\nou de maneira geral qualquer hip√≥tese ou param√™tro estimado‚Ü©Ô∏é\n",
      "last_modified": "2021-02-12T06:47:43-03:00"
    }
  ],
  "collections": []
}

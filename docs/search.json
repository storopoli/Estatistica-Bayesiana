{
  "articles": [
    {
      "path": "0-Estatistica-Bayesiana.html",
      "title": "O que é Estatística Bayesiana?",
      "description": "Noções de Probabilidade, Estatística Frequentista versus Estatística Bayesiana",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2020",
      "contents": "\n\nContents\nO que é probabilidade?\nProbabilidade Condicional\nAxiomas de Komolgorov\nAbordagem Frequentista\nAbordagem Bayesiana\nExemplo Prático\n\nO que é o maldito \\(p\\)-valor?\nTeorema de Bayes\nVantagens da Estatística Bayesiana\nDesvantagens\nStan\nAmbiente\n\n\nComentário Introdutório\nO que é probabilidade?\nDe Finetti - Probabilidade não existe. That probability as a physical quantity—physical propensity, objective chance does NOT exist. De Finetti showed that, in a certain precise sense, that if we dispense with objective chance nothing is lost. The mathematics of inductive reasoning remains exactly the same. Consider flipping a coin of fixed bias. The trials are assumed to be independent, and as a result, they exhibit another important property. Order doesn’t matter. To say that order doesn’t matter is to say that if you take any finite sequence of heads and tails and permute the outcomes any way you please, and the resulting sequence has the same probability. We say that this probability is invariant under permutations. Or, to put it another way, the only thing that does matter is relative frequency. Outcome sequences that have the same frequencies of heads and tails have the same probability. Frequency is said to be a sufficient statistic. To say that order doesn’t matter or to say that the only thing that does matter is frequency are two ways of saying exactly the same thing. This property is called exchangeability by de Finetti.\nProbabilidade Condicional\nProbabilidade de um evento ocorrer caso outro tenha ocorrido ou não.\n\\(P(\\text{covid}) = 0.07\\) \\(P(\\text{covid} | \\text{febre}) > 0.07\\) \\(P(\\text{covid} | \\text{febre} + \\text{tosse}) >> 0.07\\) \\(P(\\text{covid} | \\text{febre} + \\text{tosse} + \\text{perda de olfato}) >>> 0.07\\)\nProbabilidade Condicional não é “comutativa”\n\\[P(A|B) \\neq P(B|A)\\]\n\\[P(\\text{covid} | \\text{febre}) \\neq P(\\text{febre} | \\text{covid})\\]\nAxiomas de Komolgorov\nAbordagem Frequentista\nFrequência\nAbordagem Bayesiana\nDegree of Belief\nExemplo Prático\nImagine que você está avaliando um jogador de basquete. Você precisa decidir se irá contratá-lo para o seu time. A principal característica que você examinará será a taxa de acerto de cestas de 3 pontos. Esse é o nosso parâmetro de interesse e a partir de agora vamos chamá-lo de \\(\\theta\\) (letra grega1 theta). \\(\\theta\\) pode assumir qualquer valor entre 0 e 1, sendo 0 representando uma taxa de acerto de 0% do jogador—ele sempre erra as tentativas de cestas de 3 pontos; e 1 representando uma taxa de acerto 100% do jogador— ele sempre acerta as tentativas de cestas de 3 pontos.\nÉ claro que \\(\\theta\\) raramente será 0 ou 1, mas sim um valor entre esses dois extremos. Podemos representar \\(\\theta\\) com uma distribuição beta. A distribuição beta é especificada por dois parâmetros com valores sempre positivos (\\(\\geq 0\\)): \\(\\alpha\\)2 e \\(\\beta\\)3. Você pode pensar em \\(\\alpha - 1\\) como o número de acertos e \\(\\beta - 1\\) como o número de erros. Na figura 1 é possível ver uma distribuição beta para vários valores iguais de parâmetros \\(\\alpha\\) e \\(\\beta\\). Veja que conforme o valor de \\(\\alpha\\) e \\(\\beta\\) aumentam a probabilidade de \\(\\theta\\) tende a convergir para o valor de 0.5 (50%).\n\n\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n\nggplot(data = data.frame(x = c(0, 1))) +\n  labs(\n    title = \"Comparativo de Distribuições Beta\",\n    subtitle = expression(alpha == beta),\n    x = expression(theta),\n    y = \"Densidade\",\n    color = \"Parâmetros\"\n  ) +\n  stat_function(aes(color = \"list(alpha, beta) ==  1\"), fun = dbeta, args = list(\n    shape1 = 1, shape2 = 1), size = 2) +\n  stat_function(aes(color = \"list(alpha, beta) ==  2\"), fun = dbeta, args = list(\n    shape1 = 2, shape2 = 2), size = 2) +\n  stat_function(aes(color = \"list(alpha, beta) ==  3\"), fun = dbeta, args = list(\n    shape1 = 3, shape2 = 3), size = 2) +\n    stat_function(aes(color = \"list(alpha, beta) ==  4\"), fun = dbeta, args = list(\n    shape1 = 4, shape2 = 4), size = 2) +\n  scale_color_brewer(palette = \"Set1\",\n    labels = scales::label_parse())\n\n\n\n\nFigure 1: Comparativo de Distribuições Beta – Valores Iguais de \\(\\alpha\\) e \\(\\beta\\)\n\n\n\nConforme os valores de \\(\\alpha\\) e \\(\\beta\\) diferem um do outro começamos a ver a probabilidade de \\(\\theta\\) se distanciar de 0.5 e a distribuição começa a ser assimétrica, tendenciando para algum extremo. Na figura 2 é possível ver uma distribuição beta para vários valores diferentes de parâmetros \\(\\alpha\\) e \\(\\beta\\).\n\n\nlibrary(ggplot2)\n\nggplot(data = data.frame(x = c(0, 1))) +\n  labs(\n    title = \"Comparativo de Distribuições Beta\",\n    subtitle = expression(alpha != beta),\n    x = expression(theta),\n    y = \"Densidade\",\n    color = \"Parâmetros\"\n  ) +\n  stat_function(aes(color = \"list(alpha == 3, beta == 2)\"), fun = dbeta, args = list(\n    shape1 = 3, shape2 = 2), size = 2) +\n  stat_function(aes(color = \"list(alpha == 2, beta ==  3)\"), fun = dbeta, args = list(\n    shape1 = 2, shape2 = 3), size = 2) +\n  stat_function(aes(color = \"list(alpha == 4, beta ==  2)\"), fun = dbeta, args = list(\n    shape1 = 4, shape2 = 2), size = 2) +\n    stat_function(aes(color = \"list(alpha == 2, beta ==  4)\"), fun = dbeta, args = list(\n    shape1 = 2, shape2 = 4), size = 2) +\n  scale_color_brewer(palette = \"Set1\",\n    labels = scales::label_parse())\n\n\n\n\nFigure 2: Comparativo de Distribuições Beta – Valores Diferentes de \\(\\alpha\\) e \\(\\beta\\)\n\n\n\nO que é o maldito \\(p\\)-valor?\nTeste t: ￼\\(P(D | \\text{efeito nulo})\\) ANOVA: ￼\\(P(D|\\text{não há diferença entre os grupos})\\) Regressão: ￼\\(P(D|\\text{coeficiente é nulo})\\) Shapiro-Wilk: \\(P(D|\\text{amostra é normal})\\)\nMas o que estamos realmente interessados é na \\(P(H_0)\\)\nTeorema de Bayes\n\\[\\underbrace{P(\\theta|y)}_{\\text{Posterior}} = \\frac{\\overbrace{P(y | \\theta)}^{\\text{Likelihood}} \\cdot \\overbrace{P(\\theta)}^{\\text{Prior}}}{\\underbrace{P(y)}_{\\text{Constante Normalizadora}}}\\]$\nRanca fora a constante Normalizadora\n\\[\\underbrace{P(\\theta|y)}_{\\text{Posterior}} \\propto \\overbrace{P(y | \\theta)}^{\\text{Likelihood}} \\cdot \\overbrace{P(\\theta)}^{\\text{Prior}} = \\underbrace{P(\\theta, y)}_{\\text{Probabilidade Conjunta}}\\]\n\\(\\propto\\) (comando \\(\\LaTeX\\) \\propto) quer dizer “proporcional à.”\nAnimação com uma distribuição beta de um flip of a coin com updated beliefs by posterior\nVantagens da Estatística Bayesiana\nAbordagem Natural para expressar incerteza\nHabilidade de incorporar informações prévia\nMaior flexibilidade do modelo\nDistribuição posterior completa dos parâmetros\nIntervalos de Confiança vs Intervalos de Credibilidade\nPoint Estimate vs Full Posterior Density\nMesmo com Intervalos de Confiança você está falando ainda de Point Estimate – Optimization of Likelihood when the derivative is at zero\nMLE estimation is the value of the parameters such that the most likely dataset of size N to randomly draw from a population is the dataset that you actually drew. Every other potential dataset that could be drawn from this population is going to fit worse than the dataset that you actually have.\n\nPropagação natural da incerteza\nFisher — “were Fisher alive today, he would be a Bayesian”\nFisher published an article (Fisher, 1962) examining the possibilities of Bayesian methods, but with the prior probabilities to be determined experimentally!\nFisher, R. A. (1962), ‘Some examples of Bayes’ method of the experimental determination of probability a priori’, J. Roy. Stat. Soc. B 24, 118–124.\n\nModelos hierárquicos. lme4 não computa p-valores para os random effects\nExemplo do teste t normal e o bayesiano\nDesvantagens\nVelocidade lenta de estimativa do modelo (30 segundos ao invés de 3 segundos)\nFalar do poder computacional — flops Falar da facilidade de testes ortodoxos de computação\nStan\n\n\nknitr::include_graphics(\"images/stan_billions_subtitled.mp4\")\n\n\n\n\nrstan\nrstanarm\nbrms\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\nna estatística geralmente temos a convenção de usar letras romanas (\\(a, b, c, d, \\dots\\)) para quantidades que sabemos o valor (exemplo: média de uma amostra); e letras gregas (\\(\\alpha, \\beta, \\gamma, \\dots\\)) para quantidades que não sabemos o valor preciso e queremos estimar (exemplo: média de uma população estimada a partir da média de uma amostra).↩︎\nletra grega alpha.↩︎\nletra grega beta.↩︎\n",
      "last_modified": "2021-02-12T06:30:20-03:00"
    },
    {
      "path": "1-Comandos_Basicos.html",
      "title": "Comandos Básicos de R",
      "description": "Introdução ao R e aos comandos básicos do R",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nLendo Arquivos de Dados\nCSV\nExcel\n\nGráficos\nAmbiente\n\n\nEste arquivo é um documento R Markdown. Ele é uma proposta de prosa com código em R, além de ser o formato preferido nosso de comunicar nossas análises. Quando renderizamos o documento no formato desejado. Todo código que é inserido nele é executado e as saídas são incorporadas no documento final. Isto vale para tabelas e gráficos. Por exemplo, podemos pedir para o R imprimir algo com a função print() e o resultado será o código que foi executado e o seu resultado.\n\n\nprint(\"Você executou um código\")\n\n\n[1] \"Você executou um código\"\n\nO formato R Markdown é muito flexível. Podemos fazer relatórios (em PDF, Word e HTML), apresentações (em PDF, PowerPoint e HTML), artigos acadêmicos, livros, websites1, blogs, CVs, etc.\n\nO site do autor foi feito usando a biblioteca {postcards}(Kross, 2021) de R. O CV também foi feito em R usando a biblioteca {vitae} (O’Hara-Wild & Hyndman, 2021).\nLendo Arquivos de Dados\nCom o R conseguimos ler diversos tipo de arquivos de dados: CSV, texto, HTML, Excel, Stata, SPSS, Planilhas Google, Banco de Dados Relacionais, entre outros… Vamos demonstrar como ler arquivos de dados dos dois formatos mais comuns: CSV e Excel.\nCSV\nPara ler um arquivo CSV (.csv) no R execute a função read.csv() para arquivos CSV formato americano (vírgula como separador e decimais como ponto) ou a função read.csv2() para arquivos CSV formato europeu/brasileiro (ponto-e-vírgula como separador e decimais como vírgula). Não esqueça de designar a leitura para uma variável com o designador <-.\n\n\ndf <- read.csv2(\"datasets/mtcars.csv\", row.names = 1)\nhead(df)\n\n\n                  mpg cyl disp  hp drat  wt qsec vs am gear carb\nMazda RX4          21   6  160 110  3.9 2.6   16  0  1    4    4\nMazda RX4 Wag      21   6  160 110  3.9 2.9   17  0  1    4    4\nDatsun 710         23   4  108  93  3.9 2.3   19  1  1    4    1\nHornet 4 Drive     21   6  258 110  3.1 3.2   19  1  0    3    1\nHornet Sportabout  19   8  360 175  3.1 3.4   17  0  0    3    2\nValiant            18   6  225 105  2.8 3.5   20  1  0    3    1\n\nExcel\nPara ler um arquivo Excel (.xls ou .xlsx) no R é necessário importar um pacote chamado readxl que contem a função read_excel. Para importar um pacote no R executamos o comando library() com um argumento único sendo o nome do pacote. Caso não tenha o pacote instalado, deve instalar ele com o comando install.packages(). Não esqueça de colocar o nome do pacote entre aspas \"nome_do_pacote\" dentro do parênteses da função.\n\n\n# install.packages(\"readxl\")\nlibrary(readxl)\ndf <- read_excel(\"datasets/mtcars.xlsx\")\nhead(df)\n\n\n# A tibble: 6 x 12\n  ...1       mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear\n  <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Mazda R…  21       6   160   110  3.9   2.62  16.5     0     1     4\n2 Mazda R…  21       6   160   110  3.9   2.88  17.0     0     1     4\n3 Datsun …  22.8     4   108    93  3.85  2.32  18.6     1     1     4\n4 Hornet …  21.4     6   258   110  3.08  3.22  19.4     1     0     3\n5 Hornet …  18.7     8   360   175  3.15  3.44  17.0     0     0     3\n6 Valiant   18.1     6   225   105  2.76  3.46  20.2     1     0     3\n# … with 1 more variable: carb <dbl>\n\nGráficos\nGeralmente no R você pode plotar mostrar graficamente diversos objetos com o comando plot(). Quando você plota um dataset (conjunto de dados lido de um aquivo), o R retorna um gráfico chamado Pair Plot:\nNa diagonal: nome da variável (coluna do dataset)\nFora da diagonal: um gráfico de dispersão entre a variável no eixo horizontal e a variável no eixo vertical\nExemplo: na figura 1 veja a relação entre disp (cilindrada) e hp (cavalos de potência). Ela é uma relação positiva. Quanto maior disp maior hp.\n\n\nplot(mtcars)\n\n\n\n\nFigure 1: Pair Plot do dataset mtcars\n\n\n\nAmbiente\nEm todos os arquivos dessa disciplina, mostrarei o ambiente computacional usado para replicação.\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\nKross, S. (2021). Postcards: Create beautiful, simple personal websites. Retrieved from https://CRAN.R-project.org/package=postcards\n\n\nO’Hara-Wild, M., & Hyndman, R. (2021). Vitae: Curriculum vitae for r markdown. Retrieved from https://CRAN.R-project.org/package=vitae\n\n\nesse website foi todo feito com R↩︎\n",
      "last_modified": "2021-02-12T06:30:22-03:00"
    },
    {
      "path": "2-Regressao_Linear.html",
      "title": "Regressão Linear",
      "description": "Regressão Linear Bayesiana",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nrstanarm\nRegressão Linear\nExemplo - Score de QI de crianças\nDescritivo das variáveis\nModelo 1 - mom_hs\nModelo 2 - mom_iq\nModelo 3 - mom_hs + mom_iq\nModelo 4 - mom_hs * mom_iq\n\nVariáveis qualitativas\nAtividade Prática\nWHO Life Expectancy\nWine Quality Kaggle Dataset\n\nReferências\nAmbiente\n\n\n\nA principal ferramenta para computação Bayesiana é a linguagem probabilística Stan. O nome homenageia Stanislaw Ulam: um matemático polonês membro do projeto Manhattan (bomba atômica americana) e um dos principais criadores do método de Monte Carlo de simulação. Stan foi lançado em 2012 e é a principal ferramenta utilizada hoje para inferência estatística Bayesiana. O programa roda em linguagem C++, mas possui interfaces para R, Python, MATLAB, Julia, Stata, Mathematica, Scala e Shell.\nO problema do Stan é que ele é uma linguagem de programação e, portanto, possui um acesso dificultado a não-programadores. Abaixo um código que mostra como é um programa escrito em Stan:\n\ndata {\n  int<lower=0> N;\n  vector<lower=0, upper=200>[N] kid_score;\n  vector<lower=0, upper=200>[N] mom_iq;\n}\nparameters {\n  vector[2] beta;\n  real<lower=0> sigma;\n}\nmodel {\n  sigma ~ cauchy(0, 2.5);\n  kid_score ~ normal(beta[1] + beta[2] * mom_iq, sigma);\n}\n\nrstanarm\nPara remediar isso, temos interfaces abstratas que interpretam a intenção do usuário e lidam com a parte mais obral de codificação. A principal delas é o pacote rstanarm, que a etimologia pode ser quebrada em:\nr: pacote para R\nstan: usa a linguagem probabilística Stan\narm: acrônimo para Applied Regression Modeling\nO código anterior de Stan ficaria assim no rstanarm:\n\n\nstan_glm(kid_score ~ mom_iq, data = dataset)\n\n\n\nRegressão Linear\nA ideia aqui é modelar uma variável dependente sendo a combinação linear de variáveis independentes.\n\\[y = \\alpha + \\boldsymbol{\\beta} \\textbf{X} + \\epsilon\\]\nAonde \\(y\\) é a variável dependente, \\(\\alpha\\) um constante, \\(\\boldsymbol{\\beta}\\) um vetor de coeficientes, \\(\\textbf{X}\\) uma matriz de dados e \\(\\epsilon\\) o erro do modelo.\nExemplo - Score de QI de crianças\nVamos aplicar modelagem estatística Bayesiana em um dataset famoso chamado kidiq. São dados de uma survey de mulheres adultas norte-americanas e seus respectivos filhos. Datado de 2007 possui 434 observações e 4 variáveis:\nkid_score: QI da criança;\nmom_hs: binária (0 ou 1) se a mãe possui diploma de ensino médio;\nmom_iq: QI da mãe; e\nmom_age: idade da mãe.\nVamos usar 4 modelos para modelar QI da criança (kid_score). Os primeiros dois modelos terão apenas um único preditor (mom_hs ou mom_iq), o terceiro usará dois preditores (mom_hs + mom_iq) e o quarto incluirá uma interação entre esses dois preditores (mom_hs * mom_iq),\nDescritivo das variáveis\nAntes de tudo, analise SEMPRE os dados em mãos. Graficamente e com tabelas.\nGráficos\n\n\n# Detectar quantos cores/processadores\noptions(mc.cores = parallel::detectCores())\noptions(Ncpus = parallel::detectCores())\n\nlibrary(rstanarm)\ndata(kidiq)\n\nboxplot(kidiq)\n\n\n\n\nTabelas\nPessoalmente uso o pacote skimr com a função skim():\n\n\nlibrary(skimr)\n\nskim(kidiq)\n\n\nTable 1: Data summary\nName\nkidiq\nNumber of rows\n434\nNumber of columns\n4\n_______________________\n\nColumn type frequency:\n\nnumeric\n4\n________________________\n\nGroup variables\nNone\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nkid_score\n0\n1\n86.80\n20.41\n20\n74\n90\n102\n144\n▁▃▇▇▁\nmom_hs\n0\n1\n0.79\n0.41\n0\n1\n1\n1\n1\n▂▁▁▁▇\nmom_iq\n0\n1\n100.00\n15.00\n71\n89\n98\n110\n139\n▃▇▆▃▂\nmom_age\n0\n1\n22.79\n2.70\n17\n21\n23\n25\n29\n▂▅▇▃▂\n\nModelo 1 - mom_hs\nPrimeiro modelo é apenas a variável mom_hs como preditora:\n\n\nmodel_1 <- stan_glm(\n  kid_score ~ mom_hs,\n  data = kidiq\n  )\n\n\n\nPara ver os valores estimados pelo modelo usamos a função print:\n\n\nprint(model_1)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs\n observations: 434\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 77.6    2.1  \nmom_hs      11.7    2.3  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 19.9    0.7  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nAlém disso, temos a função summary que traz tudo que queremos:\n\n\nsummary(model_1)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 434\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 77.6    2.1 75.0  77.6  80.3 \nmom_hs      11.7    2.4  8.6  11.7  14.7 \nsigma       19.9    0.7 19.0  19.9  20.7 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 86.8    1.4 85.1  86.8  88.5 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3809 \nmom_hs        0.0  1.0  3920 \nsigma         0.0  1.0  3689 \nmean_PPD      0.0  1.0  3892 \nlog-posterior 0.0  1.0  1767 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nModelo 2 - mom_iq\nSegundo modelo é apenas a variável mom_iq como preditora:\n\n\nmodel_2 <- stan_glm(\n  kid_score ~ mom_iq,\n  data = kidiq\n  )\n\n\n\nPodemos também especificar os percentis desejados no sumário:\n\n\nsummary(model_2, probs = c(0.025, 0.975))\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_iq\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 434\n predictors:   2\n\nEstimates:\n              mean   sd   2.5%   98%\n(Intercept) 25.8    5.9 14.4   37.5 \nmom_iq       0.6    0.1  0.5    0.7 \nsigma       18.3    0.6 17.1   19.6 \n\nFit Diagnostics:\n           mean   sd   2.5%   98%\nmean_PPD 86.8    1.2 84.3   89.2 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  4373 \nmom_iq        0.0  1.0  4349 \nsigma         0.0  1.0  3823 \nmean_PPD      0.0  1.0  3687 \nlog-posterior 0.0  1.0  1799 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nModelo 3 - mom_hs + mom_iq\nTerceiro modelo usa as duas variáveis mom_hs e mom_iq como preditoras:\n\n\nmodel_3 <- stan_glm(\n  kid_score ~ mom_hs + mom_iq,\n  data = kidiq\n  )\n\n\n\n\n\nprint(model_3)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs + mom_iq\n observations: 434\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) 25.8    5.9  \nmom_hs       6.0    2.2  \nmom_iq       0.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.1    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nModelo 4 - mom_hs * mom_iq\nQuarto modelo usa as duas variáveis mom_hs e mom_iq como preditoras por meio de uma interação entre as duas:\n\n\nmodel_4 <- stan_glm(\n  kid_score ~ mom_hs * mom_iq,\n  data = kidiq\n  )\n\n\n\n\n\nprint(model_4)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs * mom_iq\n observations: 434\n predictors:   4\n------\n              Median MAD_SD\n(Intercept)   -9.5   14.0  \nmom_hs        48.9   15.5  \nmom_iq         0.9    0.1  \nmom_hs:mom_iq -0.5    0.2  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.0    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nVariáveis qualitativas\nPara as variáveis qualitativas, o R usa um tipo especial de variável chamado factor. A codificação é em números inteiros \\(1,2,\\dots,K\\) mas a relação é distinta/nominal. Ou seja 1 é distinto de 2 e não 1 é 2x menor que 2. Não há relação quantitativa entre os valores das variáveis factor.\nIsso resolve o problema de termos variáveis qualitativas (também chamadas de dummy) em modelos de regressão. Para um factor com \\(K\\) quantidade de classes distintas, temos a possibilidade de criar \\(K-1\\) coeficientes de regressão. Um para cada classe e usando uma como basal (baseline).\n\n\nlibrary(gapminder)\nlevels(gapminder$continent)\n\n\n[1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\" \n\nmodel_5 <- stan_glm(lifeExp ~ gdpPercap + factor(continent), data = gapminder)\n\n\n\n\n\nprint(model_5)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      lifeExp ~ gdpPercap + factor(continent)\n observations: 1704\n predictors:   6\n------\n                          Median MAD_SD\n(Intercept)               47.9    0.3  \ngdpPercap                  0.0    0.0  \nfactor(continent)Americas 13.6    0.6  \nfactor(continent)Asia      8.7    0.6  \nfactor(continent)Europe   17.6    0.6  \nfactor(continent)Oceania  18.1    1.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 8.4    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nObs: para mudar o basal de referência de um factor use a função relevel() do R.\nAtividade Prática\nDois datasets estão disponíveis na pasta datasets/:\nWHO Life Expectancy Kaggle Dataset: datasets/WHO_Life_Exp.csv\nWine Quality Kaggle Dataset: datasets/Wine_Quality.csv\nWHO Life Expectancy\nEsse dataset possui 193 países nos últimos 15 anos.\nVariáveis\ncountry\nyear\nstatus\nlife_expectancy\nadult_mortality\ninfant_deaths\nalcohol\npercentage_expenditure\nhepatitis_b\nmeasles\nbmi\nunder_five_deaths\npolio\ntotal_expenditure\ndiphtheria\nhiv_aids\ngdp\npopulation\nthinness_1_19_years\nthinness_5_9_years\nincome_composition_of_resources\nschooling\nWine Quality Kaggle Dataset\nEsse dataset possui 1599 vinhos e estão relacionados com variantes tintas do vinho “Vinho Verde” português. Para mais detalhes, consulte a referência [Cortez et al., 2009]. Devido a questões de privacidade e logística, apenas variáveis físico-químicas (entradas) e sensoriais (a saída) estão disponíveis (por exemplo, não há dados sobre os tipos de uva, marca de vinho, preço de venda do vinho, etc.).\nfixed_acidity\nvolatile_acidity\ncitric_acid\nresidual_sugar\nchlorides\nfree_sulfur_dioxide\ntotal_sulfur_dioxide\ndensity\np_h\nsulphates\nalcohol\nquality\n\n\n###\n\n\n\nReferências\nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:31:23-03:00"
    },
    {
      "path": "3-Distribuicoes_Estatisticas.html",
      "title": "Distribuições Estatísticas",
      "description": "Distribuições Estatísticas",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nDiscretas\nUniforme Discreta\nBinomial\nPoisson\n\nContínuas\nNormal / Gaussiana\nLog-normal\nExponencial\nDistribuição t de Student\n\nDashboard de Distribuições\nAmbiente\n\n\nA estatística usa distribuições probabilísticas como o motor de sua inferência na elaboração dos valores dos parâmetros estimados e suas incertezas.\nUma distribuição de probabilidade é a função matemática que fornece as probabilidades de ocorrência de diferentes resultados possíveis para um experimento. É uma descrição matemática de um fenômeno aleatório em termos de seu espaço amostral e as probabilidades de eventos (subconjuntos do espaço amostral)\nGeralmente usamos a notação X ~ Dist(par1, par2, ...). Onde X é a variável Dist é a distribuição e par os parâmetros que definem como a distribuição se comporta.\nDiscretas\nDistribuições de probabilidade discretas são aquelas que os resultados são números discretos (também chamados de números inteiros): \\(\\dots, -2, 1, 0,1,2,\\dots, N\\) e \\(N \\in \\mathbb{Z}\\).\nUniforme Discreta\nA distribuição uniforme discreta é uma distribuição de probabilidade simétrica em que um número finito de valores são igualmente prováveis de serem observados. Cada um dos \\(n\\) valores tem probabilidade igual \\(\\frac{1}{n}\\). Outra maneira de dizer “distribuição uniforme discreta” seria “um número conhecido e finito de resultados igualmente prováveis de acontecer.”\nA distribuição uniforme discreta possui dois parâmetros e sua notação é \\(U(a, b)\\):\nLimite Inferior (\\(a\\))\nLimite Superior (\\(b\\))\nExemplo: Um dado.\n\n\nx <- seq(1, 6)\ny <- dunif(x, min = 1, max = 6)\n\nplot(x, y, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Distribuição Uniforme Discreta\",\n  lwd=2, col=\"red\"\n)\n\n\n\n\nBinomial\nA distribuição binomial descreve um evento do número de sucessos em uma sequência de \\(n\\) experimentos independentes, cada um fazendo uma pergunta sim-não.\nA distribuição binomial é freqüentemente usada para modelar o número de sucessos em uma amostra de tamanho \\(n\\) desenhada com substituição de uma população de tamanho \\(N\\).\nA distribuição binomial possui dois parâmetros e sua notação é \\(Bin(n, p)\\):\nNúmero de Experimentos (\\(n\\))\nProbabiliade de Sucessos (\\(p\\))\nExemplo: quantidade de caras em 5 lançamentos de uma moeda.\n\n\nx <- seq(0, 5)\n\nprobs <- c(0.1, 0.2, 0.5)\ncolors <- c(\"red\", \"blue\", \"darkgreen\")\nlabels <- c(\"p=0.1\", \"p=0.2\", \"p=0.5\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribuições Binomiais\",\n  xlim = c(0, 5),\n  ylim = c(0, 1))\n\nfor (i in 1:4){\n  lines(x, dbinom(x, 5, prob = probs[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Desvio Padrões\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nPoisson\nA distribuição Poisson expressa a probabilidade de um determinado número de eventos ocorrerem em um intervalo fixo de tempo ou espaço se esses eventos ocorrerem com uma taxa média constante conhecida e independentemente do tempo desde o último evento. A distribuição de Poisson também pode ser usada para o número de eventos em outros intervalos especificados, como distância, área ou volume.\nA distribuição Poisson possui um parâmetro e sua notação é \\(pois(\\lambda)\\):\nTaxa (\\(\\lambda\\))\nExemplo: Quantidade de e-mails que você recebe diariamente. Quantidade de buracos que você encontra na rua.\n\n\nx <- seq(0, 20)\n\nrates <- c(1, 4, 10)\ncolors <- c(\"red\", \"blue\", \"darkgreen\")\nlabels <- c(\"taxa=1\", \"taxa=4\", \"taxa=10\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribuições Poisson\",\n  xlim = c(0, 20),\n  ylim = c(0, 0.5))\n\nfor (i in 1:4){\n  lines(x, dpois(x, lambda = rates[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Taxas\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nContínuas\nDistribuições de probabilidade contínuas são aquelas que os resultados são valores em uma faixa contínua (também chamados de número reais): \\([-\\infty, \\infty] \\in \\mathbb{R}\\).\nNormal / Gaussiana\nEssa distribuição geralmente é usada nas ciências sociais e naturais para representar variáveis contínuas na qual as suas distribuições não são conhecidas. Esse pressuposto é por conta do teorema do limite central. O teorema do limite central afirma que, em algumas condições, a média de muitas amostras (observações) de uma variável aleatória com média e variância finitas é ela própria uma variável aleatória cuja distribuição converge para uma distribuição normal à medida que o número de amostras aumenta. Portanto, as quantidades físicas que se espera sejam a soma de muitos processos independentes (como erros de medição) muitas vezes têm distribuições que são quase normais.\nA distribuição normal possui dois parâmetros e sua notação é \\(N(\\mu, \\sigma^2)\\):\nMédia (\\(\\mu\\)): média da distribuição e também a moda e a mediana\nDesvio Padrão (\\(\\sigma\\)): a variância da distribuição (\\(\\sigma^2\\)) é uma média de dispersão das observações em relação à média\nExemplo: Altura, Peso etc.\n\n\nx <- seq(-4, 4, length = 100)\n\ndps <- c(0.5, 1, 2, 5)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"gold\")\nlabels <- c(\"dp=0.5\", \"dp=1\", \"dp=2\", \"dp=5\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribuições Normais\",\n  xlim = c(-4, 4),\n  ylim = c(0, 1))\n\nfor (i in 1:4){\n  lines(x, dnorm(x, mean = 0, sd = dps[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Desvio Padrões\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nLog-normal\nA distribuição Log-normal é uma distribuição de probabilidade contínua de uma variável aleatória cujo logaritmo é normalmente distribuído. Assim, se a variável aleatória \\(X\\) for distribuída normalmente por log, então \\(Y = \\ln (X)\\) terá uma distribuição normal.\nUma variável aleatória com distribuição logarítmica aceita apenas valores reais positivos. É um modelo conveniente e útil para medições em ciências exatas e de engenharia, bem como medicina, economia e outros campos, por ex. para energias, concentrações, comprimentos, retornos financeiros e outros valores.\nUm processo log-normal é a realização estatística do produto multiplicativo de muitas variáveis aleatórias independentes, cada uma das quais positiva.\nA distribuição log-normal possui dois parâmetros e sua notação é \\(Lognormal(\\mu, \\sigma^2)\\):\nMédia (\\(\\mu\\)): média do logaritmo natural da distribuição\nDesvio Padrão (\\(\\sigma\\)): a variância do logaritmo natural da distribuição (\\(\\sigma^2\\)) é uma média de dispersão das observações em relação à média\n\n\nx <- seq(0, 3, length = 100)\n\ndps <- c(0.25, 0.5, 1, 1.5)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"gold\")\nlabels <- c(\"dp=0.25\", \"dp=0.5\", \"dp=1\", \"dp=1.5\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribuições Log-Normais\",\n  xlim = c(0, 3),\n  ylim = c(0, 2))\n\nfor (i in 1:4){\n  lines(x, dlnorm(x, mean = 0, sd = dps[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Desvio Padrões\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nExponencial\nA distribuição exponencial é a distribuição de probabilidade do tempo entre eventos que ocorrem de forma contínua e independente a uma taxa média constante.\nA distribuição exponencial possui um parâmetro e sua notação é \\(Exp (\\lambda)\\):\nTaxa (\\(\\lambda\\))\nExemplo: Quanto tempo até o próximo terremoto. Quanto tempo até o próximo ônibus.\n\n\nx <- seq(0, 5, length = 100)\n\nrates <- c(0.5, 1, 1.5, 2)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"gold\")\nlabels <- c(\"taxa=0.5\", \"taxa=1.0\", \"taxa=1.5\", \"taxa=2.0\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribuições Exponenciais\",\n  xlim = c(0, 5),\n  ylim = c(0, 1.5))\n\nfor (i in 1:4){\n  lines(x, dexp(x,rate = rates[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Taxas\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nDistribuição t de Student\nA distribuição t de Student surge ao estimar a média de uma população normalmente distribuída em situações onde o tamanho da amostra é pequeno e o desvio padrão da população é desconhecido.\nSe tomarmos uma amostra de \\(n\\) observações de uma distribuição normal, então a distribuição t com \\(\\nu = n-1\\) graus de liberdade pode ser definida como a distribuição da localização da média da amostra em relação à média verdadeira, dividida pela desvio padrão da amostra, após multiplicar pelo termo padronizador \\(\\sqrt{n}\\).\nA distribuição t é simétrica e em forma de sino, como a distribuição normal, mas tem caudas mais pesadas, o que significa que é mais propensa a produzir valores que estão longe de sua média.\nA distribuição t de Student possui um parâmetro e sua notação é \\(Student (\\nu)\\):\nGraus de Liberdade (\\(\\nu\\)): controla o quanto ela se assemelha com uma distribuição normal\nExemplo: Uma base de dados cheia de outliers.\n\n\nx <- seq(-4, 4, length = 100)\n\ndegfs <- c(1, 3, 8, 30)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"gold\")\nlabels <- c(\"df=1\", \"df=3\", \"df=8\", \"df=30\")\n\nplot(NA, xlab=\"valor de x\",\n  ylab=\"Densidade\",\n  main=\"Comparativo de Distribuições t de Student\",\n  xlim = c(-4, 4),\n  ylim = c(0, 0.5))\n\nfor (i in 1:4){\n  lines(x, dt(x,df = degfs[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Graus de Liberdade\",\n  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\n\n\n\n\nDashboard de Distribuições\nPara acessar todo o zoológico de distribuições use essa ferramenta do Ben Lambert (estatístico do Imperial College of London): https://ben18785.shinyapps.io/distribution-zoo/\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:31:30-03:00"
    },
    {
      "path": "4-Priors.html",
      "title": "Priors",
      "description": "As famosas e controversas Priors",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nTipos de Priors\nPriors para os Modelos\nUniforme (Flat Prior)\nInformativas\nPadrões do rstanarm\nExemplo usando o mtcars\n\nPor quê não é interessante usar priors uniformes (flat priors)\nAtividade\n\nAmbiente\n\n\nA Estatística Bayesiana é caracterizada pelo uso de informação prévia embutida como probabilidade prévia \\(P(H)\\)\n\\[P(H | D)=\\frac{P(H) \\cdot P(D | H)}{P(D)}\\]\nTipos de Priors\nDe maneira geral, podemos ter 3 tipos de priors em uma abordagem Bayesiana:\nuniforme (Flat Prior): não recomendada\nfracamente informativa (weakly informative): pequena restrição com um pouco de senso comum e baixo conhecimento de domínio incorporado\ninformativa (informative): conhecimento de domínio incorporado\nPara se aprofundar mais recomendo a vignette do rstanarm sobre priors\nPriors para os Modelos\nArgumento\nUsado em\nAplica-se à\nprior_intercept\nTodas funções de modelagem exceto stan_polr and stan_nlmer\nConstante (intercept) do modelo, após centralização dos preditores\nprior\nTodas funções de modelagem\nCoeficientes de Regressão, não inclui coeficientes que variam por grupo em modelos multiníveis (veja prior_covariance)\nprior_aux\nstan_glm, stan_glmer, stan_gamm4, stan_nlmer\nParâmetro auxiliar (ex: desvio padrão (standard error - DP), interpretação depende do modelo\nprior_covariance\nstan_glmer, stan_gamm4, stan_nlmer\nMatrizes de covariância em modelos multiníveis\nUniforme (Flat Prior)\nEspecifica-se colocando o valor NULL (nulo em R) no. Exemplo:\nprior_intercept = NULL\nprior = NULL\nprior_aux = NULL\nColocando na função de modelo ficaria stan_glm(y ~ x1 + x2, data = df, prior = NULL, prior_intercept = NULL, prior_aux = NULL)\nInformativas\nColoca-se qualquer distribuição nos argumentos. Exemplo:\nprior = normal(0, 5)\nprior_intercept = student_t(4, 0, 10)\nprior_aux = cauchy(0, 3)\nColocando na função de modelo ficaria stan_glm(y ~ x1 + x2, data = df, prior = normal(0, 5), prior_intercept = student_t(4, 0, 10), prior_aux = cauchy(0, 3))\nPadrões do rstanarm\nAcontece se você não especifica nada nos argumentos de priors. O comportamento difere conforme o modelo. Aqui divido em modelos gaussianos (segue uma likelihood gaussiana ou normal) e outros (binomial, poisson etc)\nModelos Gaussianos\nConstante(Intercept): centralizada com média \\(\\mu_y\\) e desvio padrão de \\(2.5 \\sigma_y\\) - prior_intercept = normal(mean_y, 2.5 * sd_y)\nCoeficientes: para cada coeficiente média \\(\\mu = 0\\) e desvio padrão de \\(2.5\\times\\frac{\\sigma_y}{\\sigma_{x_k}}\\) - prior = normal(0, 2.5 * sd_y/sd_xk)\nOutros Modelos (Binomial, Poisson etc.)\nConstante(Intercept): centralizada com média \\(\\mu = 0\\) e desvio padrão de \\(2.5 \\sigma_y\\) - prior_intercept = normal(0, 2.5 * sd_y)\nCoeficientes: para cada coeficiente média \\(\\mu = 0\\) e desvio padrão de \\(2.5\\times\\frac{1}{\\sigma_{x_k}}\\) - prior = normal(0, 2.5 * 1/sd_xk)\n\nOBS: em todos os modelos prior_aux, o desvio padrão do erro do modelo, a prior padrão é uma distribuição exponencial com taxa \\(\\frac{1}{\\sigma_y}\\): prior_aux = exponential(1/sd_y)\n\nExemplo usando o mtcars\nVamos estimar modelos Bayesianos usando o dataset já conhecido mtcars. Para constar, calcularemos alguns valores antes de ver o sumário das priors:\n\\(\\mu_y\\): média do mpg - 20.09\n\\(2.5 \\sigma_y\\): 2.5 * sd(mtcars$mpg) - 15.07\n\\(2.5\\times\\frac{\\sigma_y}{\\sigma_{x_{\\text{wt}}}}\\): 2.5 * (sd(mtcars$mpg)/sd(mtcars$wt)) - 15.4\n\\(2.5\\times\\frac{\\sigma_y}{\\sigma_{x_{\\text{am}}}}\\): 2.5 * (sd(mtcars$mpg)/sd(mtcars$am)) - 30.2\n\\(\\frac{1}{\\sigma_y}\\): 1/sd(mtcars$mpg) - 0.17\nA função prior_summary resulta um sumário conciso das priors utilizadas em um modelo. Coloque como argumento o modelo estimado:\n\n\nlibrary(rstanarm)\ndefault_prior_test <- stan_glm(mpg ~ wt + am, data = mtcars, chains = 1)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.049406 seconds (Warm-up)\nChain 1:                0.034335 seconds (Sampling)\nChain 1:                0.083741 seconds (Total)\nChain 1: \n\nprior_summary(default_prior_test)\n\n\nPriors for model 'default_prior_test' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 20, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 20, scale = 15)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0], scale = [2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [0,0], scale = [15.40,30.20])\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.17)\n------\nSee help('prior_summary.stanreg') for more details\n\nAgora com priors especificadas:\nComo há dois coeficientes eu especifico médias iguais (\\(0\\)), porém desvios padrões diferentes (\\(5\\) para wt e \\(6\\) para am) usando a função de combinar do R (combine) - c()\n\n\ncustom_prior_test <- stan_glm(mpg ~ wt + am, data = mtcars, chains = 1,\n         prior = normal(c(0, 0), c(5, 6)),\n         prior_intercept = student_t(4, 0, 10),\n         prior_aux = cauchy(0, 3))\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.038517 seconds (Warm-up)\nChain 1:                0.039088 seconds (Sampling)\nChain 1:                0.077605 seconds (Total)\nChain 1: \n\nprior_summary(custom_prior_test)\n\n\nPriors for model 'custom_prior_test' \n------\nIntercept (after predictors centered)\n ~ student_t(df = 4, location = 0, scale = 10)\n\nCoefficients\n ~ normal(location = [0,0], scale = [5,6])\n\nAuxiliary (sigma)\n ~ half-cauchy(location = 0, scale = 3)\n------\nSee help('prior_summary.stanreg') for more details\n\nPor quê não é interessante usar priors uniformes (flat priors)\nUma prior totalmente uniforme ou chapada (flat) é algo que devemos evitar pelo simples motivo que ela parte da premissa de que “tudo é possível.” Não há limites na crença de que tamanho o valor deve ser.\nPriors chapadas e super-vagas geralmente não são recomendadas e algum esforço deve ser incluído para ter, pelo menos, priors um pouco informativa. Por exemplo, é comum esperar que os tamanhos de efeito realistas sejam da ordem de magnitude \\(0.1\\) em uma escala padronizada (por exemplo, uma inovação educacional que pode melhorar as pontuações dos testes em \\(0.1\\) desvios padrão). Nesse caso, um prior de \\(N \\sim (0,1)\\) poderia ser considerado muito informativo, de uma maneira ruim, pois coloca a maior parte de sua massa em valores de parâmetro que são irrealisticamente grandes em valor absoluto. O ponto geral aqui é que se considerarmos uma prior como “fraca” ou “forte,” isso é uma propriedade não apenas da prior, mas também da pergunta que está sendo feita.\nQuando dizemos que a prior é “pouco informativa,” o que queremos dizer é que, se houver uma quantidade razoavelmente grande de dados, a likelihood dominará e a prior não será importante. Se os dados forem fracos, porém, esta “prior fracamente informativo” influenciará fortemente a inferência posterior.\nNão se esqueça que distribuição normal tem suporte \\(\\mathbb{R}\\), ou seja pode acontecer qualquer número entre \\(-\\infty\\) até \\(\\infty\\) independente da média \\(\\mu\\) ou desvio padrão \\(\\sigma\\).\nAtividade\nRegressão linear pensando nas priors. Usar o dataset do pacote carData chamado Salaries\n\n\nlibrary(carData)\ndata(\"Salaries\")\n?Salaries\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:31:32-03:00"
    },
    {
      "path": "5-MCMC.html",
      "title": "Markov Chain Monte Carlo -- MCMC",
      "description": "O motor por trás da Estatística Bayesiana",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nPara quê serve o denominador \\(P(\\text{data})\\)?\nSe removermos o denominador de Bayes o que temos?\nSimulação Montecarlo com correntes Markov – (MCMC)\nSimulações – Setup\nMetropolis e Metropolis-Hastings\nGibbs\nO que acontece quando rodamos correntes Markov em paralelo?\n\nHamiltonian Monte Carlo – HMC\nDistribuição dos Momentos – \\(P(\\phi)\\)\nAlgoritmo de HMC\nHMC – Implementação\n\n“Não entendi nada…”\nImplementação com o rstanarm\nMétricas da simulação MCMC\nO que fazer se não obtermos convergência?\n\nGráficos de Diagnósticos do MCMC\nTraceplot\nPosterior Predictive Check\n\nO quê fazer para convergir suas correntes Markov\nAmbiente\n\n\nA principal barreira computacional para estatística Bayesiana é o denominador \\(P(\\text{data})\\) da fórmula de Bayes:\n\\[P(\\theta | \\text{data})=\\frac{P(\\theta) \\cdot P(\\text{data} | \\theta)}{P(\\text{data})}\\]\nEm casos discretos podemos fazer o denominador virar a soma de todos os parâmetros usando a regra da cadeia de probabilidade:\n\\[P(A,B|C)=P(A|B,C) \\times P(B|C)\\]\nIsto também é chamado de marginalização:\n\\[P(\\text{data})=\\sum_{\\theta} P(\\text{data} | \\theta) \\times P(\\theta)\\]\nPorém no caso de valores contínuos o denominador \\(P(\\text{data})\\) vira uma integral bem grande e complicada de calcular:\n\\[P(\\text{data})=\\int_{\\theta} P(\\text{data} | \\theta) \\times P(\\theta)d \\theta\\]\nEm muitos casos essa integral vira intratável (incalculável) e portanto devemos achar outras maneiras de calcular a probabilidade posterior \\(P(\\theta | \\text{data})\\) de Bayes sem usar o denominador \\(P(\\text{data})\\).\nPara quê serve o denominador \\(P(\\text{data})\\)?\nPara normalizar a posterior com o intuito de torná-la uma distribuição probabilística válida. Isto quer dizer que a soma de todas as probabilidades dos eventos possíveis da distribuição probabilística devem ser iguais a 1:\nno caso de distribuição probabilística discreta: \\(\\sum_{\\theta} P(\\theta | \\text{data}) = 1\\)\nno caso de distribuição probabilística contínua: \\(\\int_{\\theta} P(\\theta | \\text{data})d \\theta = 1\\)\nSe removermos o denominador de Bayes o que temos?\nAo removermos o denominador \\((\\text{data})\\) temos que a posterior \\(P(\\theta | \\text{data})\\) é proporcional à prior multiplicada pela verossimilhança \\(P(\\theta) \\cdot P(\\text{data} | \\theta)\\)1.\n\\[P(\\theta | \\text{data}) \\propto P(\\theta) \\cdot P(\\text{data} | \\theta)\\]\nEste vídeo do YouTube explica muito bem o problema do denominador.\n\n\nPlease use a browser that supports iframe embedding. If you are seeing this message Google “browser iframe embedding not rendering.”\n\n\nSimulação Montecarlo com correntes Markov – (MCMC)\nAí que entra simulação Montecarlo com correntes Markov (do inglês Markov Chain Monte Carlo – MCMC). MCMC é uma classe ampla de ferramentas computacionais para aproximação de integrais e geração de amostras de uma probabilidade posterior (S. Brooks, Gelman, Jones, & Meng, 2011). MCMC é usada quando não é possível coletar amostras de \\(\\theta\\) direto da distribuição probabilística posterior \\(P(\\theta | \\text{data})\\). Ao invés disso, nos coletamos amostras de maneira iterativa que a cada passo do processo nós esperamos que a distribuição da qual amostramos \\(P^*(\\theta^* | \\text{data})\\) (aqui \\(*\\) quer dizer simulado) se torna cada vez mais similar à posterior \\(P(\\theta | \\text{data})\\). Tudo isso é para eliminar o cálculo (muitas vezes impossível) do denominador \\(P(\\text{data})\\).\nA ideia é definir uma corrente Markov ergódica (quer dizer que há uma distribuição estacionária única) dos quais o conjunto de estados possíveis é o espaço amostral e a distribuição estacionária é a distribuição a ser aproximada (ou amostrada). Seja \\(X_0, X_1, \\dots, X_n\\) uma simulação da corrente. A corrente Markov converge à distribuição estacionária de qualquer estado inicial \\(X_0\\) após um número suficiente grande de iterações \\(r\\), a distribuição do estado \\(X_r\\) estará similar à distribuição estacionária, então podemos usá-la com amostra. As correntes Markov possuem uma propriedade que a distribuição de probabilidade do próximo estado depende apenas do estado atual e não na sequência de eventos que precederam: \\(P(X_{n+1}=x|X_{0},X_{1},X_{2},\\ldots ,X_{n}) = P(X_{n+1}=x|X_{n})\\). Essa propriedade é chamada de Markoviana, em homenagem ao matemático Andrei Andreyevich Markov (figura 1). Similarmente, repetindo esse argumento com \\(X_r\\) como o ponto inicial, podemos usar \\(X_{2r}\\) como amostra, e assim por diante. Podemos então usar a sequência de estados \\(X_r, X_{2r}, X_{3r}, \\dots\\) como quase amostras independentes da distribuição estacionária da corrente Markov.\n\n\n\nFigure 1: Andrei Andreyevich Markov. Figura de https://www.wikipedia.org\n\n\n\nA eficácia dessa abordagem depende em:\no quão grande \\(r\\) deve ser para garantir uma amostra adequadamente boa; e\npoder computacional requerido para cada iteração da corrente Markov.\nAlém disso, é costumeiro descartarmos as primeiras iterações do algoritmo pois elas costumam não ser representativas da distribuição a ser aproximada. Nas iterações iniciais de algoritmos MCMC geralmente a corrente Markov está em um processo de aquecimento2 (warm-up) e seu estado está bem distante do ideal para começarmos uma amostragem fidedigna. Geralmente, recomenda-se que se descarte metade das iterações (Gelman et al., 2013a). Por exemplo: se a corrente Markov possui 4.000 iterações, descartamos as 2.000 primeiras como warm-up.\nSimulações – Setup\nEstou usando diversos pacotes:\nggplot2, plotly e ggforce para gráficos.\ngganimate para animações (GIFs).\nMASS para simulações aleatórias de distribuições multivariadas.\nrstan para funções de sumário e métricas de convergência e desempenho de simulações MCMC.\n\n\nlibrary(ggplot2)\ntheme_set(theme_minimal())\nlibrary(plotly)\nlibrary(gganimate)\nlibrary(ggforce)\nlibrary(MASS)\nlibrary(rstan)\n\n\n\nVamos começar com um problema didático de uma distribuição normal multivariada de \\(X\\) e \\(Y\\), onde\n\\[\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix} \\sim \\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}, \\mathbf{\\Sigma}\n\\right) \\\\\n\\mathbf{\\Sigma} \\sim\n\\begin{pmatrix}\n\\sigma^2_{X} & \\sigma_{X}\\sigma_{Y} \\rho \\\\\n\\sigma_{X}\\sigma_{Y} \\rho & \\sigma^2_{Y}\n\\end{pmatrix}\n\\]\nSe designarmos \\(\\mu_X = \\mu_Y = 0\\) e \\(\\sigma_X = \\sigma_Y = 1\\) (média 0 e desvio padrão 1 para ambos \\(X\\) e \\(Y\\)), temos a seguinte formulação:\n\\[\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix} \\sim \\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix}, \\mathbf{\\Sigma}\n\\right), \\\\\n\\mathbf{\\Sigma} \\sim\n\\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{pmatrix}.\n\\]\nSó faltando designar um valor de \\(\\rho\\) para a correlação entre \\(X\\) e \\(Y\\). Para o nosso exemplo vamos usar correlação de 0.8 (\\(\\rho = 0.8\\)):\n\\[\n\\mathbf{\\Sigma} \\sim\n\\begin{pmatrix}\n1 & 0.8 \\\\\n0.8 & 1\n\\end{pmatrix}.\n\\]\n\n\nmus  <- c(0, 0)\nsigmas <- c(1, 1)\nr <- 0.8\nSigma <- diag(sigmas)\nSigma[1, 2] <- r\nSigma[2, 1] <- r\ndft <- data.frame(mvrnorm(1e5, mus, Sigma))\n\n\n\nNa figura 2 é possível ver um gráfico de densidade de uma distribuição multivariada normal de duas variáveis normais \\(X\\) e \\(Y\\), ambas com média 0 e desvio padrão 1. Sendo que a correlação entre elas é 0.8. E na figura 3 é possível ver uma imagem 3-D interativa da mesma distribuição, fique a vontade em usar seu mouse (dedo ou caneta, dependendo do dispositivo) para movimentar a imagem.\n\n\nggplot(dft, aes(X1, X2)) +\n  geom_density2d_filled() +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(title = \"Multivariada Normal\",\n       subtitle = expression(list(mu == 0, sigma == 1, rho == 0.8)),\n       caption = \"10.000 simulações\",\n       x = expression(X), y = expression(Y)) +\n  theme(legend.position = \"NULL\")\n\n\n\n\nFigure 2: Gráfico de Densidade de uma distribuição Multivariada Normal\n\n\n\n\n\ndens <- kde2d(dft$X1, dft$X2)\nplot_ly(x = dens$x,\n        y = dens$y,\n        z = dens$z) %>% add_surface()\n\n\n\n\n{\"x\":{\"visdat\":{\"f151338bc78c\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"f151338bc78c\",\"attrs\":{\"f151338bc78c\":{\"x\":[-4.11222323909044,-3.76275945114626,-3.41329566320208,-3.0638318752579,-2.71436808731372,-2.36490429936954,-2.01544051142536,-1.66597672348118,-1.316512935537,-0.967049147592815,-0.617585359648634,-0.268121571704453,0.0813422162397277,0.430806004183909,0.780269792128089,1.12973358007227,1.47919736801645,1.82866115596063,2.17812494390481,2.52758873184899,2.87705251979317,3.22651630773736,3.57598009568154,3.92544388362572,4.2749076715699],\"y\":[-4.32381413035071,-3.95784855910568,-3.59188298786066,-3.22591741661563,-2.85995184537061,-2.49398627412558,-2.12802070288055,-1.76205513163553,-1.3960895603905,-1.03012398914547,-0.664158417900448,-0.298192846655422,0.0677727245896049,0.433738295834631,0.799703867079657,1.16566943832468,1.53163500956971,1.89760058081474,2.26356615205976,2.62953172330479,2.99549729454982,3.36146286579484,3.72742843703987,4.09339400828489,4.45935957952992],\"z\":[[3.40651108352134e-09,1.41934533126133e-08,8.54786240302624e-08,0.000130831863209747,1.94776526567504e-06,1.49783603255103e-06,1.1170089576991e-08,6.01986954373106e-12,1.16817732135535e-10,1.50245544102462e-14,4.62944974810078e-23,4.83295093316628e-35,2.28048400569722e-45,8.78734985512727e-51,2.21962779786052e-61,3.67504668072926e-77,3.98846726370328e-98,2.83738455537123e-124,1.16702068177763e-153,1.56588648116791e-183,5.9460095882168e-217,3.04648237511363e-242,1.0476778777822e-272,2.36166828733039e-308,0],[4.39462560522794e-05,6.94943900679841e-05,1.04291502349634e-05,5.75970473307081e-05,0.000176906604268204,0.000370540209494104,1.55421446807263e-05,8.30475843056959e-07,1.53069632670964e-05,7.14342142452053e-09,1.13543480848847e-15,2.9038893623358e-25,2.64561847471716e-28,1.01950388004498e-33,2.57520093025581e-44,4.26377054766638e-60,4.6281649856682e-81,2.42118913413043e-104,5.00622985414094e-129,7.25515626339583e-159,3.68875889960785e-180,1.93530271945878e-205,6.65546074071442e-236,5.25171440440826e-271,5.21884573812369e-305],[9.55774448047614e-05,0.000177213516750328,0.000316039775630404,0.000475626271585416,0.000761122825446468,0.000814816903847166,0.000578652080468251,0.000157642005236617,0.000152603615623742,3.79131793396399e-06,7.51171055499296e-11,3.17899382836251e-15,5.96544729558909e-16,2.29881835031841e-21,5.80666661659396e-32,9.62751627134557e-48,6.45601148716749e-65,2.03894997743999e-84,4.22210611955478e-109,5.68993288800346e-128,4.55423370733433e-148,2.3893730364403e-173,9.74271181456202e-204,3.23808866743861e-233,4.50467722508192e-267],[1.98596665642571e-07,4.68000995544877e-05,0.000509090380019503,0.000931319298108951,0.0012891264719117,0.00255519224543074,0.00194691180666576,0.00139812985945186,0.000387322200480871,0.000138389622336007,1.12962183753118e-05,4.51541808458966e-08,2.61431851364547e-08,1.00740783248176e-13,2.57450769746435e-24,2.19535111827777e-35,1.05790746618256e-49,3.3419444921475e-69,1.11809791229766e-85,1.36529655130866e-100,1.09278610207721e-120,5.81233702380819e-146,2.55947654424468e-171,5.43203463930242e-200,7.55679638075802e-234],[2.0971717790889e-11,2.45146152673218e-05,0.00101641626246237,0.0012023963529308,0.00431348854715178,0.00596725675304324,0.00681722538407796,0.00633846543983796,0.00269032102983323,0.00186745629509051,0.000257016293377211,6.68099172102812e-05,2.23554936688109e-05,9.43773499495428e-11,9.51160167439667e-16,6.99272785694416e-25,3.37970089697892e-39,2.8443677530022e-53,5.21415549375954e-63,6.36695308508056e-78,5.10133947007011e-98,2.57749276005559e-119,8.34462177624758e-143,1.77101149161936e-171,2.46374961111803e-205],[2.18739954530607e-13,8.27469962250889e-07,0.00068247158549802,0.00129956984621859,0.00404868380339934,0.0102606158585002,0.0148087501034371,0.0152333593380476,0.0125919827053159,0.00699472312030991,0.00241781085109058,0.000326864762138592,1.64609783322298e-05,5.34463376690191e-06,5.93226571697135e-10,5.36558733277978e-19,1.90647639364607e-28,2.53683060667389e-35,4.72578158624974e-45,5.77104072981931e-60,3.31061440322712e-77,1.63303711875425e-95,5.28749916069209e-119,1.12218648450566e-147,1.56113414734455e-181],[2.95266849169562e-16,2.85416661335113e-08,8.90740945415943e-05,0.00122580118110615,0.0046414034091961,0.0107569609558745,0.0239117407486324,0.0345320261172182,0.034266290423328,0.025908337162062,0.0111683859733492,0.00396670124627087,0.00103938025934649,0.000215424763750705,2.74471340647353e-07,1.04918685044655e-11,1.59258336217706e-17,4.46886513511614e-22,8.32438239306983e-32,5.50420318373213e-45,4.07127773250312e-58,2.01105763346145e-76,6.51146592352183e-100,1.38195370474173e-128,2.93245589422197e-157],[5.39964840899981e-19,5.73783566121195e-10,4.95237822889483e-06,0.000356036530448518,0.00364161528795007,0.0112329442468965,0.0252826501600385,0.0504539247890418,0.0644047045560643,0.0565147591080807,0.0365725560107204,0.0168018818177516,0.00511766159567819,0.00114102783214639,0.000134221597433633,3.14802960871893e-07,5.45629826051782e-09,1.53072361775968e-13,1.41245242401531e-22,1.29303645153607e-30,9.74415078109988e-44,4.81324295289534e-62,1.55844700619099e-85,4.43400824553211e-109,2.84966144455127e-127],[8.08171755432348e-23,9.9520282620662e-12,8.72151014566793e-06,0.000237656044981198,0.00242087074777216,0.0074732039291603,0.022384642393563,0.0557037311872487,0.0912557128673284,0.111817977555221,0.0876421013472796,0.0510910440320731,0.0211567749509494,0.00631968870616185,0.00125306808856439,0.000249914243381994,3.94877458670163e-05,1.74347212052722e-09,5.23218109941235e-13,6.0146277187387e-21,4.53254441838339e-34,2.23890921563308e-52,8.54657858913127e-71,8.37418644669075e-84,5.38198765821467e-102],[1.77556608338451e-26,4.68244714950935e-15,5.72836682532587e-08,5.26618049520792e-05,0.000520971666093416,0.00358304354991811,0.0146024372316178,0.0418007880446392,0.0937576081086299,0.158070989476187,0.157995985258691,0.117472705761148,0.068309234060372,0.0258290121813925,0.00614303473251792,0.00140371367897623,0.000338744362715045,2.87186516456263e-05,4.72966243885067e-08,5.43755630957161e-16,4.12176724282202e-29,8.46961929462488e-42,3.13499804301411e-50,3.07380889542921e-63,1.97549954784854e-81],[3.73881979289422e-28,4.1820126805177e-15,3.42658639075899e-07,0.000213096739325229,4.3973222484619e-05,0.00128105280915237,0.00620857351658763,0.0236609682820968,0.0673959425125629,0.142023226278179,0.202085537513268,0.206177388365867,0.142910430049431,0.0718865969637724,0.0247087506133945,0.0047660988724505,0.0012606718949474,0.000106045234620241,1.61011212783354e-07,3.22788996902712e-14,9.2123618335293e-20,1.49536811118862e-26,2.23643511113796e-34,2.19278426565827e-47,1.4092757463806e-65],[3.96017717925257e-29,3.86645225973798e-16,2.47948115467607e-08,1.04690276048912e-05,9.33739859692154e-08,0.000133111110976272,0.001505336639134,0.0100449990636382,0.0356202331310127,0.105081641208442,0.19439947445164,0.245025075795557,0.223771691425712,0.143465906225543,0.0646419650813796,0.0193175819154857,0.00436838485724641,0.000892066964783552,3.18041827586292e-05,1.69881000386951e-07,1.59463339843398e-12,2.07291272771527e-15,3.10070286927428e-23,3.04018320512746e-36,1.95388872614587e-54],[9.27339116358244e-35,9.03850964769535e-22,5.77463739774986e-14,2.41842863581794e-11,3.88926919292566e-09,0.000100098427596149,0.00050944399843051,0.00360939499961912,0.0136893590316382,0.0543814561636372,0.125589050531115,0.214168898320003,0.252702330848491,0.212680258807483,0.12368371559332,0.0571604244308721,0.0148492657453032,0.00428301746930206,0.000533538675367115,2.75095327364781e-05,2.44919907449026e-06,5.58559793676326e-09,8.35504525271334e-17,8.19197108729206e-30,5.26488017941314e-48],[4.22630081869033e-45,4.11918272416941e-32,2.63161451387286e-24,3.34109926960455e-18,4.16151119979132e-09,3.41162097549106e-05,8.99545384914694e-05,0.000537014515485226,0.00565794795375072,0.0201673293936082,0.0650968150283119,0.13459457027411,0.216709166154952,0.233737316709245,0.179942550046156,0.0954030715862073,0.0358967231160644,0.00978785209447172,0.00200550639931911,0.000394946604594165,0.000128582191560498,2.92511773464097e-07,4.37544553451306e-15,4.29280429019131e-28,6.30606127729647e-38],[3.74346108554796e-60,3.64858019840399e-47,6.70536258912719e-37,1.26938304068891e-22,1.58065154101849e-13,1.29158993030982e-09,1.61465392359957e-06,9.08018401722032e-05,0.000992221126994708,0.00512632101729583,0.0213674249325609,0.0609771333181536,0.127073954333846,0.182795988224094,0.182286136161845,0.128019925458391,0.0639035259991266,0.0214065384771248,0.00490871234422249,0.000973054976315337,4.01022180114187e-05,5.72791275188721e-09,9.59043068097496e-17,2.97039793505775e-18,6.78914545169897e-25],[6.44423115594109e-80,1.76612292089083e-65,4.93647515191911e-46,9.37777211109363e-32,1.1698365693386e-22,1.41425275871238e-13,6.22698386517161e-07,2.62951805516803e-05,0.000279527597873221,0.00157860429446793,0.00602574560623074,0.018752616701671,0.0543021443993639,0.103895733283513,0.132977779790837,0.128374603771506,0.0753939336135714,0.0355212183761688,0.0120559569069517,0.00206194514165532,0.000441096970059341,2.73484240954569e-07,1.78469373193845e-08,6.21521805008958e-10,1.42055105743125e-16],[7.68729358424888e-104,2.44560642444526e-79,7.08775698856484e-60,1.3645720073284e-45,1.85668279000812e-30,1.24717099176323e-18,5.51514928616487e-12,3.59878422919363e-08,7.11836377827706e-06,0.000206159016212575,0.000677559737625132,0.00454096610629842,0.0184411701444843,0.0423810497069583,0.0716411419156444,0.0888352172498334,0.0766481090433616,0.0426458883542985,0.0175314894554273,0.00410447707852974,0.00104229547613349,0.000236141437094331,7.29387726175071e-05,2.52746047466519e-06,5.77674769488895e-13],[1.54348158058503e-122,6.82438055278491e-98,2.1764367130903e-78,3.10524776593914e-57,3.18217737427384e-40,2.1376226328524e-28,2.96521296109842e-21,3.35038012197265e-15,1.97523298476773e-08,0.000133046448270912,0.000137471144441657,0.000903339786529172,0.00339562223667828,0.0128631814378422,0.0284907613212608,0.0459004645516009,0.0504343969800885,0.0368454386741711,0.020295131275482,0.00834622530427719,0.0025276260264015,0.000264485978452582,7.42808296982338e-05,2.97754176124738e-07,5.54723640489585e-14],[8.37072222983159e-146,6.47496277221125e-121,6.61607426510692e-94,1.03435132478381e-71,1.05997695834429e-54,7.26629325820164e-43,3.44825400430606e-33,1.16750358152517e-20,8.87833074222978e-12,6.03705086913571e-08,3.0979146677307e-07,0.000111121155685573,0.000546159808841194,0.00234731614653475,0.00811613899679378,0.01524872508983,0.0224969610775458,0.0222048655285225,0.0165153003974538,0.00796127257796058,0.00238983759119942,0.000568475706754498,0.000111703499381798,1.37762505288305e-06,7.78323799691101e-13],[5.8174658995359e-173,1.79576621930351e-140,4.28308842051526e-113,6.69614350285288e-91,6.86338053828388e-74,5.70790798535558e-61,9.45947538521377e-43,1.05207977853767e-28,8.00037164310086e-20,5.43819520219381e-16,1.1447107087918e-12,7.6069411229749e-08,1.88824406030061e-05,0.000282589139759991,0.0013566840600888,0.0043332775154336,0.00841566316788047,0.0115467512308974,0.00989353732973917,0.00702460127430449,0.00248407446365753,0.000835282652146337,0.00019689042809369,3.98684425041365e-05,1.13279935581737e-06],[6.20932658184517e-197,2.25939039635564e-164,5.3888801122331e-137,8.42494444063686e-115,9.85285809081692e-98,9.81016236238191e-75,1.65677535778078e-55,1.84264779117487e-41,1.40112690321962e-32,9.7734482363793e-29,8.84275975640076e-21,5.42678565856387e-14,3.55725848792818e-09,2.50688031203498e-05,0.00037257061396806,0.000652072224755515,0.00220596575329706,0.00390824530472625,0.00383520710782124,0.00356746514374532,0.00198660746096792,0.000847086231459706,0.000420124430561813,0.000206161951735914,0.000139297549047621],[1.51834614100114e-225,5.52481278783358e-193,1.31772509679596e-165,2.06533081524484e-143,1.29673322046541e-116,3.33931942363595e-92,5.63955821152384e-73,6.27221302516466e-59,4.76902554018659e-50,2.05571003722159e-43,6.4564082722214e-32,9.97509200941749e-22,2.50203874102328e-11,5.38184636508631e-06,2.28640215213753e-05,5.21287951737725e-05,0.000417456509937771,0.000641775046044822,0.00116730324640233,0.00158181795727826,0.00100667520344559,0.00068510033767778,0.000277599060128235,4.61300248449355e-05,4.35426880886139e-05],[7.215756742882e-259,2.6256005832423e-226,6.26274913779807e-199,2.18370540673512e-168,8.57861654858231e-139,2.20914667283235e-114,3.73088057671666e-95,4.14938831485597e-81,3.15554639168223e-72,1.05662622923261e-59,1.43227275358807e-42,5.4931932302565e-27,1.4524397658288e-16,3.79389505892861e-11,2.20909001199427e-10,3.05546580679959e-09,4.35913186858356e-06,3.31707747133925e-05,0.000430542207310076,0.000586742208506192,0.000307810148878761,0.000253683181760847,0.000409041984564786,7.35977011753656e-06,5.00851432147429e-07],[6.66465670367594e-297,2.42507689762034e-264,4.68473831753259e-230,2.80766623244054e-195,1.10298265871409e-165,2.84037672197857e-141,4.79691936622069e-122,5.33497231006159e-108,2.51418740546229e-97,2.69430717937953e-73,1.56978231833671e-52,6.05179120216435e-37,1.68469487817489e-26,5.83767588635578e-21,4.46168128161558e-20,1.47718573329587e-15,3.03760156043252e-12,1.11175862458942e-08,4.77448480759622e-07,1.37527946349091e-05,3.27369657863926e-05,4.5069603918654e-05,2.18665863961594e-05,2.45790160940866e-07,2.16743981653795e-12],[0,1.28033478220917e-301,1.17063504610327e-261,7.01587297133337e-227,2.75616313180305e-197,7.09761020470235e-173,1.1986662995098e-153,1.33312158277165e-139,6.45923897222376e-114,5.73686958901279e-88,3.34560845825795e-67,1.30290708566137e-51,3.98283248267262e-41,1.95445068709012e-35,2.5163612212255e-34,2.13073194576148e-26,2.2103622973781e-22,1.36299881951402e-16,5.4223823620928e-13,9.64890337003241e-07,4.75103088380026e-05,1.74076683148872e-08,2.3982844765835e-05,4.12736376796499e-05,4.65593304437425e-10]],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"surface\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":[]},\"yaxis\":{\"title\":[]},\"zaxis\":{\"title\":[]}},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"colorbar\":{\"title\":\"\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"x\":[-4.11222323909044,-3.76275945114626,-3.41329566320208,-3.0638318752579,-2.71436808731372,-2.36490429936954,-2.01544051142536,-1.66597672348118,-1.316512935537,-0.967049147592815,-0.617585359648634,-0.268121571704453,0.0813422162397277,0.430806004183909,0.780269792128089,1.12973358007227,1.47919736801645,1.82866115596063,2.17812494390481,2.52758873184899,2.87705251979317,3.22651630773736,3.57598009568154,3.92544388362572,4.2749076715699],\"y\":[-4.32381413035071,-3.95784855910568,-3.59188298786066,-3.22591741661563,-2.85995184537061,-2.49398627412558,-2.12802070288055,-1.76205513163553,-1.3960895603905,-1.03012398914547,-0.664158417900448,-0.298192846655422,0.0677727245896049,0.433738295834631,0.799703867079657,1.16566943832468,1.53163500956971,1.89760058081474,2.26356615205976,2.62953172330479,2.99549729454982,3.36146286579484,3.72742843703987,4.09339400828489,4.45935957952992],\"z\":[[3.40651108352134e-09,1.41934533126133e-08,8.54786240302624e-08,0.000130831863209747,1.94776526567504e-06,1.49783603255103e-06,1.1170089576991e-08,6.01986954373106e-12,1.16817732135535e-10,1.50245544102462e-14,4.62944974810078e-23,4.83295093316628e-35,2.28048400569722e-45,8.78734985512727e-51,2.21962779786052e-61,3.67504668072926e-77,3.98846726370328e-98,2.83738455537123e-124,1.16702068177763e-153,1.56588648116791e-183,5.9460095882168e-217,3.04648237511363e-242,1.0476778777822e-272,2.36166828733039e-308,0],[4.39462560522794e-05,6.94943900679841e-05,1.04291502349634e-05,5.75970473307081e-05,0.000176906604268204,0.000370540209494104,1.55421446807263e-05,8.30475843056959e-07,1.53069632670964e-05,7.14342142452053e-09,1.13543480848847e-15,2.9038893623358e-25,2.64561847471716e-28,1.01950388004498e-33,2.57520093025581e-44,4.26377054766638e-60,4.6281649856682e-81,2.42118913413043e-104,5.00622985414094e-129,7.25515626339583e-159,3.68875889960785e-180,1.93530271945878e-205,6.65546074071442e-236,5.25171440440826e-271,5.21884573812369e-305],[9.55774448047614e-05,0.000177213516750328,0.000316039775630404,0.000475626271585416,0.000761122825446468,0.000814816903847166,0.000578652080468251,0.000157642005236617,0.000152603615623742,3.79131793396399e-06,7.51171055499296e-11,3.17899382836251e-15,5.96544729558909e-16,2.29881835031841e-21,5.80666661659396e-32,9.62751627134557e-48,6.45601148716749e-65,2.03894997743999e-84,4.22210611955478e-109,5.68993288800346e-128,4.55423370733433e-148,2.3893730364403e-173,9.74271181456202e-204,3.23808866743861e-233,4.50467722508192e-267],[1.98596665642571e-07,4.68000995544877e-05,0.000509090380019503,0.000931319298108951,0.0012891264719117,0.00255519224543074,0.00194691180666576,0.00139812985945186,0.000387322200480871,0.000138389622336007,1.12962183753118e-05,4.51541808458966e-08,2.61431851364547e-08,1.00740783248176e-13,2.57450769746435e-24,2.19535111827777e-35,1.05790746618256e-49,3.3419444921475e-69,1.11809791229766e-85,1.36529655130866e-100,1.09278610207721e-120,5.81233702380819e-146,2.55947654424468e-171,5.43203463930242e-200,7.55679638075802e-234],[2.0971717790889e-11,2.45146152673218e-05,0.00101641626246237,0.0012023963529308,0.00431348854715178,0.00596725675304324,0.00681722538407796,0.00633846543983796,0.00269032102983323,0.00186745629509051,0.000257016293377211,6.68099172102812e-05,2.23554936688109e-05,9.43773499495428e-11,9.51160167439667e-16,6.99272785694416e-25,3.37970089697892e-39,2.8443677530022e-53,5.21415549375954e-63,6.36695308508056e-78,5.10133947007011e-98,2.57749276005559e-119,8.34462177624758e-143,1.77101149161936e-171,2.46374961111803e-205],[2.18739954530607e-13,8.27469962250889e-07,0.00068247158549802,0.00129956984621859,0.00404868380339934,0.0102606158585002,0.0148087501034371,0.0152333593380476,0.0125919827053159,0.00699472312030991,0.00241781085109058,0.000326864762138592,1.64609783322298e-05,5.34463376690191e-06,5.93226571697135e-10,5.36558733277978e-19,1.90647639364607e-28,2.53683060667389e-35,4.72578158624974e-45,5.77104072981931e-60,3.31061440322712e-77,1.63303711875425e-95,5.28749916069209e-119,1.12218648450566e-147,1.56113414734455e-181],[2.95266849169562e-16,2.85416661335113e-08,8.90740945415943e-05,0.00122580118110615,0.0046414034091961,0.0107569609558745,0.0239117407486324,0.0345320261172182,0.034266290423328,0.025908337162062,0.0111683859733492,0.00396670124627087,0.00103938025934649,0.000215424763750705,2.74471340647353e-07,1.04918685044655e-11,1.59258336217706e-17,4.46886513511614e-22,8.32438239306983e-32,5.50420318373213e-45,4.07127773250312e-58,2.01105763346145e-76,6.51146592352183e-100,1.38195370474173e-128,2.93245589422197e-157],[5.39964840899981e-19,5.73783566121195e-10,4.95237822889483e-06,0.000356036530448518,0.00364161528795007,0.0112329442468965,0.0252826501600385,0.0504539247890418,0.0644047045560643,0.0565147591080807,0.0365725560107204,0.0168018818177516,0.00511766159567819,0.00114102783214639,0.000134221597433633,3.14802960871893e-07,5.45629826051782e-09,1.53072361775968e-13,1.41245242401531e-22,1.29303645153607e-30,9.74415078109988e-44,4.81324295289534e-62,1.55844700619099e-85,4.43400824553211e-109,2.84966144455127e-127],[8.08171755432348e-23,9.9520282620662e-12,8.72151014566793e-06,0.000237656044981198,0.00242087074777216,0.0074732039291603,0.022384642393563,0.0557037311872487,0.0912557128673284,0.111817977555221,0.0876421013472796,0.0510910440320731,0.0211567749509494,0.00631968870616185,0.00125306808856439,0.000249914243381994,3.94877458670163e-05,1.74347212052722e-09,5.23218109941235e-13,6.0146277187387e-21,4.53254441838339e-34,2.23890921563308e-52,8.54657858913127e-71,8.37418644669075e-84,5.38198765821467e-102],[1.77556608338451e-26,4.68244714950935e-15,5.72836682532587e-08,5.26618049520792e-05,0.000520971666093416,0.00358304354991811,0.0146024372316178,0.0418007880446392,0.0937576081086299,0.158070989476187,0.157995985258691,0.117472705761148,0.068309234060372,0.0258290121813925,0.00614303473251792,0.00140371367897623,0.000338744362715045,2.87186516456263e-05,4.72966243885067e-08,5.43755630957161e-16,4.12176724282202e-29,8.46961929462488e-42,3.13499804301411e-50,3.07380889542921e-63,1.97549954784854e-81],[3.73881979289422e-28,4.1820126805177e-15,3.42658639075899e-07,0.000213096739325229,4.3973222484619e-05,0.00128105280915237,0.00620857351658763,0.0236609682820968,0.0673959425125629,0.142023226278179,0.202085537513268,0.206177388365867,0.142910430049431,0.0718865969637724,0.0247087506133945,0.0047660988724505,0.0012606718949474,0.000106045234620241,1.61011212783354e-07,3.22788996902712e-14,9.2123618335293e-20,1.49536811118862e-26,2.23643511113796e-34,2.19278426565827e-47,1.4092757463806e-65],[3.96017717925257e-29,3.86645225973798e-16,2.47948115467607e-08,1.04690276048912e-05,9.33739859692154e-08,0.000133111110976272,0.001505336639134,0.0100449990636382,0.0356202331310127,0.105081641208442,0.19439947445164,0.245025075795557,0.223771691425712,0.143465906225543,0.0646419650813796,0.0193175819154857,0.00436838485724641,0.000892066964783552,3.18041827586292e-05,1.69881000386951e-07,1.59463339843398e-12,2.07291272771527e-15,3.10070286927428e-23,3.04018320512746e-36,1.95388872614587e-54],[9.27339116358244e-35,9.03850964769535e-22,5.77463739774986e-14,2.41842863581794e-11,3.88926919292566e-09,0.000100098427596149,0.00050944399843051,0.00360939499961912,0.0136893590316382,0.0543814561636372,0.125589050531115,0.214168898320003,0.252702330848491,0.212680258807483,0.12368371559332,0.0571604244308721,0.0148492657453032,0.00428301746930206,0.000533538675367115,2.75095327364781e-05,2.44919907449026e-06,5.58559793676326e-09,8.35504525271334e-17,8.19197108729206e-30,5.26488017941314e-48],[4.22630081869033e-45,4.11918272416941e-32,2.63161451387286e-24,3.34109926960455e-18,4.16151119979132e-09,3.41162097549106e-05,8.99545384914694e-05,0.000537014515485226,0.00565794795375072,0.0201673293936082,0.0650968150283119,0.13459457027411,0.216709166154952,0.233737316709245,0.179942550046156,0.0954030715862073,0.0358967231160644,0.00978785209447172,0.00200550639931911,0.000394946604594165,0.000128582191560498,2.92511773464097e-07,4.37544553451306e-15,4.29280429019131e-28,6.30606127729647e-38],[3.74346108554796e-60,3.64858019840399e-47,6.70536258912719e-37,1.26938304068891e-22,1.58065154101849e-13,1.29158993030982e-09,1.61465392359957e-06,9.08018401722032e-05,0.000992221126994708,0.00512632101729583,0.0213674249325609,0.0609771333181536,0.127073954333846,0.182795988224094,0.182286136161845,0.128019925458391,0.0639035259991266,0.0214065384771248,0.00490871234422249,0.000973054976315337,4.01022180114187e-05,5.72791275188721e-09,9.59043068097496e-17,2.97039793505775e-18,6.78914545169897e-25],[6.44423115594109e-80,1.76612292089083e-65,4.93647515191911e-46,9.37777211109363e-32,1.1698365693386e-22,1.41425275871238e-13,6.22698386517161e-07,2.62951805516803e-05,0.000279527597873221,0.00157860429446793,0.00602574560623074,0.018752616701671,0.0543021443993639,0.103895733283513,0.132977779790837,0.128374603771506,0.0753939336135714,0.0355212183761688,0.0120559569069517,0.00206194514165532,0.000441096970059341,2.73484240954569e-07,1.78469373193845e-08,6.21521805008958e-10,1.42055105743125e-16],[7.68729358424888e-104,2.44560642444526e-79,7.08775698856484e-60,1.3645720073284e-45,1.85668279000812e-30,1.24717099176323e-18,5.51514928616487e-12,3.59878422919363e-08,7.11836377827706e-06,0.000206159016212575,0.000677559737625132,0.00454096610629842,0.0184411701444843,0.0423810497069583,0.0716411419156444,0.0888352172498334,0.0766481090433616,0.0426458883542985,0.0175314894554273,0.00410447707852974,0.00104229547613349,0.000236141437094331,7.29387726175071e-05,2.52746047466519e-06,5.77674769488895e-13],[1.54348158058503e-122,6.82438055278491e-98,2.1764367130903e-78,3.10524776593914e-57,3.18217737427384e-40,2.1376226328524e-28,2.96521296109842e-21,3.35038012197265e-15,1.97523298476773e-08,0.000133046448270912,0.000137471144441657,0.000903339786529172,0.00339562223667828,0.0128631814378422,0.0284907613212608,0.0459004645516009,0.0504343969800885,0.0368454386741711,0.020295131275482,0.00834622530427719,0.0025276260264015,0.000264485978452582,7.42808296982338e-05,2.97754176124738e-07,5.54723640489585e-14],[8.37072222983159e-146,6.47496277221125e-121,6.61607426510692e-94,1.03435132478381e-71,1.05997695834429e-54,7.26629325820164e-43,3.44825400430606e-33,1.16750358152517e-20,8.87833074222978e-12,6.03705086913571e-08,3.0979146677307e-07,0.000111121155685573,0.000546159808841194,0.00234731614653475,0.00811613899679378,0.01524872508983,0.0224969610775458,0.0222048655285225,0.0165153003974538,0.00796127257796058,0.00238983759119942,0.000568475706754498,0.000111703499381798,1.37762505288305e-06,7.78323799691101e-13],[5.8174658995359e-173,1.79576621930351e-140,4.28308842051526e-113,6.69614350285288e-91,6.86338053828388e-74,5.70790798535558e-61,9.45947538521377e-43,1.05207977853767e-28,8.00037164310086e-20,5.43819520219381e-16,1.1447107087918e-12,7.6069411229749e-08,1.88824406030061e-05,0.000282589139759991,0.0013566840600888,0.0043332775154336,0.00841566316788047,0.0115467512308974,0.00989353732973917,0.00702460127430449,0.00248407446365753,0.000835282652146337,0.00019689042809369,3.98684425041365e-05,1.13279935581737e-06],[6.20932658184517e-197,2.25939039635564e-164,5.3888801122331e-137,8.42494444063686e-115,9.85285809081692e-98,9.81016236238191e-75,1.65677535778078e-55,1.84264779117487e-41,1.40112690321962e-32,9.7734482363793e-29,8.84275975640076e-21,5.42678565856387e-14,3.55725848792818e-09,2.50688031203498e-05,0.00037257061396806,0.000652072224755515,0.00220596575329706,0.00390824530472625,0.00383520710782124,0.00356746514374532,0.00198660746096792,0.000847086231459706,0.000420124430561813,0.000206161951735914,0.000139297549047621],[1.51834614100114e-225,5.52481278783358e-193,1.31772509679596e-165,2.06533081524484e-143,1.29673322046541e-116,3.33931942363595e-92,5.63955821152384e-73,6.27221302516466e-59,4.76902554018659e-50,2.05571003722159e-43,6.4564082722214e-32,9.97509200941749e-22,2.50203874102328e-11,5.38184636508631e-06,2.28640215213753e-05,5.21287951737725e-05,0.000417456509937771,0.000641775046044822,0.00116730324640233,0.00158181795727826,0.00100667520344559,0.00068510033767778,0.000277599060128235,4.61300248449355e-05,4.35426880886139e-05],[7.215756742882e-259,2.6256005832423e-226,6.26274913779807e-199,2.18370540673512e-168,8.57861654858231e-139,2.20914667283235e-114,3.73088057671666e-95,4.14938831485597e-81,3.15554639168223e-72,1.05662622923261e-59,1.43227275358807e-42,5.4931932302565e-27,1.4524397658288e-16,3.79389505892861e-11,2.20909001199427e-10,3.05546580679959e-09,4.35913186858356e-06,3.31707747133925e-05,0.000430542207310076,0.000586742208506192,0.000307810148878761,0.000253683181760847,0.000409041984564786,7.35977011753656e-06,5.00851432147429e-07],[6.66465670367594e-297,2.42507689762034e-264,4.68473831753259e-230,2.80766623244054e-195,1.10298265871409e-165,2.84037672197857e-141,4.79691936622069e-122,5.33497231006159e-108,2.51418740546229e-97,2.69430717937953e-73,1.56978231833671e-52,6.05179120216435e-37,1.68469487817489e-26,5.83767588635578e-21,4.46168128161558e-20,1.47718573329587e-15,3.03760156043252e-12,1.11175862458942e-08,4.77448480759622e-07,1.37527946349091e-05,3.27369657863926e-05,4.5069603918654e-05,2.18665863961594e-05,2.45790160940866e-07,2.16743981653795e-12],[0,1.28033478220917e-301,1.17063504610327e-261,7.01587297133337e-227,2.75616313180305e-197,7.09761020470235e-173,1.1986662995098e-153,1.33312158277165e-139,6.45923897222376e-114,5.73686958901279e-88,3.34560845825795e-67,1.30290708566137e-51,3.98283248267262e-41,1.95445068709012e-35,2.5163612212255e-34,2.13073194576148e-26,2.2103622973781e-22,1.36299881951402e-16,5.4223823620928e-13,9.64890337003241e-07,4.75103088380026e-05,1.74076683148872e-08,2.3982844765835e-05,4.12736376796499e-05,4.65593304437425e-10]],\"type\":\"surface\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\nFigure 3: Imagem 3-D Interativa de uma distribuição Multivariada Normal\n\n\n\nMetropolis e Metropolis-Hastings\nO primeiro algoritmo MCMC amplamente utilizado para gerar amostras de correntes Markov foi originário na física na década de 1950 (inclusive uma relação muito próxima com a bomba atômica no projeto Manhattan) e chama-se Metropolis (Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller, 1953) em homenagem ao primeiro autor Nicholas Metropolis (figura 4). Em síntese, o algoritmo de Metropolis é uma adaptação de um passeio aleatório (random walk) com uma regra de aceitação/rejeição para convergir à distribuição-alvo.\nO algorimo de Metropolis usa uma distribuição de propostas \\(J_t(\\theta^*)\\) (\\(J\\) quer dizer jumping distribution e \\(t\\) indica em qual estado da corrente Markov estamos) para definir próximos valores da distribuição \\(P^*(\\theta^* | \\text{data})\\). Essa distribuição deve ser simétrica:\n\\[\nJ_t (\\theta^* | \\theta^{t-1}) = J_t(\\theta^{t-1}|\\theta^*).\n\\]\nNa década de 1970, surgiu um generalização do algoritmo de Metropolis que não necessita que as distribuições de proposta sejam simétricas. A generalização foi proposta por Wilfred Keith Hastings (Hastings, 1970) (figura 4) e chama-se algoritmo de Metropolis-Hastings.\n\n\n\nFigure 4: Da esquerda para direita: Nicholas Metropolis e Wilfred Hastings – Figuras de https://www.wikipedia.org\n\n\n\nAlgoritmo de Metropolis\nA essência do algoritmo é um passeio aleatório (random walk) pelo espaço amostral dos parâmetros, onde a probabilidade da corrente Markov mudar de estado é definida como:\n\\[\nP_{\\text{mudar}} = \\min\\left({\\frac{P (\\theta_{\\text{proposto}})}{P (\\theta_{\\text{atual}})}},1\\right).\n\\]\nIsso quer dizer a corrente Markov somente mudará para um novo estado em duas condições:\nQuando a probabilidade dos parâmetros propostos pelo passeio aleatório \\(P(\\theta_{\\text{proposto}})\\) é maior que a probabilidade dos parâmetros do estado atual \\(P(\\theta_{\\text{atual}})\\), mudamos com 100% de probabilidade. Vejam que se \\(P(\\theta_{\\text{proposto}}) > P(\\theta_{\\text{atual}})\\) então a função \\(\\min\\) escolhe o valor 1 que quer dizer 100%.\nQuando a probabilidade dos parâmetros propostos pelo passeio aleatório \\(P(\\theta_{\\text{proposto}})\\) é menor que a probabilidade dos parâmetros do estado atual \\(P(\\theta_{\\text{atual}})\\), mudamos com probabilidade igual a proporção dessa diferença. Vejam que se \\(P(\\theta_{\\text{proposto}}) < P(\\theta_{\\text{atual}})\\) então a função \\(\\min\\) não escolhe o valor 1, mas sim o valor \\(\\frac{P (\\theta_{\\text{proposto}})}{P (\\theta_{\\text{atual}})}\\) que equivale a proporção da probabilidade dos parâmetros propostos pela probabilidade dos parâmetros do estado atual.\nDe qualquer maneira, a cada iteração do algoritmo de Metropolis, mesmo que a corrente muda de estado ou não, amostramos o parâmetro \\(\\theta\\) de qualquer maneira. Ou seja, se a corrente não mudar em um certo estado \\(\\theta\\) será amostrado duas vezes (ou mais caso a corrente fique estacionária no mesmo estado).\nO algoritmo de Metropolis-Hastings pode ser descrito na seguinte maneira3 (\\(\\theta\\) é o parâmetro, ou conjunto de parâmetros, de interesse e \\(y\\) são os dados):\nDefina um ponto inicial \\(\\theta^0\\) do qual \\(p(\\theta^0|y) > 0\\), ou amostre-o de uma distribuição inicial \\(p_0 (\\theta)\\). \\(p_0(\\theta)\\) pode ser uma distribuição normal ou uma distribuição prévia de \\(\\theta\\) (\\(p(\\theta)\\)).\nPara \\(t = 1, 2, \\dots\\):\nAmostra uma proposta \\(\\theta^*\\) de uma distribuição de propostas no tempo \\(t\\), \\(J_t (\\theta^* | \\theta^{t-1})\\).\nCalcule a proporção das probabilidades:\nMetropolis: \\(r = \\frac{p(\\theta^* | y)}{p(\\theta^{t-1} | y)}\\)\nMetropolis-Hastings: \\(r = \\frac{\\frac{p(\\theta^* | y)}{J_t(\\theta^*|\\theta^{t-1})}}{\\frac{p(\\theta^{t-1} | y)}{J_t(\\theta^{t-1}|\\theta^*)}}\\)\n\nDesigne:\n\\[\\theta^t =\n  \\begin{cases}\n  \\theta^* & \\text{com probabilidade $\\min(r,1)$}\\\\\n  \\theta^{t-1} & \\text{caso contrário}\n  \\end{cases}\\]\n\nLimitações do Algoritmo de Metropolis\nAs limitações do algoritmo de Metropolis-Hastings são principalmente computacionais. Com propostas geradas aleatoriamente, geralmente leva um grande número de iterações para entrar em áreas de densidade posterior mais alta (mais provável). Mesmo algoritmos de Metropolis-Hastings eficientes às vezes aceitam menos de 25% das propostas (Roberts, Gelman, & Gilks, 1997). Em situações dimensionais mais baixas, o poder computacional aumentado pode compensar a eficiência mais baixa até certo ponto. Mas em situações de modelagem de dimensões mais altas e mais complexas, computadores maiores e mais rápidos sozinhos raramente são suficientes para superar o desafio.\nMetropolis – Implementação\nNo nosso exemplo didático vamos partir do pressuposto que \\(J_t(\\theta^* | \\theta^{t-1})\\) é simétrico à \\(J_t (\\theta^* | \\theta^{t-1}) = J_t(\\theta^{t-1}|\\theta^*)\\), portanto vamos apenas demonstrar o algoritmo de Metropolis (e não o algoritmo de Metropolis-Hastings).\nO Stan (Carpenter et al., 2017) (e consequentemente seu ecossistema inteiro de pacotes) não tem implementações de outros algoritmos a não ser o HMC (Hamiltonean Monte Carlo), portanto abaixo criei um amostrador Metropolis para o nosso exemplo didático. No fim ele imprime a porcentagem total de aceitação das propostas. Aqui estamos usando a mesma distribuição de propostas para tanto \\(X\\) e \\(Y\\): uma distribuição uniforme parameterizada com um parâmetro largura width:\n\\[\nX \\sim \\text{Uniforme} \\left( X - \\frac{\\text{largura}}{2}, X + \\frac{\\text{largura}}{2} \\right) \\\\\nY \\sim \\text{Uniforme} \\left( Y - \\frac{\\text{largura}}{2}, Y + \\frac{\\text{largura}}{2} \\right)\n\\] O pacote mnormt possui algumas funcionalidades para lidar com distribuições multivariadas, a função dmnorm() em especial calcula a função densidade de probabilidade (FDP)4 de uma distribuição normal multivariada, que é usada no cálculo proporção das probabilidades \\(r\\):\n\\[\n\\begin{aligned}\nr &= \\frac{\n\\operatorname{FDP}\\left(\n\\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\nx_{\\text{proposto}} \\\\\ny_{\\text{proposto}}\n\\end{bmatrix}\n\\right)\n\\Bigg|\n\\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}, \\mathbf{\\Sigma}\n\\right)\n\\right)}\n{\n\\operatorname{FDP}\\left(\n\\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\nx_{\\text{atual}} \\\\\ny_{\\text{atual}}\n\\end{bmatrix}\n\\right)\n\\Bigg|\n\\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix}, \\mathbf{\\Sigma}\n\\right)\n\\right)}\\\\\n&=\\frac{\\operatorname{FDP}_{\\text{proposto}}}{\\operatorname{FDP}_{\\text{atual}}}\\\\\n&= \\exp\\Big(\n\\log\\left(\\operatorname{FDP}_{\\text{proposto}}\\right)\n-\n\\log\\left(\\operatorname{FDP}_{\\text{atual}}\\right)\n\\Big)\n\\end{aligned}\n\\]\n\n\nmetropolis <- function(S, half_width,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho,\n                       start_x, start_y,\n                       seed = 123) {\n   set.seed(seed)\n   Sigma <- diag(2)\n   Sigma[1, 2] <- rho\n   Sigma[2, 1] <- rho\n   draws <- matrix(nrow = S, ncol = 2)\n   x <- start_x\n   y <- start_y\n   accepted <- 0\n   draws[1, 1] <- x\n   draws[1, 2] <- y\n   for (s in 2:S) {\n      x_ <- runif(1, x - half_width, x + half_width)\n      y_ <- runif(1, y - half_width, y + half_width)\n      r <- exp(mnormt::dmnorm(c(x_, y_), mean = c(mu_X, mu_Y), varcov = Sigma, log = TRUE) -\n                        mnormt::dmnorm(c(x, y), mean = c(mu_X, mu_Y), varcov = Sigma, log = TRUE))\n      if (r > runif(1, 0, 1)) {\n        x <- x_\n        y <- y_\n        accepted <- accepted + 1\n      }\n      draws[s, 1] <- x\n      draws[s, 2] <- y\n   }\n   print(paste0(\"Taxa de aceitação \", accepted / S))\n   return(draws)\n}\n\n\n\n\n\nn_sim <- 1e4\n\n\n\nVamos executar nosso algoritmo Metropolis com 10,000 iterações.\n\n\nX_met <- metropolis(\n  S = n_sim, half_width = 2.75,\n  mu_X = 0, mu_Y = 0,\n  sigma_X = 1, sigma_Y = 1,\n  rho = r,\n  start_x = -2.5, start_y = 2.5\n)\n\n\n[1] \"Taxa de aceitação 0.2076\"\n\nhead(X_met, 7)\n\n\n      [,1] [,2]\n[1,] -2.50  2.5\n[2,] -2.50  2.5\n[3,] -2.50  2.5\n[4,] -2.50  2.5\n[5,] -2.50  2.5\n[6,] -1.52  2.9\n[7,]  0.68  1.5\n\nNa nossa primeira execução do algoritmo Metropolis temos como resultado uma matriz X_met com 10,000 linhas e 2 colunas (uma para cada valor de \\(X\\) e \\(Y\\), que passarei a chamar de \\(\\theta_1\\) e \\(\\theta_2\\), respectivamente). Vejam que a aceitação das propostas ficou em 20.8%, o esperado para algoritmos Metropolis (em torno de 20-25%) (Roberts, Gelman, & Gilks, 1997).\nPara métricas de convergência e desempenho vamos usar a função rstan::monitor() que simula um print(stanfit)5mas para matrizes.\n\n\nres <- monitor(X_met, digits_summary = 1)\n\n\nInference for the input samples (2 chains: each with iter = 10000; warmup = 5000):\n\n     Q5 Q50 Q95 Mean SD  Rhat Bulk_ESS Tail_ESS\nV1 -1.6   0 1.7    0  1     1      952      909\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS > 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat <= 1.05).\n\nneff <- res[, \"n_eff\"]\nreff <- mean(neff / (nrow(X_met))) #  9.5%\n\n\n\nVejam que o número de amostras eficientes em relação ao número total de iterações reff é 9,5% para todas as iterações incluindo warm-up.\nMetropolis – Intuição Visual\nEu acredito que uma boa intuição visual, mesmo que você não tenha entendido nenhuma fórmula matemática, é a chave para você começar a jornada de aprendizagem. Portanto fiz algumas animações com GIFs.\nA animação na figura 5 mostra as 100 primeiras simulações do algoritmo Metropolis usado para gerar X_met. Vejam que em diversas iterações a proposta é recusada e o algoritmo amostra os parâmetros \\(\\theta_1\\) e \\(\\theta_2\\) do estado anterior (que se torna o atual, pois a proposta é recusada).\nObservação: HPD é a sigla para Highest Probability Density (que é o intervalo de 90% de probabilidade da posterior).\n\n\ndf100 <- data.frame(\n    id = rep(1, 100),\n    iter = 1:100,\n    th1 = X_met[1:100, 1],\n    th2 = X_met[1:100, 2],\n    th1l = c(X_met[1, 1], X_met[1:(100 - 1), 1]),\n    th2l = c(X_met[1, 2], X_met[1:(100 - 1), 2])\n)\n\nlabs1 <- c(\"Amostras\", \"Iterações do Algoritmo\", \"90% HPD\")\n\np1 <- ggplot() +\n  geom_jitter(data = df100, width = 0.05, height = 0.05,\n             aes(th1, th2, group = id, color = \"1\"), alpha = 0.3) +\n  geom_segment(data = df100, aes(x = th1, xend = th1l, color = \"2\",\n                                 y = th2, yend = th2l)) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"3\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Metropolis\", subtitle = \"100 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"red\", \"forestgreen\", \"blue\"), labels = labs1) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA, NA), linetype = c(0, 1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\nanimate(p1 +\n  transition_reveal(along = iter) +\n  shadow_trail(0.01),\n  # animation options\n  height = 7, width = 7, units = \"in\", res = 300\n)\n\n\n\n\nFigure 5: Animação Metropolis\n\n\n\nNa figura 6 é possível ver como ficaram as primeiras 1.000 simulações excluindo 1.000 iterações iniciais como warmup.\n\n\n# Take all the 10,000 observations after warmup of 1,000\nwarm <- 1e3\ndfs <- data.frame(\n  th1 = X_met[(warm + 1):nrow(X_met), 1],\n  th2 = X_met[(warm + 1):nrow(X_met), 2]\n)\n\nlabs2 <- c(\"Amostras\", \"90% HPD\")\n\nggplot() +\n  geom_point(data = dfs[1:1000, ],\n             aes(th1, th2, color = \"1\"), alpha = 0.3) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"2\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Metropolis\", subtitle = \"1.000 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"steelblue\", \"blue\"), labels = labs2) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), alpha = c(1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nFigure 6: Primeiras 1.000 simulações Metropolis após descarte de 1.000 iterações como warmup\n\n\n\nE na figura 7 é possível ver as restantes 9.000 simulações excluindo 1.000 iterações iniciais como warmup.\n\n\n# Show all 10,000 samples\nggplot() +\n  geom_point(data = dfs,\n             aes(th1, th2, color = \"1\"), alpha = 0.3) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"2\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Metropolis\", subtitle = \"10.000 Amostragens\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"steelblue\", \"blue\"), labels = labs2) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), alpha = c(1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nFigure 7: 9.000 simulações Metropolis após descarte de 1.000 iterações como warmup\n\n\n\nGibbs\nPara contornar o problema de baixa taxa de aceitação do algoritmo de Metropolis (e Metropolis-Hastings) foi desenvolvido o algoritmo de Gibbs que não possui uma regra de aceitação/rejeição para a mudança de estado da corrente Markov. Todas as propostas são aceitas.\nO algoritmo de Gibbs teve ideia original concebida pelo físico Josiah Willard Gibbs (figura 8), em referência a uma analogia entre um algoritmo de amostragem e a física estatística (statistical physics um ramo da física que tem sua base em mecânica estatística statistical mechanics). O algoritmo foi descrito pelos irmãos Stuart e Donald Geman (figura 8) em 1984 (Geman & Geman, 1984), cerca de oito décadas após a morte de Gibbs.\n\n\n\nFigure 8: Da esquerda para direita: Josiah Gibbs,Stuart Geman e Donald Geman – Figuras de https://www.wikipedia.org\n\n\n\nO algoritmo de Gibbs é muito útil em espaços amostrais multidimensionais (no qual há bem mais que 2 parâmetros a serem amostrados da probabilidade posterior). Também é conhecido como amostragem condicional alternativa (alternating conditional sampling), pois amostramos sempre um parâmetro condicionado à probabilidade dos outros parâmetros do modelo.\nO algoritmo de Gibbs pode ser visto como um caso especial do algoritmo de Metropolis-Hastings porque todas as propostas são aceitas (Gelman, 1992).\nAlgoritmo de Gibbs\nA essência do algoritmo de Gibbs é a amostragem de parâmetros condicionada à outros parâmetros \\(P(\\theta_1 | \\theta_2, \\dots \\theta_n)\\).\nO algoritmo de Gibbs pode ser descrito na seguinte maneira6 (\\(\\theta\\) é o parâmetro, ou conjunto de parâmetros, de interesse e \\(y\\) são os dados):\nDefina \\(p(\\theta_1), p(\\theta_2), \\dots, p(\\theta_n)\\): a probabilidade prévia (prior) de cada um dos parâmetros \\(\\theta_n\\).\nAmostre um ponto inicial \\(\\theta^0_1, \\theta^0_2, \\dots, \\theta^0_n\\). Geralmente amostramos de uma distribuição normal ou de uma distribuição especificada como a distribuição prévia (prior) de \\(\\theta_n\\).\nPara \\(t = 1,2,\\dots\\):\n\\[\\begin{aligned}\n \\theta^t_1 &\\sim p(\\theta_1 | \\theta^0_2, \\dots, \\theta^0_n) \\\\\n \\theta^t_2 &\\sim p(\\theta_2 | \\theta^{t-1}_1, \\dots, \\theta^0_n) \\\\\n &\\vdots \\\\\n \\theta^t_n &\\sim p(\\theta_n | \\theta^{t-1}_1, \\dots, \\theta^{t-1}_{n-1})\n \\end{aligned}\\]\nLimitações do Algoritmo de Gibbs\nA principal limitação do algoritmo de Gibbs é com relação a amostragem condicional alternativa.\nSe compararmos com o algoritmo Metropolis (e consequentemente Metropolis-Hastings) temos propostas aleatórias de uma distribuição de propostas na qual amostramos cada parâmetro incondicionalmente à outros parâmetros. Para que as propostas nos levem a locais corretos da probabilidade posterior para amostrarmos temos uma regra de aceitação/rejeição dessas propostas, se não as amostras do algoritmo de Metropolis não se aproximariam à distribuição-alvo de interesse. As mudanças de estado da corrente Markov são então executadas multidimensionalmente7. Como você viu nas figuras 5, 6 e 7 de intuição visual do algoritmo de Metropolis, em um espaço 2-D (como é o nosso exemplo didático bivariado normal), quando há uma mudança de estado na corrente Markov, o novo local de proposta considera tanto \\(\\theta_1\\) quanto \\(\\theta_2\\), provocando uma movimentação na diagonal no espaço amostral 2-D.\nNo caso do algoritmo de Gibbs, no nosso exemplo, essa movimentação se dá apenas em um único parâmetro, pois amostramos sequencialmente e condicionalmente à outros parâmetros. Isto provoca movimentos horizontais (no caso de \\(\\theta_1\\)) e movimentos verticais (no caso de \\(\\theta_2\\)), mas nunca movimentos diagonais como o que vemos no algoritmo de Metropolis.\nGibbs – Implementação\nO Stan (Carpenter et al., 2017) (e consequentemente seu ecossistema inteiro de pacotes) não tem implementações de outros algoritmos a não ser o HMC (Hamiltonian Monte Carlo), portanto abaixo criei um amostrador Gibbs para o nosso exemplo didático.\nAqui temos algumas coisas novas comparando com a implementação do amostrador Metropolis. Primeiro para amostrar condicionalmente os parâmetros \\(P(\\theta_1 | \\theta_2)\\) e \\(P(\\theta_2 | \\theta_1)\\), precisamos criar duas variáveis novas beta (\\(\\beta\\)) e lambda (\\(\\lambda\\)). Essas variáveis representam a correlação entre \\(X\\) e \\(Y\\) (\\(\\theta_1\\) e \\(\\theta_2\\) respectivamente). E então usamos essas variáveis na amostragem de \\(\\theta_1\\) e \\(\\theta_2\\):\n\\[\n\\begin{aligned}\n\\beta &= \\rho \\cdot \\frac{\\sigma_Y}{\\sigma_X} = \\rho \\\\\n\\lambda &= \\rho \\cdot \\frac{\\sigma_X}{\\sigma_Y} = \\rho \\\\\n\\sigma_{YX} &= 1 - \\rho^2\\\\\n\\sigma_{XY} &= 1 - \\rho^2\\\\\n\\theta_1 &\\sim \\text{Normal} \\bigg( \\mu_X + \\lambda \\cdot (y^* - \\mu_Y), \\sigma_{XY} \\bigg) \\\\\n\\theta_2 &\\sim \\text{Normal} \\bigg( \\mu_y + \\beta \\cdot (x^* - \\mu_X), \\sigma_{YX} \\bigg).\n\\end{aligned}\n\\]\n\n\ngibbs <- function(S,\n                  mu_X = 0, mu_Y = 0,\n                  sigma_X = 1, sigma_Y = 1,\n                  rho,\n                  start_x, start_y,\n                  seed = 123) {\n   set.seed(seed)\n   Sigma <- diag(2)\n   Sigma[1, 2] <- rho\n   Sigma[2, 1] <- rho\n   draws <- matrix(nrow = S, ncol = 2)\n   x <- start_x\n   y <- start_y\n   beta <- rho * sigma_Y / sigma_X\n   lambda <- rho * sigma_X / sigma_Y\n   sqrt1mrho2 <- sqrt(1 - rho^2)\n   sigma_YX <- sigma_Y * sqrt1mrho2\n   sigma_XY <- sigma_X * sqrt1mrho2\n   draws[1, 1] <- x\n   draws[1, 2] <- y\n   for (s in 2:S) {\n     if (s %% 2 == 0) {\n        y <- rnorm(1, mu_Y + beta * (x - mu_X), sigma_YX)\n     }\n     else {\n        x <- rnorm(1, mu_X + lambda * (y - mu_Y), sigma_XY)\n     }\n     draws[s, 1] <- x\n     draws[s, 2] <- y\n   }\n   return(draws)\n}\n\n\n\nVamos executar nosso algoritmo Gibbs com 10,000 iterações.\n\n\nX_gibbs <- gibbs(\n  S = n_sim,\n  mu_X = 0, mu_Y = 0,\n  sigma_X = 1, sigma_Y = 1,\n  rho = r,\n  start_x = -2.5, start_y = 2.5\n)\nhead(X_gibbs, 7)\n\n\n      [,1]  [,2]\n[1,] -2.50  2.50\n[2,] -2.50 -2.34\n[3,] -2.01 -2.34\n[4,] -2.01 -0.67\n[5,] -0.49 -0.67\n[6,] -0.49 -0.32\n[7,]  0.77 -0.32\n\nNa nossa primeira execução do algoritmo Gibbs temos como resultado uma matriz X_gibbs com 10,000 linhas e 2 colunas (as mesmas condições já mostradas no exemplo anterior com algoritmo Metropolis).\n\n\nres <- monitor(X_gibbs, digits_summary = 1)\n\n\nInference for the input samples (2 chains: each with iter = 10000; warmup = 5000):\n\n     Q5 Q50 Q95 Mean SD  Rhat Bulk_ESS Tail_ESS\nV1 -1.7   0 1.6    0  1     1     1156     1972\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS > 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat <= 1.05).\n\nneff <- res[, \"n_eff\"]\nreff <- mean(neff / (nrow(X_gibbs) / 2)) #  23.2%\n\n\n\nVejam que o número de amostras eficientes em relação ao número total de iterações reff8 é 23% para todas as iterações incluindo warm-up. A eficiência do algoritmo Gibbs, no nosso exemplo didático, é o mais que dobro da eficiência do algoritmo de Metropolis (9,5% vs 23%).\nGibbs – Intuição Visual\nA animação na figura 9 mostra as 100 primeiras simulações do algoritmo Gibbs usado para gerar X_gibbs. Vejam que aqui não há movimentação na diagonal no espaço amostral devido à amostragem condicional alternativa dos parâmetros \\(\\theta_1\\) e \\(\\theta_2\\). A movimentação do algoritmo Gibbs no espaço amostral está condicionada a apenas um movimento por dimensão de parâmetro (que no nosso exemplo didático 2-D são as dimensões horizontais \\(\\theta_1\\) e verticais \\(\\theta_2\\)).\n\n\ndf100 <- data.frame(\n    id = rep(1, 100),\n    iter = 1:100,\n    th1 = X_gibbs[1:100, 1],\n    th2 = X_gibbs[1:100, 2],\n    th1l = c(X_gibbs[1, 1], X_gibbs[1:(100 - 1), 1]),\n    th2l = c(X_gibbs[1, 2], X_gibbs[1:(100 - 1), 2])\n)\n\nlabs1 <- c(\"Amostras\", \"Iterações do Algoritmo\", \"90% HPD\")\n\nind1 <- (1:50) * 2 - 1\ndf100s <- df100\ndf100s[ind1 + 1, 3:4] <- df100s[ind1, 3:4]\np1 <- ggplot() +\n  geom_point(data = df100s,\n             aes(th1, th2, group = id, color = \"1\")) +\n  geom_segment(data = df100, aes(x = th1, xend = th1l, color = \"2\",\n                                 y = th2, yend = th2l)) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"3\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Gibbs\", subtitle = \"100 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"red\", \"forestgreen\", \"blue\"), labels = labs1) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA, NA), linetype = c(0, 1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\nanimate(p1 +\n  transition_reveal(along = iter) +\n  shadow_trail(0.01),\n  # animation options\n  height = 7, width = 7, units = \"in\", res = 300\n)\n\n\n\n\nFigure 9: Animação Gibbs\n\n\n\nNa figura 10 é possível ver como ficaram as primeiras 1.000 simulações excluindo 1.000 iterações iniciais como warmup.\n\n\n# Take all the 10,000 observations after warmup of 1,000\nwarm <- 1e3\ndfs <- data.frame(\n  th1 = X_gibbs[(warm + 1):nrow(X_gibbs), 1],\n  th2 = X_gibbs[(warm + 1):nrow(X_gibbs), 2]\n)\n\nlabs2 <- c(\"Amostras\", \"90% HPD\")\n\nggplot() +\n  geom_point(data = dfs[1:1000, ],\n             aes(th1, th2, color = \"1\"), alpha = 0.3) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"2\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Gibbs\", subtitle = \"1.000 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"steelblue\", \"blue\"), labels = labs2) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), alpha = c(1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nFigure 10: Primeiras 1.000 simulações Gibbs após descarte de 1.000 iterações como warmup\n\n\n\nE na figura 11 é possível ver as restantes 9.000 simulações excluindo 1.000 iterações iniciais como warmup.\n\n\n# Show all 10,000 samples\nggplot() +\n  geom_point(data = dfs,\n             aes(th1, th2, color = \"1\"), alpha = 0.3) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"2\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Gibbs\", subtitle = \"10.000 Amostragens\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"steelblue\", \"blue\"), labels = labs2) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), alpha = c(1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nFigure 11: 9.000 simulações Gibbs após descarte de 1.000 iterações como warmup\n\n\n\nO que acontece quando rodamos correntes Markov em paralelo?\nComo as correntes Markov são independentes, podemos executá-las em paralelo no computador. A chave para isso é definir pontos iniciais diferentes de cada corrente Markov (caso você use como ponto inicial uma amostra de uma distribuição prévia dos parâmetros isto não é um problema). Vamos usar o mesmo exemplo didático de uma distribuição normal bivariada \\(X\\) e \\(Y\\) que usamos nos exemplos anteriores, mas agora com 4 correntes Markov com diferentes pontos de início.\n\n\nstarts <- list(c(-2.5, 2.5),\n               c(2.5, -2.5),\n               c(-2.5, -2.5),\n               c(2.5, 2.5)\n               )\n\n\n\nCorrentes Markov em Paralelo – Metropolis\nPara criar 4 correntes Markov com pontos diferentes de início dos parâmetros, usamos 4 vezes o amostrador Metropolis que codificamos anterior, mas agora passamos diferentes argumentos start_x e start_y, além de diferentes seed do pseudogerador de número aleatórios para termos diferentes comportamentos das correntes Markov. Todo o resultado é combinado em um dataframe com uma coluna id representando o número de cada corrente (de 1 a 4).\n\n\nlibrary(dplyr)\nn_sim <- 100\nXs_met <- bind_rows(\n  as_tibble(metropolis(S = n_sim, half_width = 2.75,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = -2.5, start_y = 2.5,\n                       seed = 1)),\n  as_tibble(metropolis(S = n_sim, half_width = 2.75,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = 2.5, start_y = -2.5,\n                       seed = 2)),\n  as_tibble(metropolis(S = n_sim, half_width = 2.75,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = -2.5, start_y = -2.5,\n                       seed = 3)),\n  as_tibble(metropolis(S = n_sim, half_width = 2.75,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = 2.5, start_y = 2.5,\n                       seed = 4)),\n  .id = \"chain\")\n\n\n[1] \"Taxa de aceitação 0.28\"\n[1] \"Taxa de aceitação 0.19\"\n[1] \"Taxa de aceitação 0.29\"\n[1] \"Taxa de aceitação 0.15\"\n\nVejam que aqui não estamos interessados em muitas iterações, portanto cada corrente Markov amostrará 100 amostras dando um total de 400 amostras.\nHouveram algumas mudanças significativas na taxa de aprovação das propostas Metropolis. Todas ficaram em torno de 15%-29%, isso é por conta do baixo número de amostras (100), caso as amostras fosse maiores veremos esses valores convergirem para próximo de 20% conforme o exemplo anterior de 10.000 amostras com uma única corrente. errores Na figura 12 é possível ver as 4 correntes Markov do algoritmo de Metropolis explorando o espaço amostral.\n\n\ndfs100_met <- Xs_met %>%\n  group_by(chain) %>%\n  transmute(\n    chain,\n    iter = 1:n_sim,\n    th1 = V1,\n    th2 = V2,\n    th1l = dplyr::lag(V1, default = V1[1]),\n    th2l = dplyr::lag(V2, default = V2[1])\n  ) %>%\n  ungroup()\np1 <- ggplot(dfs100_met) +\n  geom_jitter(width = 0.05, height = 0.05,\n              aes(th1, th2, group = chain, color = chain), alpha = 0.3) +\n  geom_segment(aes(x = th1, xend = th1l, y = th2, yend = th2l,\n                   color = chain)) +\n  #geom_point(aes(x = th1, y = th2, color = chain)) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2), color = \"black\", level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Metropolis\",subtitle = \"100 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"NULL\")\n\nanimate(p1 +\n          transition_reveal(along = iter) +\n          shadow_trail(0.01),\n        # animation options\n        height = 7, width = 7, units = \"in\", res = 300\n)\n\n\n\n\nFigure 12: Animação Metropolis – 4 correntes Markov em Paralelo\n\n\n\nCorrentes Markov em Paralelo – Gibbs\nSimilar ao exemplo das correntes Markov em paralelo com o algoritmo Metropoli, para criarmos 4 correntes Markov com pontos diferentes de início dos parâmetros, usamos 4 vezes o amostrador Gibbs que codificamos anterior, mas agora passamos diferentes argumentos start_x e start_y, além de diferentes seed do pseudogerador de número aleatórios para termos diferentes comportamentos das correntes Markov. Todo o resultado é combinado em um dataframe com uma coluna id representando o número de cada corrente (de 1 a 4).\n\n\nXs_gibbs <- bind_rows(\n  as_tibble(gibbs(S = n_sim,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = -2.5, start_y = 2.5,\n                       seed = 1)),\n  as_tibble(gibbs(S = n_sim,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = 2.5, start_y = -2.5,\n                       seed = 2)),\n  as_tibble(gibbs(S = n_sim,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = -2.5, start_y = -2.5,\n                       seed = 3)),\n  as_tibble(gibbs(S = n_sim,\n                       mu_X = 0, mu_Y = 0,\n                       sigma_X = 1, sigma_Y = 1,\n                       rho = r,\n                       start_x = 2.5, start_y = 2.5,\n                       seed = 4)),\n  .id = \"chain\")\n\n\n\nVejam que aqui não estamos interessados em muitas iterações, portanto cada corrente Markov amostrará 100 amostras dando um total de 400 amostras.\nNa figura 13 é possível ver as 4 correntes Markov do algoritmo de Gibbs explorando o espaço amostral.\n\n\ndfs100_gibbs <- Xs_gibbs %>%\n  group_by(chain) %>%\n  transmute(\n    chain,\n    iter = 1:n_sim,\n    th1 = V1,\n    th2 = V2,\n    th1l = dplyr::lag(V1, default = V1[1]),\n    th2l = dplyr::lag(V2, default = V2[1])\n  ) %>%\n  ungroup()\np1 <- ggplot(dfs100_gibbs) +\n  geom_point(aes(x = th1, y = th2, group = chain, color = chain)) +\n  geom_segment(aes(x = th1, xend = th1l, y = th2, yend = th2l,\n                   color = chain)) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2), color = \"black\", level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"Gibbs\", subtitle = \"100 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"NULL\")\n\nanimate(p1 +\n  transition_reveal(along = iter) +\n  shadow_trail(0.01),\n  # animation options\n  height = 7, width = 7, units = \"in\", res = 300\n)\n\n\n\n\nFigure 13: Animação Gibbs – 4 correntes Markov em Paralelo\n\n\n\nHamiltonian Monte Carlo – HMC\nOs problemas de baixas taxas de aceitação de propostas das técnicas de Metropolis e do desempenho baixo do algoritmo de Gibbs em problemas multidimensionais nas quais a topologia da posterior é complexa fizeram com que surgisse uma nova técnica MCMC usando dinâmica Hamiltoniana (em homenagem ao físico irlandês William Rowan Hamilton (1805-1865) figura 14). O nome em inglês dessa técnica é Hamiltonean Monte Carlo – HMC.\n\n\n\nFigure 14: William Rowan Hamilton. Figura de https://www.wikipedia.org\n\n\n\nO HMC é uma adaptação da técnica de Metropolis e emprega um esquema guiado de geração de novas proposta: isso melhora a taxa de aceitação de propostas e, consequentemente, a eficiência. Mais especificamente, o HMC usa o gradiente do log posterior para direcionar a cadeia de Markov para regiões de maior densidade posterior, onde a maioria das amostras são coletadas. Como resultado, uma corrente Markov com o algoritmo HMC bem ajustada aceitará propostas em uma taxa muito mais alta do que o algoritmo Metropolis tradicional (Roberts, Gelman, & Gilks, 1997).\nHMC foi inicialmente descrito na literatura de física Duane, Kennedy, Pendleton, & Roweth (1987) (que chamaram de “hybrid” Monte Carlo – HMC). Logo depois, HMC foi aplicado a problemas estatísticos por Radford M. Neal (1994) (que chamou de Hamiltonean Monte Carlo – HMC). Para uma discussão aprofundada (que não é o foco deste conteúdo) de HMC eu recomendo Radford M. Neal (2011) e Betancourt (2017).\nHMC usa dinâmica Hamiltoniana aplicada para partículas explorando a topologia de uma probabilidade posterior. Em algumas simulações Metropolis possui taxa de aceitação de aproximadamente 23%, enquanto HMC 65% (Gelman et al., 2013b). Além de explorar melhor a topologia da posterior e tolerar topologias complexas, HMC é muito mais eficiente que Metropolis e não sofre do problema de correlação dos parâmetros que Gibbs.\nPara cada componente \\(\\theta_j\\), o HMC adiciona uma variável de momento \\(\\phi_j\\). A densidade posterior \\(P(\\theta | y)\\) é incrementada por uma distribuição independente \\(P(\\phi)\\) dos momentos, definindo assim uma distribuição conjunta:\n\\[\nP(\\theta, \\phi | y) = P(\\phi) \\cdot P(\\theta|y)\n\\]\nO HMC usa uma distribuição de propostas que muda dependendo do estado atual na corrente Markov. O HMC descobre a direção em que a distribuição posterior aumenta, chamada de gradiente, e distorce a distribuição de propostas em direção ao gradiente. No algoritmo de Metropolis, a distribuição das propostas seria uma distribuição Normal (geralmente) centrada na posição atual, de modo que saltos acima ou abaixo da posição atual teriam a mesma probabilidade de serem propostos. Mas o HMC gera propostas de maneira bem diferente.\nVocê pode imaginar que para distribuições posteriores de alta dimensão que têm vales diagonais estreitos e até mesmo vales curvos, a dinâmica do HMC encontrará posições propostas que são muito mais promissoras do que uma distribuição de proposta simétrica ingênua, e mais promissoras do que a amostragem de Gibbs, que pode obter preso em paredes diagonais.\nA probabilidade da corrente Markov mudar de estado no algoritmo HMC é definida como:\n\\[\nP_{\\text{mudar}} = \\min\\left({\\frac{P(\\theta_{\\text{proposto}}) \\cdot P(\\phi_{\\text{proposto}})}{P(\\theta_{\\text{atual}})\\cdot P(\\phi_{\\text{atual}})}}, 1\\right),\n\\]\nonde \\(\\phi\\) é o momento.\nDistribuição dos Momentos – \\(P(\\phi)\\)\nNormalmente damos a \\(\\phi\\) uma distribuição normal multivariada com média 0 e covariância de \\(\\mathbf{M}\\), uma “matriz de massa.” Para mantêr as coisas um pouco mais simples, usamos uma matriz de massa diagonal \\(\\mathbf{M}\\). Isso faz com que os componentes de \\(\\phi\\) sejam independentes com \\(\\phi_j \\sim \\text{Normal}(0, M_{jj})\\)\nAlgoritmo de HMC\nO algoritmo de HM é bem similar ao algoritmo Metropolis mas com a inclusão do momento \\(\\phi\\) como uma maneira de quantificar o gradiente da posterior.\nAmostre \\(\\phi\\) de uma \\(\\text{Normal}(0,\\mathbf{M})\\)\nSimultaneamente amostre \\(\\theta\\) e \\(\\phi\\) com \\(L\\) leapfrog steps (não sei como traduzir isso, talvez múltiplos passos) cada um reduzido por um fator \\(\\epsilon\\). Em um leapfrog step, tanto \\(\\theta\\) quanto \\(\\phi\\) são alterados, um em relação ao outro. Repita os seguintes passos \\(L\\) vezes:\nUse o gradiente do log da posterior9 de \\(\\theta\\) para produzir um meio-salto(half-step) de \\(\\phi\\):\n\\[\\phi \\leftarrow \\phi + \\frac{1}{2} \\epsilon \\frac{d \\log p(\\theta | y)}{d \\theta}\\]\nUse o vetor de momentos \\(\\phi\\) para atualizar o vetor de parâmetros \\(\\theta\\):\n\\[\\theta \\leftarrow \\theta + \\epsilon \\mathbf{M}^{-1} \\phi\\]\nNovamente use o gradiente de \\(\\theta\\) para produzir um meio-salto(half-step) de \\(\\phi\\):\n\\[\\phi \\leftarrow \\phi + \\frac{1}{2} \\epsilon \\frac{d \\log p(\\theta | y)}{d \\theta}\\]\n\nDesigne \\(\\theta^{t-1}\\) e \\(\\phi^{t-1}\\) como os valores do vetor de parâmetros e do vetor de momentos, respectivamente, no início do processo de leapfrog (etapa 2) e \\(\\theta^*\\) e \\(\\phi^*\\) como os valores após \\(L\\) passos. Como regra de aceitação/rejeição calcule:\n\\[r = \\frac{p(\\theta^* | y) p(\\phi^*)}{p(\\theta^{t-1} | y) p(\\phi^{-1})}\\]\nDesigne:\n\\[\\theta^t\n \\begin{cases}\n \\theta^* & \\text{with probability min($r$,1)} \\\\\n \\theta^{t-1} & \\text{caso contrário}\n \\end{cases}\\]\nHMC – Implementação\nPara HMC, não vou codificar o algoritmo na mão, pois envolve derivadas que não vai ser muito eficiente no R. Para isso temos o Stan. O arquivo hmc.rds possui 1.000 amostragens com um leapfrog \\(L = 40\\), então no total são 40.001 iterações10. O exemplo é o mesmo que usamos para Metropolis e Gibbs, uma distribuição normal multivariada de \\(X\\) e \\(Y\\) (ambos com média 0 e desvio padrão 1), com correlação 0.8 (\\(\\rho = 0.8\\)):\n\\[\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix} \\sim \\text{Normal Multivariada} \\left(\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix}, \\mathbf{\\Sigma}\n\\right) \\\\\n\\mathbf{\\Sigma} \\sim\n\\begin{pmatrix}\n1 & 0.8 \\\\\n0.8 & 1\n\\end{pmatrix}\n\\]\n\n\nload(here::here(\"R\", \"hmc.RData\"))\ndf <- tibble(id = rep(1, 40000),\n                 iter = rep(1:1000, each = 40),\n                 th1 = tt[1:40000, 1],\n                 th2 = tt[1:40000, 2],\n                 th1l = c(tt[1, 1], tt[1:(40000 - 1), 1]),\n                 th2l = c(tt[1, 2], tt[1:(40000 - 1), 2]))\n\n\n\n\n\nX_hmc <- tt[seq(2, 40001, by = 40), ]\nres <- monitor(X_hmc, digits_summary = 1)\n\n\nInference for the input samples (2 chains: each with iter = 1000; warmup = 500):\n\n     Q5  Q50 Q95 Mean  SD  Rhat Bulk_ESS Tail_ESS\nV1 -1.6 -0.1 1.4 -0.1 0.9     1      604      655\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS > 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat <= 1.05).\n\nneff <- res[, \"n_eff\"]\nreff <- mean(neff / (nrow(X_hmc))) #  61%!!!\n\n\n\nNa nossa execução do algoritmo HMC temos como resultado uma matriz X_hmc com 100 linhas e 2 colunas (as mesmas condições já mostradas nos exemplos anteriores com algoritmo Metropolis e Gibbs, porém agora somente com 1.000 amostras).\nVejam que o número de amostras eficientes em relação ao número total de iterações reff é 61% para todas as iterações incluindo warm-up (no caso 1 leapfrog step \\(L = 1\\)). A eficiência do algoritmo HMC, no nosso exemplo didático, é o mais que 6x a eficiência do algoritmo de Metropolis (9,5% vs 61%) e quase 3x a eficiência do Gibbs (23% vs 61%).\nHMC – Intuição Visual\nA animação na figura 15 mostra as 50 primeiras simulações do algoritmo HMC usado para gerar X_hmc. Vejam que aqui temos em amarelo temos os leapfrog steps moldandos e distorcendo a distribuição de propostas em direção ao gradiente da posterior (conduzindo-as para áreas de maior probabilidade da posterior) e em vermelho temos as amostras após os 40 leapfrog step \\(L = 40\\) de cada interação. Notem como a exploração da posterior é muito mais eficiente e focada em locais onde realmente a distribuição de interesse possui maior probabilidade.\n\n\nlabs3 <- c(\"Amostras\", \"Iterações do Algoritmo\", \"90% HPD\", \"Leapfrog\")\n# base plot\np0 <- ggplot() +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"3\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"HMC\", subtitle = \"50 Amostragens Iniciais\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"red\", \"forestgreen\", \"blue\", \"yellow\"), labels = labs3) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA, NA, 16), linetype = c(0, 1, 1, 0)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n# first 100 iterations\ndf50 <- df %>% filter(iter <= 50)\npp <- p0 + geom_point(data = df50,\n                      aes(th1, th2, color = \"4\"), alpha = 0.3, size = 1) +\n  geom_segment(data = df50,\n               aes(x = th1, xend = th1l, color = \"2\", y = th2, yend = th2l),\n               alpha = 0.5) +\n        geom_point(data = df50[seq(1, nrow(df50), by = 40), ],\n                   aes(th1, th2, color = \"1\"), size = 2)\n\nanimate(pp +\n  transition_manual(iter, cumulative = TRUE) +\n  shadow_trail(0.05),\n  # animation options\n  height = 7, width = 7, units = \"in\", res = 300\n)\n\n\n\n\nFigure 15: Animação HMC\n\n\n\nNa figura 16 é possível ver como ficaram as 1.000 simulações excluindo o primeiro leapfrog step \\(L = 1\\) como warmup.\n\n\n# Take all the 1,000 observations after warmup of 1,000\nwarm <- 1\ndfs <- data.frame(\n  th1 = tt[(warm + 1):nrow(tt), 1],\n  th2 = tt[(warm + 1):nrow(tt), 2]\n)\n\nggplot() +\n  geom_point(data = dfs[seq(1, nrow(dfs), by = 40), ],\n             aes(th1, th2, color = \"1\"), alpha = 0.3) +\n  stat_ellipse(data = dft, aes(x = X1, y = X2, color = \"2\"), level = 0.9) +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +\n  labs(\n    title = \"HMC\", subtitle = \"1.000 Amostragens\",\n    x = expression(theta[1]), y = expression(theta[2])) +\n  scale_color_manual(values = c(\"steelblue\", \"blue\"), labels = labs2) +\n  guides(color = guide_legend(override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), alpha = c(1, 1)))) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nFigure 16: 1.000 simulações HMC após descarte da primeira iteração como warmup\n\n\n\n“Não entendi nada…”\nSe você não entendeu nada até agora, não se desespere. Pule todas as fórmulas e pegue a intuição visual dos algoritmos. Veja as limitações de Metropolis e Gibbs e compare as animações e figuras com as do HMC. A superioridade de eficiência (mais amostras com baixa autocorrelação) e eficácia (mais amostras próximas das áreas de maior probabilidade da distribuição-alvo) é autoexplicativa pelas imagens.\nAlém disso, você provavelmente nunca terá que codificar o seu algoritmo HMC (Gibbs, Metropolis ou qualquer outro MCMC) na mão. Para isso há pacotes como Stan (e seu ecossistema de pacotes: rstan, PyStan, brms, rstanarm, Stan.jl etc.). Além disso, Stan implementa um HMC modificado com uma técnica chamada No-U-Turn Sampling (NUTS) (Hoffman & Gelman, 2011) que seleciona automaticamente os valores de \\(\\epsilon\\) (fator de redução) e \\(L\\) (quantidade de leapfrog steps).11 O desempenho do HMC é altamente sensível à esses dois “hiperparâmetros” (parâmetros que devem ser especificados pelo usuário). Em particular, se \\(L\\) for muito pequeno, o algoritmo exibe comportamento indesejável de um passeio aleatório, enquanto se \\(L\\) for muito grande, o algoritmo desperdiça eficiência computacional. NUTS usa um algoritmo recursivo para construir um conjunto de pontos candidatos prováveis que abrangem uma ampla faixa da distribuição de propostas, parando automaticamente quando começa a voltar e refazer seus passos (por isso que ele não dá meia-volta – No U-turn), adicionalmente NUTS também calibra automaticamente (e de maneira simultânea) \\(L\\) e \\(\\epsilon\\).\nImplementação com o rstanarm\nComo configuração padrão, o pacote rstanarm utiliza HMC com NUTS. Além disso, os argumentos padrões do HMC no rstanarm são:\n4 correntes Markov de amostragem (chains = 4); e\n2.000 iterações de cada corrente (iter = 2000)12.\nRelembrando o exemplo da aula de regressão linear, vamos usar o mesmo dataset kidiq. São dados de uma survey de mulheres adultas norte-americanas e seus respectivos filhos. Datado de 2007 possui 434 observações e 4 variáveis:\nkid_score: QI da criança;\nmom_hs: binária (0 ou 1) se a mãe possui diploma de ensino médio;\nmom_iq: QI da mãe; e\nmom_age: idade da mãe.\nVamos estimar um modelo de regressão linear Bayesiano na qual a variável dependente é kid_score e as independentes são mom_hs e mom_iq.\nO modelo é o especificado da seguinte maneira:\n\\[\n\\begin{aligned}\n\\alpha &\\sim \\text{Normal}(\\mu_y, s_y) \\\\\n\\beta_k &\\sim \\text{Normal}(0, 2.5 \\cdot \\frac{s_y}{s_x}) \\\\\n\\sigma &\\sim \\text{Exponencial}(\\frac{1}{s_y})\\\\\ny &\\sim \\text{Normal}(\\alpha + \\beta_1 x_1 + \\dots + \\beta_K x_K, \\sigma),\n\\end{aligned}\n\\]\nonde \\(s_x = \\tt{sd(x)}\\), \\[\ns_y =\n\\begin{cases}\n\\tt{sd(y)} & \\text{se } \\tt{family = gaussian}, \\\\\n1 & \\text{caso contrário}.\n\\end{cases}\n\\] e \\[\n\\mu_y =\n\\begin{cases}\n\\tt{mean(y)} & \\text{se } \\tt{family = gaussian}, \\\\\n0 & \\text{caso contrário}.\n\\end{cases}\n\\]\nNo caso temos apenas duas variáveis independentes, então \\(K=2\\) e \\(\\beta_1 = \\tt{mom\\_hs}\\) e \\(\\beta_2 = \\tt{mom\\_iq}\\); variável dependente \\(y = \\tt{kid\\_score}\\) e o erro do modelo $= _score ~ mom_hs + mom_iq`.\n\n\noptions(mc.cores = parallel::detectCores())\noptions(Ncpus = parallel::detectCores())\n\nlibrary(rstanarm)\nmodel <- stan_glm(\n  kid_score ~ mom_hs + mom_iq,\n  data = kidiq\n)\n\n\n\nMétricas da simulação MCMC\nUm modelo estimado pelo rstanarm pode ser inspecionado em relação ao desempenho da amostragem MCMC. Ao chamarmos a função summary() no modelo estimado há uma parte chamada MCMC diagnostics.\n\n\nsummary(model)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs + mom_iq\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 434\n predictors:   3\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 26.0    6.0 18.4  26.0  33.4 \nmom_hs       5.9    2.2  3.1   5.9   8.8 \nmom_iq       0.6    0.1  0.5   0.6   0.6 \nsigma       18.2    0.6 17.4  18.1  19.0 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 86.8    1.3 85.2  86.8  88.4 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  5365 \nmom_hs        0.0  1.0  4079 \nmom_iq        0.0  1.0  4724 \nsigma         0.0  1.0  3809 \nmean_PPD      0.0  1.0  4289 \nlog-posterior 0.0  1.0  1801 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nA seção MCMC diagnostics possui três colunas de valores para cada parâmetro estimado do modelo.\nNo nosso caso, temos três parâmetros importantes:\nvalor do coeficiente da variável mom_hs;\nvalor do coeficiente da variável mom_iq; e\nvalor do erro residual do modelo linear sigma. As três métricas são:\nmcse: Monte Carlo Standard Error, o erro de mensuração da amostragem Monte Carlo do parâmetro;\nn_eff: uma aproximação crua do número de amostras efetivas amostradas pelo MCMC estimada pelo valor de Rhat; e\nRhat: uma métrica de convergência e estabilidade da corrente Markov.\nA métrica mais importante para levarmos em consideração é a Rhat que é uma métrica que mensura se as correntes Markov são estáveis e convergiram para um valor durante o progresso total das simulações. Ela é basicamente a proporção de variação ao compararmos duas metades das correntes após o descarte dos warmups. Valor de 1 implica em convergência e estabilidade. Como padrão o Rhat deve ser menor que 1.01 para que a estimação Bayesiana seja válida (S. P. Brooks & Gelman, 1998; Gelman & Rubin, 1992).\nO que fazer se não obtermos convergência?\nDependendo do modelo e dos dados é possível que HMC (mesmo com NUTS) não atinja convergência. Nesse caso, ao rodar o modelo rstanarm dará diversos avisos de divergências. Aqui vou restringir o amostrador HMC do rstanarm para apenas 200 iterações com warmup padrão de metade das iterações (100) com duas correntes em paralelo (chains). Portanto, teremos \\(2 \\cdot (200 - 100) = 200\\) amostras de MCMC. se atente as mensagens de erro.\n\n\nbad_model <- stan_glm(\n  kid_score ~ mom_hs + mom_iq,\n  data = kidiq,\n  chains = 2,\n  iter = 200\n  )\n\n\n\nEsta é uma vantagem dos pacotes do ecossistema do Stan (incluindo o rstanarm). Quando o amostrador MCMC mostra problemas ele falha de uma maneira bem escandalosa com diversos avisos. Nunca ignore esses avisos, eles estão lá para te ajudar e indicar que seu modelo possui problemas sérios que devem ser inspecionados e sanados.\nE vemos que o Rhat dos parâmetros estimados do modelo estão bem acima do limiar de \\(1.01\\).\n\n\nsummary(bad_model)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs + mom_iq\n algorithm:    sampling\n sample:       200 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 434\n predictors:   3\n\nEstimates:\n              mean   sd    10%   50%   90%\n(Intercept)  13.6   17.3 -10.5  20.2  30.7\nmom_hs        5.9    3.3   2.3   5.6  10.2\nmom_iq        0.6    0.1   0.5   0.6   0.7\nsigma        24.6    9.5  17.9  19.1  42.1\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 75.4   15.1 48.8  82.5  88.2 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse  Rhat  n_eff\n(Intercept)    12.7   1.9   2  \nmom_hs          0.2   1.0 305  \nmom_iq          0.0   1.0 245  \nsigma           6.1   2.6   2  \nmean_PPD       12.1   3.4   2  \nlog-posterior 102.6   3.2   2  \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nGráficos de Diagnósticos do MCMC\nO pacote rstanarm tem diversos gráficos interessantes de diagnósticos de convergência das simulações MCMC. Eu recomendo um guia de visualizações de modelos Bayesianos de Gabry, Simpson, Vehtari, Betancourt, & Gelman (2019).\nTraceplot\nA primeira coisa que devemos ver quando há mensagens de avisos sobre divergências ou valores indesejáveis de Rhat é inspecionar as correntes Markov para ver se elas estão estacionárias ou se divergiram durante a amostragem do MCMC. Fazemos isso com a função plot(stanreg, \"trace\"). Objetos stanreg são modelos oriundos do rstanarm. No nosso caso temos dois objetos stanreg: o model e o bad_model.\nO traceplot é a sobreposição das amostragens MCMC das correntes para cada parâmetro estimado (eixo vertical). A ideia é que as correntes se misturam e que não haja nenhuma inclinação ao longo das iterações (eixo horizontal). Isso demonstra que elas convergiram para um certo valor do parâmetro e se mantiveram nessa região durante boa parte (ou toda) da(a) amostragem das correntes Markov.\nDetalhe: o traceplot usa somente as iterações válidas, após a remoção das iterações de warmup.\nVejam na figura 17 o traceplot do modelo que as correntes Markov convergiram e ficaram estacionárias durante a amostragem do MCMC (afinal esse é o modelo model que designamos iter = 2.000 e chains = 4, ambos padrões do rstanarm). O ideal é sempre esse padrão no qual as correntes não apresentam uma tendência específica, ou seja, elas ficam geralmente “planas” na horizontal e não há uma grande variação de valores no eixo vertical (valor dos parâmetros). Esse padrão é muito parecido com “taturana.”\n\n\nplot(model, \"trace\")\n\n\n\n\nFigure 17: Traceplot do model\n\n\n\nNa figura 18 temos o traceplot do modelo que as correntes Markov não convergiram, o bad_model (designamos iter = 200 e chains = 2). Aqui você vê que se aumentarmos o período de warmup e o número de iterações, provavelmente as correntes Markov convergiriam e ficariam estacionárias na região de maior probabilidade da posterior (e, consequentemente, dos parâmetros de interesse).\n\n\nplot(bad_model, \"trace\")\n\n\n\n\nFigure 18: Traceplot do bad_model\n\n\n\nPosterior Predictive Check\nUm bom gráfico de diagnóstico é o posterior predictive check (PPC) que compara o histograma da variável dependente \\(y\\) contra o histograma variáveis dependentes simuladas pelo modelo \\(y_{\\text{rep}}\\) após a estimação dos parâmetros. A ideia é que os histogramas reais e simulados se misturem e não haja divergências. Fazemos isso com a função pp_check(stanreg).\nVejam na figura 19 o PPC do modelo que as correntes Markov convergiram e ficaram estacionárias durante a amostragem do MCMC (model). Podemos ver que as simulações \\(y_{\\text{rep}}\\) realmente capturaram a natureza da variável dependente \\(y\\).\n\n\npp_check(model)\n\n\n\n\nFigure 19: Posterior Preditive Check do model\n\n\n\nJá na na figura 20 temos o PPC do modelo que as correntes Markov não convergiram, o bad_model. Aqui vemos que as simulações \\(y_{\\text{rep}}\\) falharam em capturar a natureza da variável dependente \\(y\\). O PPC do bad_model também indica que se mantivéssemos um periodo maior de warmup e mais iterações das correntes Markov, provavelmente conseguiríamos ter um modelo que representasse muito bem o processo de geração de dados da nossa variável dependente \\(y\\).\n\n\npp_check(bad_model)\n\n\n\n\nFigure 20: Posterior Preditive Check do bad_model\n\n\n\nO quê fazer para convergir suas correntes Markov\nPrimeiro: Antes de fazer ajustes finos no número de correntes chains ou no número de iterações iter (entre outros …) saiba que o amostrador HMC-NUTS do Stan e seu ecossistema de pacotes (rstanarm incluso) é muito eficiente e eficaz em explorar as mais diversas complexas e “malucas” topologias de distribuições-alvo posterior. Os argumentos padrões (iter = 2000, chains = 4 e warmup = floor(iter/2)) funcionam perfeitamente para 99% dos casos (mesmo em modelos complexos). Dito isto, na maioria das vezes quando você possui problemas de amostragem e computacionais no seu modelo Bayesiano, o problema está na especificação do modelo e não no algoritmo de amostragem MCMC. Esta frase foi dita por Andrew Gelman (o “pai” do Stan) e é conhecido como o Folk Theorem (Gelman, 2008): “When you have computational problems, often there’s a problem with your model”.\nSe o seu modelo Bayesiano está com problemas de convergência há alguns passos que podem ser tentados13. Aqui listados do mais simples para o mais complexo:\nAumentar o número de iterações e correntes: primeira opção é aumentar o número de iterações do MCMC com o argumento iter = XXX e também é possível aumentar o número de correntes com o argumento chains = X. Lembrando que o padrão é iter = 2000 e chains = 4.\nAlterar a rotina de adaptação do HMC: a segunda opção é fazer com que o algoritmo de amostragem HMC fique mais conservador (com proposições de pulos menores). Isto pode ser alterado com o argumento adapt_delta da lista de opções control. control=list(adapt_delta=0.9). O padrão do adapt_delta é control=list(adapt_delta=0.8). Então qualquer valor entre \\(0.8\\) e \\(1.0\\) o torna mais conservador.\nReparametrização do Modelo: a terceira opção é reparametrizar o modelo. Há duas maneiras de parametrizar o modelo: a primeira com parametrização centrada (centered parameterization) e a segunda com parametrização não-centrada (non-centered parameterization). Não são assuntos que vamos cobrir aqui no curso. Recomendo o material de um dos desenvolvedores da linguagem Stan, Michael Betancourt.\nColetar mais dados: às vezes o modelo é complexo demais e precisamos de uma amostragem maior para conseguirmos estimativas estáveis.\nRepensar o modelo: falha de convergência quando temos uma amostragem adequada geralmente é por conta de uma especificação de priors e verossimilhança que não são compatíveis com os dados. Nesse caso, é preciso repensar o processo generativo de dados no qual os pressupostos do modelo estão ancorados.\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] rstanarm_2.21.1      Rcpp_1.0.6           dplyr_1.0.4         \n [4] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n [7] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[10] gganimate_1.0.7      plotly_4.9.3         ggplot2_3.3.3       \n\nloaded via a namespace (and not attached):\n  [1] minqa_1.2.4        colorspace_2.0-0   ellipsis_0.3.1    \n  [4] ggridges_0.5.3     rsconnect_0.8.16   rprojroot_2.0.2   \n  [7] markdown_1.1       base64enc_0.1-3    farver_2.0.3      \n [10] DT_0.17            fansi_0.4.2        splines_4.0.3     \n [13] codetools_0.2-18   downlit_0.2.1      mnormt_2.0.2      \n [16] knitr_1.31         shinythemes_1.2.0  polyclip_1.10-0   \n [19] bayesplot_1.8.0    jsonlite_1.7.2     nloptr_1.2.2.2    \n [22] png_0.1-7          shiny_1.6.0        compiler_4.0.3    \n [25] httr_1.4.2         Matrix_1.3-2       assertthat_0.2.1  \n [28] fastmap_1.1.0      lazyeval_0.2.2     cli_2.3.0         \n [31] later_1.1.0.1      tweenr_1.0.1       htmltools_0.5.1.1 \n [34] prettyunits_1.1.1  tools_4.0.3        igraph_1.2.6      \n [37] gtable_0.3.0       glue_1.4.2         reshape2_1.4.4    \n [40] V8_3.4.0           vctrs_0.3.6        nlme_3.1-152      \n [43] crosstalk_1.1.1    xfun_0.21          stringr_1.4.0     \n [46] ps_1.5.0           lme4_1.1-26        mime_0.9          \n [49] miniUI_0.1.1.1     lifecycle_0.2.0    gtools_3.8.2      \n [52] statmod_1.4.35     zoo_1.8-8          scales_1.1.1      \n [55] colourpicker_1.1.0 hms_1.0.0          promises_1.2.0.1  \n [58] parallel_4.0.3     inline_0.3.17      shinystan_2.5.0   \n [61] RColorBrewer_1.1-2 yaml_2.2.1         curl_4.3          \n [64] gridExtra_2.3      loo_2.4.1          distill_1.2       \n [67] stringi_1.5.3      highr_0.8          dygraphs_1.1.1.6  \n [70] gifski_0.8.6       boot_1.3-27        pkgbuild_1.2.0    \n [73] rlang_0.4.10       pkgconfig_2.0.3    matrixStats_0.58.0\n [76] evaluate_0.14      lattice_0.20-41    purrr_0.3.4       \n [79] rstantools_2.1.1   htmlwidgets_1.5.3  labeling_0.4.2    \n [82] processx_3.4.5     tidyselect_1.1.0   here_1.0.1        \n [85] plyr_1.8.6         magrittr_2.0.1     R6_2.5.0          \n [88] magick_2.6.0       generics_0.1.0     DBI_1.1.1         \n [91] pillar_1.4.7       withr_2.4.1        xts_0.12.1        \n [94] survival_3.2-7     tibble_3.0.6       crayon_1.4.1      \n [97] tmvnsim_1.0-2      rmarkdown_2.6      jpeg_0.1-8.1      \n[100] progress_1.2.2     grid_4.0.3         isoband_0.2.3     \n[103] data.table_1.13.6  callr_3.5.1        threejs_0.3.3     \n[106] digest_0.6.27      xtable_1.8-4       tidyr_1.1.2       \n[109] httpuv_1.5.5       RcppParallel_5.0.2 stats4_4.0.3      \n[112] munsell_0.5.0      viridisLite_0.3.0  shinyjs_2.0.0     \n\n\n\n\nBetancourt, M. (2017, January 9). A Conceptual Introduction to Hamiltonian Monte Carlo. Retrieved November 6, 2019, from http://arxiv.org/abs/1701.02434\n\n\nBrooks, S., Gelman, A., Jones, G., & Meng, X.-L. (2011). Handbook of Markov Chain Monte Carlo. Retrieved from http://books.google.com?id=qfRsAIKZ4rIC\n\n\nBrooks, S. P., & Gelman, A. (1998). General Methods for Monitoring Convergence of Iterative Simulations. Journal of Computational and Graphical Statistics, 7(4), 434–455. https://doi.org/10.1080/10618600.1998.10474787\n\n\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., … Riddell, A. (2017). Stan : A Probabilistic Programming Language. Journal of Statistical Software, 76(1). https://doi.org/10.18637/jss.v076.i01\n\n\nCasella, G., & George, E. I. (1992). Explaining the gibbs sampler. The American Statistician, 46(3), 167–174. https://doi.org/10.1080/00031305.1992.10475878\n\n\nChib, S., & Greenberg, E. (1995). Understanding the Metropolis-Hastings Algorithm. The American Statistician, 49(4), 327–335. https://doi.org/10.1080/00031305.1995.10476177\n\n\nDuane, S., Kennedy, A. D., Pendleton, B. J., & Roweth, D. (1987). Hybrid Monte Carlo. Physics Letters B, 195(2), 216–222. https://doi.org/10.1016/0370-2693(87)91197-X\n\n\nGabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378\n\n\nGelman, A. (1992). Iterative and Non-Iterative Simulation Algorithms. Computing Science and Statistics (Interface Proceedings), 24, 457–511. PROCEEDINGS PUBLISHED BY VARIOUS PUBLISHERS.\n\n\nGelman, A. (2008). The folk theorem of statistical computing. Retrieved from https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013a). Basics of Markov Chain Simulation. In Bayesian Data Analysis. Chapman and Hall/CRC.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013b). Bayesian Data Analysis. Chapman and Hall/CRC.\n\n\nGelman, A., & Rubin, D. B. (1992). Inference from Iterative Simulation Using Multiple Sequences. Statistical Science, 7(4), 457–472. https://doi.org/10.1214/ss/1177011136\n\n\nGeman, S., & Geman, D. (1984). Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-6(6), 721–741. https://doi.org/10.1109/TPAMI.1984.4767596\n\n\nHastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1), 97–109. https://doi.org/10.1093/biomet/57.1.97\n\n\nHoffman, M. D., & Gelman, A. (2011). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1), 1593–1623. Retrieved from http://arxiv.org/abs/1111.4246\n\n\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953). Equation of State Calculations by Fast Computing Machines. The Journal of Chemical Physics, 21(6), 1087–1092. https://doi.org/10.1063/1.1699114\n\n\nNeal, Radford M. (1994). An Improved Acceptance Procedure for the Hybrid Monte Carlo Algorithm. Journal of Computational Physics, 111(1), 194–203. https://doi.org/10.1006/jcph.1994.1054\n\n\nNeal, Radford M. (2011). MCMC using Hamiltonian dynamics. In S. Brooks, A. Gelman, G. L. Jones, & X.-L. Meng (Eds.), Handbook of markov chain monte carlo.\n\n\nRoberts, G. O., Gelman, A., & Gilks, W. R. (1997). Weak convergence and optimal scaling of random walk Metropolis algorithms. Annals of Applied Probability, 7(1), 110–120. https://doi.org/10.1214/aoap/1034625254\n\n\no símbolo \\(\\propto\\) (\\propto) deve ser lido como “proporcional à.”↩︎\nAlgumas referências chamam esse processo de burnin.↩︎\nCaso queira uma melhor explanação do algoritmo de Metropolis e Metropolis-Hastings sugiro ver Chib & Greenberg (1995)↩︎\ndo ingles probability density function (PDF).↩︎\nobjetos stanfit são objetos resultantes de modelos rstan ou rstanarm.↩︎\nCaso queira uma melhor explanação do algoritmo de Gibbs sugiro ver Casella & George (1992).↩︎\nisto ficará claro nas imagens e animações.↩︎\nVejam que aqui eu propositalmente dividi a neff por nrow(X_gibbs) / 2 (metade do número de iterações). Isso foi necessário, pois da maneira que eu codifiquei o algoritmo Gibbs ele amostra um parâmetro a cada interação e geralmente não se implementa um amostrador Gibbs dessa maneira (amostra-se todos os parâmetros por iteração). Eu fiz de propósito pois quero gerar nos GIFs animados na figura 9 a real trajetória do amostrador Gibbs no espaço amostral (vertical e horizontal, e não diagonal).↩︎\npor questões de transbordamento numérico (numeric overflow) sempre trabalhamos com log de probabilidades.↩︎\n1.000 * 40 = 40.000. Esse 1 a mais é que usei a primeira iteração com Leapfrog \\(L = 1\\) como warmup.↩︎\nalém disso, todos os pacotes do ecossistema Stan aplicam uma decomposição QR na matriz \\(X\\) de dados, criando uma base ortogonal (não correlacionada) para amostragem. Isso faz com a distribuição-alvo (posterior) fique muito mais amigável do ponto de vista topológico/geométrico para o amostrador MCMC explorá-la de maneira mais eficiente e eficaz.↩︎\nSendo que, por padrão, Stan e rstanarm descartam a primeira metade (1.000) das iterações como aquecimento (warmup = floor(iter/2)).↩︎\nalém disso, vale a pena ativar a decomposição QR na matriz \\(X\\) de dados, criando uma base ortogonal (não correlacionada) para amostragem. Isso faz com a distribuição-alvo (posterior) fique muito mais amigável do ponto de vista topológico/geométrico para o amostrador MCMC explorá-la de maneira mais eficiente e eficaz. Só você especificar o argumento QR = TRUE dentro da funções do rstanarm, exemplo stan_glm(..., QR = TRUE).↩︎\n",
      "last_modified": "2021-02-12T09:06:47-03:00"
    },
    {
      "path": "6-Regressao_Binomial.html",
      "title": "Regressão Binomial",
      "description": "Modelos Lineares Generalizados -- Binomial",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nComparativo com a Regressão Linear\nExemplo\n\nRegressão logística com o rstanarm\nInterpretação dos coeficientes\nPriors\nAtividade Prática\nAmbiente\n\n\nSaindo do universo dos modelos lineares, começamos a nos aventurar nos modelos linares generalizados (generalized linear models - GLM). O primeiro deles é a regressão logística (também chamada de regressão binomial).\nUma regressão logística se comporta exatamente como um modelo linear: faz uma predição simplesmente computando uma soma ponderada das variáveis independentes, mais uma constante. Porém ao invés de retornar um valor contínuo, como a regressão linear, retorna a função logística desse valor.\n\\[\\operatorname{Logística}(x) = \\frac{1}{1 + e^{(-x)}}\\]\nUsamos regressão logística quando a nossa variável dependente é binária. Ela possui apenas dois valores distintos, geralmente codificados como \\(0\\) ou \\(1\\).\n\n\nx <- seq(-10, 10, length.out = 100)\nsig <- 1 / (1 + exp(-x))\nplot(x, sig, type = \"l\", lwd = 2, ylab = \"Logística(x)\")\n\n\n\n\nComparativo com a Regressão Linear\n\\[ \\operatorname{Linear} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\\]\n\\(\\operatorname{Linear}\\) - regressão linear\n\\(\\theta\\) - parâmetro do modelo\n\\(n\\) - número de atributos (features)\n\\(x_i\\) - o valor do inésimo atributo (feature)\n\\(\\hat{p} = \\sigma(\\operatorname{Linear}) = \\frac{1}{1 + e^{-\\operatorname{Linear}}}\\)\n\\(\\hat{p}\\) - probabilidade prevista da observação ser 1\n\\(\\hat{y}=\\left\\{\\begin{array}{ll} 0 & \\text { se } \\hat{p} < 0.5 \\\\ 1 & \\text { se } \\hat{p} \\geq 0.5 \\end{array}\\right.\\)\nExemplo\n\\[\\mathrm{Previsão~de~Morte} = \\sigma \\big(-10 + 10\\times \\mathrm{cancer} + 12 \\times \\mathrm{diabetes} + 8 \\times \\mathrm{obesidade} \\big)\\]\nRegressão logística com o rstanarm\nO rstanarm pode tolerar qualquer modelo linear generalizado e regressão logística não é uma exceção. Para rodar um modelo binomial no rstanarm é preciso simplesmente alterar o argumento family da função stan_glm.\nPara exemplo, usaremos um dataset chamado wells do pacote rstanarm. É uma survey com 3200 residentes de uma pequena área de Bangladesh na qual os lençóis freáticos estão contaminados por arsênico. Respondentes com altos níveis de arsênico nos seus poços foram encorajados para trocar a sua fonte de água para uma níveis seguros de arsênico.\nPossui as seguintes variáveis:\nswitch: dependente indicando se o respondente trocou ou não de poço\narsenic: nível de arsênico do poço do respondente\ndist: distância em metros da casa do respondente até o poço seguro mais próximo\nassociation: dummy se os membros da casa do respondente fazem parte de alguma organização da comunidade\neduc: quantidade de anos de educação que o chefe da família respondente possui\n\n\noptions(mc.cores = parallel::detectCores())\noptions(Ncpus = parallel::detectCores())\n\nlibrary(rstanarm)\ndata(wells)\n\nmodel_binomial <- stan_glm(\n  switch ~ dist + arsenic + assoc + educ,\n  data = wells,\n  family = binomial()\n    )\n\n\n\n\n\nsummary(model_binomial)\n\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      switch ~ dist + arsenic + assoc + educ\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 3020\n predictors:   5\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) -0.2    0.1 -0.3  -0.2   0.0 \ndist         0.0    0.0  0.0   0.0   0.0 \narsenic      0.5    0.0  0.4   0.5   0.5 \nassoc       -0.1    0.1 -0.2  -0.1   0.0 \neduc         0.0    0.0  0.0   0.0   0.1 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.6    0.0  0.6   0.6   0.6  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  5643 \ndist          0.0  1.0  4302 \narsenic       0.0  1.0  4433 \nassoc         0.0  1.0  4583 \neduc          0.0  1.0  4852 \nmean_PPD      0.0  1.0  4369 \nlog-posterior 0.0  1.0  1947 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nInterpretação dos coeficientes\nAo vermos a fórmula de regressão binomial vemos que para analisarmos o efeito de um preditor na variável dependente temos que calcular o valor logístico dos coeficientes do preditor. E interpretamos como chances (odds ratio) na qual 1 é neutro e qualquer valor abaixo de 1 tende a respostas codificadas como 0 e qualquer valor acima de 1 tende a respostas codificadas como 1.\n\\[\\text{odds ratio} = e^{(x)}\\]\n\n\ncoeff <- exp(model_binomial$coefficients)\ncoeff\n\n\n(Intercept)        dist     arsenic       assoc        educ \n       0.85        0.99        1.60        0.88        1.04 \n\n(Intercept): a chance basal de respondentes mudarem de poço (15% de não mudarem)\ndist: a cada metro de distância diminui a chance de troca de poço em 1%\narsenic: a cada incremento do nível de arsênico aumenta a chance de troca de poço em 60%\nassoc: residências com membros que fazem parte de alguma organização da comunidade diminui a chance de troca de poço em 12%\neduc: a cada incremento dos anos de estudo aumenta a chance de troca de poço em 4%\nPriors\nrstanarm possui as seguintes configurações como padrão de priors para regressão binomial:\nConstante (Intercept): centralizada com média \\(\\mu = 0\\) e desvio padrão de \\(2.5 \\sigma_y\\) - prior_intercept = normal(0, 2.5 * sd_y)\nCoeficientes: para cada coeficiente média \\(\\mu = 0\\) and standard deviation of \\(2.5\\times\\frac{1}{\\sigma_{x_k}}\\) - prior = normal(0, 2.5 * 1/sd_xk)\nErro residual (prior_aux): uma distribuição exponencial com taxa \\(\\frac{1}{\\sigma_y}\\): prior_aux = exponential(1/sd_y)\nAtividade Prática\nDois datasets estão disponíveis na pasta datasets/:\nTitanic Survival: datasets/Titanic_Survival.csv\nIBM HR Analytics Employee Attrition & Performance: datasets/IBM_HR_Attrition.csv\n\n\n###\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:37:35-03:00"
    },
    {
      "path": "7-Regressao_Poisson.html",
      "title": "Regressão de Poisson",
      "description": "Modelos Lineares Generalizados -- Poisson",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nRegressão de Poisson com o rstanarm\nInterpretação dos coeficientes\nPriors\nAtividade Prática\nAmbiente\n\n\nSaindo do universo dos modelos lineares, começamos a nos aventurar nos modelos linares generalizados (generalized linear models - GLM). O segundo deles é a regressão de Poisson.\nUma regressão de Poisson se comporta exatamente como um modelo linear: faz uma predição simplesmente computando uma soma ponderada das variáveis independentes, mais uma constante. Porém ao invés de retornar um valor contínuo, como a regressão linear, retorna o logarítmo natural desse valor.\n\\[\\log(y)= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\\] que é o mesmo que\n\\[y = e^{(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n)}\\] Regressão de Poisson é usada quando a nossa variável dependente só pode tomar valores positivos e discretos (número inteiros), geralmente em contextos de dados de contagem.\n\n\nx <- seq(-5, 5, length.out = 100)\nplot(x, exp(x), type = \"l\", lwd = 2, ylab = \"Exponencial(x)\")\n\n\n\n\nRegressão de Poisson com o rstanarm\nO rstanarm pode tolerar qualquer modelo linear generalizado e regressão de Poisson não é uma exceção. Para rodar um modelo de Poisson no rstanarm é preciso simplesmente alterar o argumento family da função stan_glm.\nPara exemplo, usaremos um dataset chamado roaches do pacote rstanarm. É uma base de dados com 262 observações sobre a eficácia de um sistema de controle de pragas em reduzir o número de baratas (roaches) em apartamentos urbanos.\nPossui as seguintes variáveis:\ny: variável dependente - número de baratas mortas\nroach1: número de baratas antes da dedetização\ntreatment: dummy para indicar se o apartamento foi dedetizado ou não\nsenior: dummy para indicar se há apenas idosos no apartamento\nexposure2: número de dias que as armadilhas de baratas foram usadas\n\n\noptions(mc.cores = parallel::detectCores())\noptions(Ncpus = parallel::detectCores())\n\nlibrary(rstanarm)\ndata(roaches)\n\nmodel_poisson <- stan_glm(\n  y ~ roach1 + treatment + senior,\n  data = roaches,\n  family = poisson()\n    )\n\n\n\n\n\nsummary(model_poisson)\n\n\n\nModel Info:\n function:     stan_glm\n family:       poisson [log]\n formula:      y ~ roach1 + treatment + senior\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 262\n predictors:   4\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept)  3.1    0.0  3.1   3.1   3.2 \nroach1       0.0    0.0  0.0   0.0   0.0 \ntreatment   -0.5    0.0 -0.5  -0.5  -0.5 \nsenior      -0.4    0.0 -0.4  -0.4  -0.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 25.6    0.4 25.1  25.6  26.2 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3726 \nroach1        0.0  1.0  3599 \ntreatment     0.0  1.0  3655 \nsenior        0.0  1.0  2811 \nmean_PPD      0.0  1.0  4134 \nlog-posterior 0.0  1.0  1811 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nInterpretação dos coeficientes\nAo vermos a fórmula de regressão de Poisson vemos que para analisarmos o efeito de um preditor na variável dependente temos que calcular o valor \\(e\\) elevado ao coeficiente do preditor\n\\[y = e^{(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n)}\\]\n\n\ncoeff <- exp(model_poisson$coefficients)\ncoeff\n\n\n(Intercept)      roach1   treatment      senior \n      23.02        1.01        0.60        0.69 \n\n(Intercept): a taxa basal de exterminação das baratas \\(y\\)\nroach1: a cada uma barata antes da exterminação há um aumento de 1.01 barata exterminada a mais\ntreatment: se o apartamento foi dedetizado há um aumento de 0.6 barata exterminada a mais\nsenior: se o apartamento possui somente idoso há um aumento de 0.69 barata exterminada a mais\nPriors\nrstanarm possui as seguintes configurações como padrão de priors para regressão de Poisson:\nConstante (Intercept): centralizada com média \\(\\mu = 0\\) e desvio padrão de \\(2.5 \\sigma_y\\) - prior_intercept = normal(0, 2.5 * sd_y)\nCoeficientes: para cada coeficiente média \\(\\mu = 0\\) and standard deviation of \\(2.5\\times\\frac{1}{\\sigma_{x_k}}\\) - prior = normal(0, 2.5 * 1/sd_xk)\nErro residual (prior_aux): uma distribuição exponencial com taxa \\(\\frac{1}{\\sigma_y}\\): prior_aux = exponential(1/sd_y)\nAtividade Prática\nUm datasets está disponível na pasta datasets/:\nNew York City - East River Bicycle Crossings: datasets/NYC_bicycle.csv\n\n\n###\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:37:50-03:00"
    },
    {
      "path": "8-Regressao_Robusta.html",
      "title": "Regressão Robusta",
      "description": "Modelos Lineares Generalizados -- $t$ de Student",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nComparativo Normal vs Student\nModelos Lineares Robustos com o pacote brms\nExemplo com os dados de Prestígio de Duncan (1961)\nPriors do brms\n\nAtividade Prática\nAmbiente\n\n\nLembrando da curva normal gaussiana que possui um formato de sino. Ela não é muito alongada nas “pontas.” Ou seja, as observações não fogem muito da média. Quando usamos essa distribuição como verossimilhança na inferência modelos Bayesianos, forçamos a que todas as estimativas sejam condicionadas à uma distribuição normal da variável dependente. Se nos dados houverem muitas observações com valores discrepantes (bem diferentes da média - outliers), isso faz com que as estimativas dos coeficientes das variáveis independentes fiquem instáveis. Isso ocorre porquê a distribuição normal não consegue contemplar observações muito divergentes da média sem mudar a média de local.\n\n\nx <- seq(-4, 4, length = 100)\nplot(x, dnorm(x),\n     type = \"l\",\n     col = \"red\",\n     lwd = 3,\n     xlab = \"valor de x\",\n     ylab = \"Densidade\",\n     main = \"Distribuição Normal\",\n     sub = \"Média 0 e Desvio Padrão 1\",\n     xlim = c(-4, 4),\n     ylim = c(0, 0.4))\n\n\n\n\nEntão precisamos de uma distribuição mais “maleável” como verossimilhança. Precisamos de uma distribuição que seja mais robusta à observações discrepantes (outliers). Precisamos de uma distribuição similar à Normal mas que possua caudas mais longas para justamente contemplar observações muito longe da média sem ter que mudar a média de local. Para isso temos a distribuição t de Student. Lembrando o formato dela:\n\n\nx <- seq(-4, 4, length = 100)\nplot(x, dt(x, 2),\n     type = \"l\",\n     col = \"blue\",\n     lwd = 3,\n     xlab = \"valor de x\",\n     ylab = \"Densidade\",\n     main = \"Distribuição t de Student\",\n     sub = \"Média 0 e Graus de Liberdade 2\",\n     xlim = c(-4, 4),\n     ylim = c(0, 0.4))\n\n\n\n\nComparativo Normal vs Student\nReparem nas caudas:\n\n\nplot(NA, xlab = \"valor de x\",\n  ylab = \"Densidade\",\n  main = \"Comparativo de Distribuições\",\n  sub = \"Normal vs t de Student\",\n  xlim = c(-4, 4),\n  ylim = c(0, 0.4))\nlines(x, dnorm(x), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 2), lwd = 2, col = \"blue\")\nlegend(\"topright\", legend = c(\"Normal\", \"Student\"),\n       col = c(\"red\", \"blue\"), title = \"Distribuições\", lty = 1)\n\n\n\n\nModelos Lineares Robustos com o pacote brms\nO rstanarm não possui a possibilidade de usar distribuições t de Student como verossimilhança do modelo Bayesiano. Para usarmos distribuições t de Student, precisamos do pacote brms. O brms usa a mesma síntaxe que o rstanarm e a única diferença é que o brms não possui os modelos pré-compilados então os modelos devem ser todos compilados antes de serem rodados. A diferença prática é que você irá esperar alguns instantes antes do R começar a simular MCMC e amostrar do modelo.\nA função que usa-se para designar modelos lineares no brms é a brm():\n\n\nbrm(y ~ x1 + x2 + x3,\n    data = df,\n    family = student)\n\n\n\nExemplo com os dados de Prestígio de Duncan (1961)\nPara exemplicar regressão robusta vamos usar um dataset que tem muitas observações discrepantes (outliers) chamado Duncan. Ele possui 45 observações sobre ocupações nos EUA e 4 variáveis:\ntype: Tipo de ocupação. Uma variável qualitativa:\nprof - profissional ou de gestão\nwc - white-collar (colarinho branco)\nbc - blue-collar (colarinho azul)\n\nincome: Porcentagem de pessoas da ocupação que ganham acima $ 3.500 por ano em 1950 (mais ou menos $36.000 em 2017);\neducation: Porcentagem de pessoas da ocupação que possuem diploma de ensino médio em 1949 (que, sendo cínicos, podemos dizer que é de certa maneira equivalente com diploma de Doutorado em 2017); e\nprestige:Porcentagem de respondentes na pesquisa que classificam a sua ocupação como no mínimo “boa” em respeito à prestígio.\n\n\nduncan <- read.csv2(\"datasets/Duncan.csv\", row.names = 1, stringsAsFactors = TRUE)\n\nhist(duncan$prestige,\n     main = \"Histograma do Prestígio\",\n     xlab = \"Prestígio\",\n     ylab = \"Frequência\")\n\n\n\n\nPrimeiro modelo: Regressão Linear\nVamos estimar primeiramente uma regressão linear usando a distribuição Normal como verossimilhança:\n\n\nlibrary(rstanarm)\nmodel_1 <- stan_glm(\n  prestige ~ income + education,\n  data = duncan,\n  family = gaussian\n)\n\n\n\nE na sequência o sumário das estimativas do modelo, assim como os diagnósticos da MCMC:\n\n\nsummary(model_1)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      prestige ~ income + education\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 45\n predictors:   3\n\nEstimates:\n              mean   sd    10%   50%   90%\n(Intercept)  -6.1    4.3 -11.7  -6.1  -0.6\nincome        0.6    0.1   0.4   0.6   0.8\neducation     0.5    0.1   0.4   0.5   0.7\nsigma        13.7    1.5  11.8  13.6  15.7\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 47.6    2.9 43.9  47.6  51.3 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  5171 \nincome        0.0  1.0  1988 \neducation     0.0  1.0  2026 \nsigma         0.0  1.0  2501 \nmean_PPD      0.0  1.0  3393 \nlog-posterior 0.0  1.0  1338 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nAparentemente parece que o modelo possui boas métricas mas quando olhamos o posterior predictive check, vemos uma bagunça:\n\n\npp_check(model_1, nsamples = 45)\n\n\n\n\nSegundo modelo: Regressão Robusta\nPara rodar um modelo Bayesiano que usa como verossimilhança a distribuição t de Student é somente usar a mesma síntaxe que o stan_glm mas colocando argumento family = student:\n\n\nlibrary(brms)\nmodel_2 <- brm(\n  prestige ~ income + education,\n  data = duncan,\n  family = student)\n\n\n\nE na sequência o sumário das estimativas do modelo, assim como os diagnósticos da MCMC. Vemos que as estimativas não alteraram muito. Além disso temos um novo parâmetro estimado pelo modelo que é o parâmetro nu (\\(\\nu\\)), que é os graus de liberdade da distribuição t de Student usada como verossimilhança:\n\n\nsummary(model_2, prob =  0.9)\n\n\n Family: student \n  Links: mu = identity; sigma = identity; nu = identity \nFormula: prestige ~ income + education \n   Data: duncan (Number of observations: 45) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -6.65      4.19   -13.40     0.26 1.00     4079     3241\nincome        0.66      0.14     0.43     0.89 1.00     1695     1996\neducation     0.51      0.11     0.33     0.70 1.00     1767     1867\n\nFamily Specific Parameters: \n      Estimate Est.Error l-90% CI u-90% CI Rhat Bulk_ESS Tail_ESS\nsigma    12.42      1.88     9.39    15.56 1.00     1985     1670\nnu       18.39     13.65     3.64    45.00 1.00     2117     1708\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nMas a posterior predictive check ficou com um aspecto muito melhor que o modelo linear:\n\n\npp_check(model_2, nsamples = 45)\n\n\n\n\nPriors do brms\nbrms possui as seguintes configurações como padrão de priors para regressão robusta usando t de Student:\nConstante (Intercept): t de Student com média \\(\\mu = \\text{median}_y\\), desvio padrão de \\(\\max(2.5, MAD(y)\\) e graus de liberdade \\(3\\) - prior = student_t(3, median_y, mad_y), class = intercept\nCoeficientes: para cada coeficiente média \\(\\mu = 0\\) e desvio padrão de \\(2.5\\times\\frac{1}{\\sigma_{x_k}}\\) - prior = normal(0, 2.5 * 1/sd_xk)\nErro residual (sigma): t de Student com média \\(\\mu = 0\\), desvio padrão de \\(\\max(2.5, MAD(y)\\) e graus de liberdade \\(3\\) - prior = student_t(3, 0, mad_y), class = sigma\nGraus de liberdade (nu): distribuição gamma com \\(\\alpha = 2\\) e \\(\\beta = 0.1\\) - prior = gamma(2, 0.1), class = nu\nAtividade Prática\nO dataset Boston Housing está disponível em datasets/Boston_Housing.csv. Possui 506 observações e possui 14 variáveis:\nCRIM - per capita crime rate by town\nZN - proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS - proportion of non-retail business acres per town.\nCHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\nNOX - nitric oxides concentration (parts per 10 million)\nRM - average number of rooms per dwelling\nAGE - proportion of owner-occupied units built prior to 1940\nDIS - weighted distances to five Boston employment centres\nRAD - index of accessibility to radial highways\nTAX - full-value property-tax rate per $10,000\nPTRATIO - pupil-teacher ratio by town\nB - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\nLSTAT - % lower status of the population\nMEDV - Median value of owner-occupied homes in $1000’s\n\n\n###\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:38:19-03:00"
    },
    {
      "path": "9-Regressao_Multinivel.html",
      "title": "Modelos Multiniveis",
      "description": "Modelos Multiniveis ou Modelos Hierárquicos",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nQuando usar Modelos Multiníveis?\nHyperprior\nTrês abordagens\nRandom Intercept Model\nRandom Slope Model\nRandom Intercept-Slope Model\nExemplo com o dataset cheese\n\nPriors de Modelos Multiníveis\nAtividade Prática\nAmbiente\n\n\nModelos hierárquicos Bayesianos (também chamados de modelos multiníveis) são um modelo estatístico escrito em níveis múltiplos (forma hierárquica) que estima os parâmetros da distribuição posterior usando o método Bayesiano. Os submodelos se combinam para formar o modelo hierárquico, e o teorema de Bayes é usado para integrá-los aos dados observados e contabilizar toda a incerteza que está presente. O resultado dessa integração é a distribuição posterior, também conhecida como estimativa de probabilidade atualizada, à medida que evidências adicionais sobre a distribuição anterior são adquiridas.\nA modelagem hierárquica é usada quando as informações estão disponíveis em vários níveis diferentes de unidades de observação. A forma hierárquica de análise e organização auxilia no entendimento de problemas multiparâmetros e também desempenha um papel importante no desenvolvimento de estratégias computacionais.\nOs modelos hierárquicos são descrições matemáticas que envolvem vários parâmetros, de modo que as estimativas de alguns parâmetros dependem significativamente dos valores de outros parâmetros.\nModelo HierárquicoQuando usar Modelos Multiníveis?\nModelos multiníveis são particularmente apropriados para projetos de pesquisa onde os dados dos participantes são organizados em mais de um nível (ou seja, dados aninhados - nested data). As unidades de análise geralmente são indivíduos (em um nível inferior) que estão aninhados em unidades contextuais/agregadas (em um nível superior).\nHá um pressuposto principal que não pode ser violado em modelos multiníveis que é o de permutabilidade. Esse pressuposto parte do princípio que os grupos são permutáveis. Se esse pressuposto é violado na sua inferência, então modelos multiníveis não são apropriados.\nHyperprior\nComo as priors dos parâmetros são amostradas de uma outra prior do hiperparâmetro (parâmetro do nível superior), as priors do nível superior são chamadas de hyperpriors. Isso faz com que estimativas de um grupo ajudem o modelo a estimar melhor os outros grupos e dando estimativas mais robustas e estáveis.\nTrês abordagens\nModelos multiníveis geralmente se dividem em três abordagens:\nRandom intercept model: Modelo no qual cada grupo recebe uma constante (intercept) diferente\nRandom slope model: Modelo no qual cada grupo recebe um coeficiente diferente para cada variável independente\nRandom intercept-slope model: Modelo no qual cada grupo recebe tanto uma constante (intercept) quanto um coeficiente diferente para cada variável independente\nRandom Intercept Model\nA primeira abordagem é o random intercept model na qual especificamos para cada grupo uma constante diferente. Essas constantes são amostrada de uma hyperprior.\nO pacote rstanarm tem as funcionalidades completas para rodar modelos multiníveis e a única coisa a se fazer é alterar a formula. Há uma segunda mudança também que não usamos mais a função stan_glm() mas sim a função stan_glmer().\nNo caso de random intercept model, a formula a ser usada segue este padrão:\ny ~ (1 | group) + x1 + x2\nRandom Slope Model\nA segunda abordagem é o random slope model na qual especificamos para cada grupo um coeficiente diferente para cada variável independente. Esses coeficientes são amostrada de uma hyperprior.\nNo caso de random slope model, a formula a ser usada segue este padrão:\ny ~ (0 + x1 | group) + (0 + x2 | group)\nRandom Intercept-Slope Model\nA terceira abordagem é o random intercept-slope model na qual especificamos para cada grupo uma constante diferente além de coeficientes diferentes para cada variável independente. Essas constantes e coeficientes são amostrados de duas ou mais hyperpriors.\nNo caso de random intercept-slope model, a formula a ser usada segue este padrão:\ny ~ (1 + x1 | group) + (1 + x2 | group)\nExemplo com o dataset cheese\nO dataset cheese possui 160 observações de avaliações de queijo. Um grupo de 10 avaliadores “rurais” e 10 “urbanos” avaliaram 4 queijos diferentes \\((A,B,C,D)\\) em duas amostras. Portanto \\(4 \\cdot 20 \\cdot 2 = 160\\). Possui 4 variáveis:\ncheese: tipo do queijo \\((A,B,C,D)\\)\nrater: avaliador \\((1,\\dots, 10)\\)\nbackground: origem do avaliador em “urbano” ou “rural”\ny: variável dependente - nota da avaliação\n\n\ncheese <- read.csv2(\"datasets/cheese.csv\", stringsAsFactors = TRUE, row.names = 1)\n\n\n\nRandom Intercept Model\nNo primeiro exemplo vamos usar um modelo que cada grupo de cheese recebe uma constante diferente:\n\n\nlibrary(rstanarm)\nrandom_intercept <- stan_glmer(\n  y ~ (1 | cheese) + background,\n  data = cheese\n)\n\n\n\nNo sumário do modelo vemos que os avaliadores urbanos avaliam melhor os queijos que os avaliadores rurais, mas também observamos que cada queijo possui uma “taxa basal” de avaliação. Sendo \\(B\\) o pior queijo e \\(C\\) o melhor queijo:\n\n\nsummary(random_intercept)\n\n\n\nModel Info:\n function:     stan_glmer\n family:       gaussian [identity]\n formula:      y ~ (1 | cheese) + background\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 160\n groups:       cheese (4)\n\nEstimates:\n                                        mean   sd    10%   50%   90%\n(Intercept)                            67.2    5.9  60.3  67.2  74.1\nbackgroundurban                         7.4    1.1   5.9   7.4   8.7\nb[(Intercept) cheese:A]                 3.8    5.9  -3.1   3.7  10.8\nb[(Intercept) cheese:B]               -14.1    5.9 -21.2 -13.9  -7.1\nb[(Intercept) cheese:C]                 8.6    5.9   1.7   8.6  15.7\nb[(Intercept) cheese:D]                 1.4    5.9  -5.6   1.4   8.3\nsigma                                   7.1    0.4   6.6   7.1   7.6\nSigma[cheese:(Intercept),(Intercept)] 138.9  131.0  43.1  98.7 270.7\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 70.8    0.8 69.9  70.8  71.8 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                      mcse Rhat n_eff\n(Intercept)                           0.2  1.0  1003 \nbackgroundurban                       0.0  1.0  3702 \nb[(Intercept) cheese:A]               0.2  1.0   998 \nb[(Intercept) cheese:B]               0.2  1.0  1013 \nb[(Intercept) cheese:C]               0.2  1.0  1009 \nb[(Intercept) cheese:D]               0.2  1.0  1014 \nsigma                                 0.0  1.0  3482 \nSigma[cheese:(Intercept),(Intercept)] 3.5  1.0  1365 \nmean_PPD                              0.0  1.0  4036 \nlog-posterior                         0.1  1.0  1307 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nRandom Slope Model\nNo segundo exemplo vamos usar um modelo que cada grupo de cheese recebe um coeficiente diferente para background:\n\n\nrandom_slope <- stan_glmer(\n  y ~ (0 + background | cheese),\n  data = cheese\n)\n\n\n\nAqui vemos que todos os queijos recebem a mesma constante mas cada queijo possui um coeficiente diferente para background do avaliador:\n\n\nsummary(random_slope)\n\n\n\nModel Info:\n function:     stan_glmer\n family:       gaussian [identity]\n formula:      y ~ (0 + background | cheese)\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 160\n groups:       cheese (4)\n\nEstimates:\n                                                mean   sd    10%\n(Intercept)                                    70.0    5.8  63.0\nb[backgroundrural cheese:A]                     0.7    5.9  -6.7\nb[backgroundurban cheese:A]                     8.6    6.0   1.1\nb[backgroundrural cheese:B]                   -15.8    6.2 -23.4\nb[backgroundurban cheese:B]                   -10.3    5.8 -17.3\nb[backgroundrural cheese:C]                     5.3    5.9  -1.9\nb[backgroundurban cheese:C]                    13.5    6.1   5.9\nb[backgroundrural cheese:D]                    -0.7    5.9  -8.0\nb[backgroundurban cheese:D]                     5.3    6.0  -2.2\nsigma                                           7.1    0.4   6.6\nSigma[cheese:backgroundrural,backgroundrural] 136.3  131.4  40.9\nSigma[cheese:backgroundurban,backgroundrural]  75.5   92.7   5.7\nSigma[cheese:backgroundurban,backgroundurban] 160.7  144.9  49.9\n                                                50%   90%\n(Intercept)                                    70.0  77.2\nb[backgroundrural cheese:A]                     0.6   7.8\nb[backgroundurban cheese:A]                     8.7  15.9\nb[backgroundrural cheese:B]                   -15.7  -8.3\nb[backgroundurban cheese:B]                   -10.4  -3.2\nb[backgroundrural cheese:C]                     5.3  12.5\nb[backgroundurban cheese:C]                    13.5  21.1\nb[backgroundrural cheese:D]                    -0.7   6.3\nb[backgroundurban cheese:D]                     5.3  12.5\nsigma                                           7.1   7.7\nSigma[cheese:backgroundrural,backgroundrural]  96.6 274.3\nSigma[cheese:backgroundurban,backgroundrural]  53.6 170.5\nSigma[cheese:backgroundurban,backgroundurban] 117.4 311.3\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 70.8    0.8 69.8  70.8  71.9 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                              mcse Rhat n_eff\n(Intercept)                                   0.2  1.0   662 \nb[backgroundrural cheese:A]                   0.2  1.0   674 \nb[backgroundurban cheese:A]                   0.2  1.0   716 \nb[backgroundrural cheese:B]                   0.2  1.0   689 \nb[backgroundurban cheese:B]                   0.2  1.0   716 \nb[backgroundrural cheese:C]                   0.2  1.0   683 \nb[backgroundurban cheese:C]                   0.2  1.0   680 \nb[backgroundrural cheese:D]                   0.2  1.0   673 \nb[backgroundurban cheese:D]                   0.2  1.0   682 \nsigma                                         0.0  1.0  4728 \nSigma[cheese:backgroundrural,backgroundrural] 3.1  1.0  1742 \nSigma[cheese:backgroundurban,backgroundrural] 2.5  1.0  1362 \nSigma[cheese:backgroundurban,backgroundurban] 4.7  1.0   942 \nmean_PPD                                      0.0  1.0  3721 \nlog-posterior                                 0.1  1.0  1050 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nRandom Intercept-Slope Model\nNo terceiro exemplo vamos usar um modelo que cada grupo de cheese recebe uma constante diferente e um coeficiente diferente para background:\n\n\nrandom_intercept_slope <- stan_glmer(\n  y ~ (1 + background | cheese),\n  data = cheese\n)\n\n\n\nAqui vemos que os queijos recebem a constantes diferentes e que cada queijo possui um coeficiente diferente para background do avaliador:\n\n\nsummary(random_intercept_slope)\n\n\n\nModel Info:\n function:     stan_glmer\n family:       gaussian [identity]\n formula:      y ~ (1 + background | cheese)\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 160\n groups:       cheese (4)\n\nEstimates:\n                                                mean   sd    10%\n(Intercept)                                    65.8    7.8  56.3\nb[(Intercept) cheese:A]                         5.1    7.8  -4.3\nb[backgroundurban cheese:A]                     7.8    2.2   5.0\nb[(Intercept) cheese:B]                       -11.2    8.1 -21.0\nb[backgroundurban cheese:B]                     4.7    2.3   1.7\nb[(Intercept) cheese:C]                         9.5    7.8   0.4\nb[backgroundurban cheese:C]                     8.3    2.2   5.4\nb[(Intercept) cheese:D]                         3.5    7.8  -5.8\nb[backgroundurban cheese:D]                     6.0    2.1   3.3\nsigma                                           7.1    0.4   6.6\nSigma[cheese:(Intercept),(Intercept)]         149.5  144.5  43.3\nSigma[cheese:backgroundurban,(Intercept)]      16.8   75.6 -55.4\nSigma[cheese:backgroundurban,backgroundurban]  85.6   82.5  24.2\n                                                50%   90%\n(Intercept)                                    65.7  75.0\nb[(Intercept) cheese:A]                         5.1  14.6\nb[backgroundurban cheese:A]                     7.8  10.5\nb[(Intercept) cheese:B]                       -11.1  -1.4\nb[backgroundurban cheese:B]                     4.7   7.6\nb[(Intercept) cheese:C]                         9.5  18.9\nb[backgroundurban cheese:C]                     8.3  11.2\nb[(Intercept) cheese:D]                         3.7  12.9\nb[backgroundurban cheese:D]                     6.0   8.7\nsigma                                           7.1   7.7\nSigma[cheese:(Intercept),(Intercept)]         104.7 295.0\nSigma[cheese:backgroundurban,(Intercept)]      13.7  92.4\nSigma[cheese:backgroundurban,backgroundurban]  61.7 168.3\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 70.8    0.8 69.8  70.8  71.9 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                              mcse Rhat n_eff\n(Intercept)                                   0.3  1.0   664 \nb[(Intercept) cheese:A]                       0.3  1.0   675 \nb[backgroundurban cheese:A]                   0.0  1.0  4607 \nb[(Intercept) cheese:B]                       0.3  1.0   670 \nb[backgroundurban cheese:B]                   0.0  1.0  2970 \nb[(Intercept) cheese:C]                       0.3  1.0   684 \nb[backgroundurban cheese:C]                   0.0  1.0  3896 \nb[(Intercept) cheese:D]                       0.3  1.0   664 \nb[backgroundurban cheese:D]                   0.0  1.0  5162 \nsigma                                         0.0  1.0  3677 \nSigma[cheese:(Intercept),(Intercept)]         3.9  1.0  1384 \nSigma[cheese:backgroundurban,(Intercept)]     2.6  1.0   836 \nSigma[cheese:backgroundurban,backgroundurban] 1.9  1.0  1944 \nmean_PPD                                      0.0  1.0  3871 \nlog-posterior                                 0.1  1.0  1126 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nPriors de Modelos Multiníveis\nRelembrando a tabela de priors da Aula 4:\nArgumento\nUsado em\nAplica-se à\nprior_intercept\nTodas funções de modelagem exceto stan_polr and stan_nlmer\nConstante (intercept) do modelo, após centralização dos preditores\nprior\nTodas funções de modelagem\nCoeficientes de Regressão, não inclui coeficientes que variam por grupo em modelos multiníveis (veja prior_covariance)\nprior_aux\nstan_glm, stan_glmer, stan_gamm4, stan_nlmer\nParâmetro auxiliar (ex: desvio padrão (standard error - DP), interpretação depende do modelo\nprior_covariance\nstan_glmer, stan_gamm4, stan_nlmer\nMatrizes de covariância em modelos multiníveis\nConstante(Intercept): centralizada com média \\(\\mu_{y_{group}}\\) para cada grupo e desvio padrão de \\(2.5 \\sigma_{y_{group}}\\) para cada grupo - prior_intercept = normal(mean_y_group, 2.5 * sd_y_group)\nCoeficientes: aqui não especifica-se uma prior para cada coeficiente, mas sim uma prior para a matriz de correlação das variáveis independentes usando uma distribuição LKJ - prior_covariance = lkj(regularization = 1, concentration = 1, shape = 1, scale = 1)\nAtividade Prática\nPara atividade prática, temos o dataset rikz em datasets/rikz.csv.\nFor each of 9 intertidal areas (denoted ‘Beaches’), the researchers sampled five sites (denoted ‘Sites’) and at each site they measured abiotic variables and the diversity of macro-fauna (e.g. aquatic invertebrates). Here, species richness refers to the total number of species found at a given site while NAP ( i.e. Normal Amsterdams Peil) refers to the height of the sampling location relative to the mean sea level and represents a measure of the amount of food available for birds, etc. For our purpose, the main question is:\nWhat is the influence of NAP on species richness?\nRikz Dataset\n\nrikz <- read.csv2(\"datasets/rikz.csv\", row.names = 1)\nrikz$Beach <- as.factor(rikz$Beach)\nrikz$Site <- as.factor(rikz$Site)\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:39:07-03:00"
    },
    {
      "path": "aux-Dados_Faltantes.html",
      "title": "Dados Faltantes",
      "description": "Dados Faltantes",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nRemover dados faltantes\nImputar valores nos dados faltantes\nImputar a média\nImputar a mediana\nImputar o último valor ocorrido\n\nComparação dos resultados\nAmbiente\n\n\nDados faltantes são um problema comum em qualquer análise de dados. Tanto o rstan, quanto brms e rstanarm usam observações completas nas suas inferências. Então, toma observação que contiver qualquer dado faltante será removida por completa. Temos duas abordagens básicas para lidar com dados faltantes1:\nremover os dados faltantes\nimputar valores nos dados faltantes\nRemover dados faltantes\nA remoção de dados faltantes se divide em duas principais abordagens usando a função na.omit() do pacote base stats:\nremoção de observações com dados faltantes: aqui removemos as linhas com dados faltantes df <- na.omit(df)\nremoção de variáveis com dados faltantes: aqui removemos as colunas com dados faltantes df <- t(na.omit(t(df)))\nImputar valores nos dados faltantes\nDentre as diversas maneiras de imputar valores ao dados faltantes, as mais comuns são três:\nimputar a média\nimputar a mediana\nimputar o último valor ocorrido (muito usada em séries temporais)\nMas ainda há maneiras mais avançadas e que desempenham melhor em certas condições (não cobriremos essas técnicas nesse curso):\nk-nearest neighbors imputation\nrandom forest imputation\nHá um pacote de R chamado DescTools que é uma coleção de funções focadas especialmente na parte descritiva de análise de um dataset.\nPara mostrar as abordagens, geramos um dataset de uma série temporal de uma semana com dados faltantes:\n\n\nlibrary(DescTools)\nset.seed(123)\ndf <- data.frame(\n  dia = c(\"seg\", \"ter\", \"qua\", \"qui\", \"sex\", \"sab\", \"dom\"),\n  valor = runif(7))\nindices_aleatorios <- sample(1:nrow(df), 2)\ndf[indices_aleatorios[1], 2] <- NA\ndf[indices_aleatorios[2], 2] <- NA\n\n\n\nImputar a média\n\n\ndf$media <- Impute(df$valor, FUN = mean(df$valor, na.rm = T))\n\n\n\nImputar a mediana\n\n\ndf$mediana <- Impute(df$valor, FUN = median(df$valor, na.rm = T))\n\n\n\nImputar o último valor ocorrido\n\n\ndf$ultimo <- LOCF(df$valor)\n\n\n\nComparação dos resultados\n\n\ndf\n\n\n  dia valor media mediana ultimo\n1 seg  0.29  0.29    0.29   0.29\n2 ter  0.79  0.79    0.79   0.79\n3 qua    NA  0.69    0.79   0.79\n4 qui  0.88  0.88    0.88   0.88\n5 sex  0.94  0.94    0.94   0.94\n6 sab    NA  0.69    0.79   0.94\n7 dom  0.53  0.53    0.53   0.53\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\nhá uma terceira que é modelar os dados faltantes, veja a vinheta do brms para mais detalhes↩︎\n",
      "last_modified": "2021-02-12T06:39:10-03:00"
    },
    {
      "path": "aux-Regressao_Coeficientes.html",
      "title": "Coeficientes de uma Regressão",
      "description": "Diferenças entre Coeficientes Padronizados vs Brutos",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nSimulação\nMédia e Desvio Padrões\nCoeficientes Brutos vs Padronizados\n\nAmbiente\n\n\nEm tabelas de regressão temos geralmente temos duas opções de reportar os coeficientes:\nCoeficientes Brutos: não há transformações e as associações das variáveis independentes/controles (covariáveis) com a dependente são reportadas em suas medidas originais. Exemplo: A cada 1 unidades de aumento de \\(x\\), \\(y\\) aumenta 0.45.\nCoeficientes Padronizados: os coeficientes são transformados para expressarem as associações das variáveis independentes/controles (covariáveis) com a dependente em relação à variação dos seus desvios padrões. Exemplo: A cada 1 desvio padrão de variação positiva de \\(x\\), \\(y\\) possui variação de 0.1 desvio padrão.\nSimulação\nPara explicar melhor esses conceitos, simularemos alguns dados:\n\\(x\\): 1,000 observações amostradas de uma distribuição normal com média 1 e desvio padrão 0.1. \\(x \\sim \\mathcal{N} (1, 0.1)\\)\n\\(y\\): uma combinação linear de \\(100x\\) com uma constante e um erro pequeno normalmente distribuído. \\(y = 10 + 100x + \\epsilon\\) e \\(\\epsilon \\sim \\mathcal{N} (0, 1)\\).\n\n\nN <- 1000\nx <- rnorm(N, 1, 0.1)\nerror <- rnorm(N, 0, 1)\ny <- rep(10, N) + 100*x + error\n\ndf <- data.frame(x, y)\n\n\n\n\n\nlibrary(skimr)\nskim(df)\n\n\nTable 1: Data summary\nName\ndf\nNumber of rows\n1000\nNumber of columns\n2\n_______________________\n\nColumn type frequency:\n\nnumeric\n2\n________________________\n\nGroup variables\nNone\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nx\n0\n1\n1\n0.1\n0.67\n0.93\n1\n1.1\n1.3\n▁▃▇▃▁\ny\n0\n1\n110\n10.1\n75.94\n103.01\n109\n116.5\n142.7\n▁▃▇▃▁\n\nMédia e Desvio Padrões\nPrestem atenção:\n\\(x\\): média 1, desvio padrão 0.1\n\\(y\\): média 109.6, desvio padrão 10.05\nCoeficientes Brutos vs Padronizados\nAgora vamos rodar uma regressão e mostrar coeficientes tanto os coeficientes brutos e os padronizados\n\n\nlibrary(lm.beta)\nmodel <- lm.beta(lm(y ~ x, df))\nsummary(model)\n\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.701 -0.727 -0.009  0.683  2.728 \n\nCoefficients:\n            Estimate Standardized Std. Error t value\n(Intercept)    9.460        0.000      0.324    29.2\nx            100.506        0.995      0.324   310.5\n                       Pr(>|t|)    \n(Intercept) <0.0000000000000002 ***\nx           <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 998 degrees of freedom\nMultiple R-squared:  0.99,  Adjusted R-squared:  0.99 \nF-statistic: 9.64e+04 on 1 and 998 DF,  p-value: <0.0000000000000002\n\nPor fim, ambas colunas mostram a mesma coisa\nColuna não padronizada Estimate: a cada 1 unidade que \\(x\\) aumenta, \\(y\\) aumenta 100.51\nColuna padronizada Standardized: a cada 1 desvio padrão de \\(x\\) de incremento (dp = 0.1), há um aumento de 0.99 desvio padrão de \\(y\\) (10). Um total de 100.51. \\(\\big( \\frac{0.955 * \\operatorname{sd}_y}{\\operatorname{sd}_x}\\big)\\)\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:39:11-03:00"
    },
    {
      "path": "aux-Tabelas_para_Publicacao.html",
      "title": "Tabelas para Publicação",
      "description": "Como montar tabelas de modelos Bayesianos prontas para publicação",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nEstatísticas Descritivas\nTabela de Correlações\nRegressão Linear Bayesiana\nTabela de Regressão Linear\n\nModelo de Regressão Binomial/Logística\nTabela de Regressão Binomial/Logística\n\nAmbiente\n\n\n\nknitr::opts_chunk$set(echo = TRUE)\n# Detectar quantos cores/processadores\noptions(mc.cores = parallel::detectCores())\noptions(Ncpus = parallel::detectCores())\n\nlibrary(dplyr)\nlibrary(rstanarm)\nlibrary(gtsummary)\nlibrary(sjPlot)\n\n# algumas modificações no datadset kidiq\nkidiq <- kidiq %>%\n  mutate(mom_hs = factor(mom_hs, labels = c(\"no\", \"yes\")))\n\n\n\n\nAo invés de ser obrigado a passar horas a fio formatando tabelas em Excel softwares pagos, você pode usar pacotes gratuitos do R para formatar automaticamente suas tabelas:\nEstatísticas Descritivas: gtsummary::tbl_summary()\nCorrelações: sjPlot::tab_corr()\nRegressões: sjPlot::tab_model()\nEstatísticas Descritivas\nO pacote gtsummary possui um conjunto de funções para sumarizar dados e tabelas. Eu particularmente gosto da função gtsummary::tbl_summary(). Ela formata uma tabela de Estatística Descritiva de maneira bem conveniente.\n\n\ngtsummary::tbl_summary(\n  kidiq,\n  by = mom_hs,\n  type = all_continuous() ~ \"continuous2\",\n  statistic = list(\n    all_continuous() ~ c(\"{N_nonmiss}\",\n                         \"{median} ({p25}, {p75})\",\n                         \"{min}, {max}\"),\n    all_categorical() ~ \"{n} ({p}%)\"),\n  missing = \"no\",\n  digits = all_continuous() ~ 2) %>%\n  # add p value and overall\n  add_p(pvalue_fun = ~style_pvalue(.x, digits = 2)) %>%\n  add_overall() %>%\n  # bold variable labels, italicize levels\n  bold_labels() %>%\n  italicize_levels() %>%\n  # change stuff\n  modify_header(label ~ \"**Variable**\") %>%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Mom High School**\") %>%\n  add_n()\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#rjepkiiayr .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#rjepkiiayr .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#rjepkiiayr .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#rjepkiiayr .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#rjepkiiayr .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#rjepkiiayr .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#rjepkiiayr .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#rjepkiiayr .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#rjepkiiayr .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#rjepkiiayr .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#rjepkiiayr .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#rjepkiiayr .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#rjepkiiayr .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#rjepkiiayr .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rjepkiiayr .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rjepkiiayr .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#rjepkiiayr .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#rjepkiiayr .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#rjepkiiayr .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#rjepkiiayr .gt_left {\n  text-align: left;\n}\n\n#rjepkiiayr .gt_center {\n  text-align: center;\n}\n\n#rjepkiiayr .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#rjepkiiayr .gt_font_normal {\n  font-weight: normal;\n}\n\n#rjepkiiayr .gt_font_bold {\n  font-weight: bold;\n}\n\n#rjepkiiayr .gt_font_italic {\n  font-style: italic;\n}\n\n#rjepkiiayr .gt_super {\n  font-size: 65%;\n}\n\n#rjepkiiayr .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nVariable\n      N\n      Overall, N = 434\n      \n        Mom High School\n      \n      p-value1\n    no, N = 93\n      yes, N = 341\n    kid_score\n      434.00\n      \n      \n      \n      <0.001\n    N\n      \n      434.00\n      93.00\n      341.00\n      \n    Median (IQR)\n      \n      90.00 (74.00, 102.00)\n      80.00 (58.00, 95.00)\n      92.00 (77.00, 103.00)\n      \n    Range\n      \n      20.00, 144.00\n      20.00, 136.00\n      38.00, 144.00\n      \n    mom_iq\n      434.00\n      \n      \n      \n      <0.001\n    N\n      \n      434.00\n      93.00\n      341.00\n      \n    Median (IQR)\n      \n      97.92 (88.66, 110.27)\n      88.66 (81.83, 99.16)\n      100.24 (90.45, 113.17)\n      \n    Range\n      \n      71.04, 138.89\n      74.23, 127.54\n      71.04, 138.89\n      \n    mom_age\n      434.00\n      \n      \n      \n      <0.001\n    N\n      \n      434.00\n      93.00\n      341.00\n      \n    Median (IQR)\n      \n      23.00 (21.00, 25.00)\n      21.00 (20.00, 24.00)\n      23.00 (21.00, 25.00)\n      \n    Range\n      \n      17.00, 29.00\n      17.00, 28.00\n      17.00, 29.00\n      \n    \n        \n          1\n          \n           \n          Wilcoxon rank sum test\n          \n      \n    \n\nTabela de Correlações\nPara as tabelas de correlações, eu uso o pacote sjPlot com a função sjPlot::tab_cor()\nOs astericos significam:\n* - \\(p < 0.05\\)\n** - \\(p < 0.01\\)\n*** - \\(p < 0.001\\)\n\n\nsjPlot::tab_corr(\n  kidiq %>% mutate(mom_hs = as.integer(mom_hs)),\n  digits = 2,\n  triangle = \"lower\"\n)\n\n\n\n \n\n\nkid_score\n\n\nmom_hs\n\n\nmom_iq\n\n\nmom_age\n\n\nkid_score\n\n\n \n\n\n \n\n\n \n\n\n \n\n\nmom_hs\n\n\n0.24***\n\n\n \n\n\n \n\n\n \n\n\nmom_iq\n\n\n0.45***\n\n\n0.28***\n\n\n \n\n\n \n\n\nmom_age\n\n\n0.09\n\n\n0.21***\n\n\n0.09\n\n\n \n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\nRegressão Linear Bayesiana\nVamos começar com o caso simples da Aula 2 - Regressão Linear\n\n\nmodel <- stan_glm(\n  kid_score ~ mom_hs + mom_iq,\n  data = kidiq\n  )\n\n\n\nTabela de Regressão Linear\nPara as tabelas de regressões eu geralmente uso o mesmo pacote sjPlot, mas agora com a função sjPlot::tab_model() que aceita um modelo bayesiano.\n\n\ntab_model(model, show.reflvl = TRUE)\n\n\n\n \n\n\nkid_score\n\n\nPredictors\n\n\nEstimates\n\n\nCI (95%)\n\n\n(Intercept)\n\n\n25.43\n\n\n13.84 – 37.36\n\n\nno\n\n\nReference\n\n\n\n\nyes\n\n\n5.99\n\n\n1.72 – 10.31\n\n\nmom_iq\n\n\n0.57\n\n\n0.45 – 0.69\n\n\nObservations\n\n\n434\n\n\nR2 Bayes\n\n\n0.216\n\n\nModelo de Regressão Binomial/Logística\nVamos utilizar o caso da Aula 6 - Regressão Binomial\n\n\nmodel_binomial <- stan_glm(\n  switch ~ dist + arsenic + assoc + educ,\n  data = wells,\n  family = binomial()\n)\n\n\n\nTabela de Regressão Binomial/Logística\nA função sjPlot::tab_model() quando aplicada à um modelo bayesiano linear generalizado (binomial, Poisson etc.) já faz a transformação necessária para uma melhor interpretação dos coeficientes.\nNo caso de modelos binomiais/logísticos geralmente é aplicada uma exponenciação (exp()) dos coeficientes para transformá-los em razões de probabilidades (odds ratio)\nCaso queira deixar os coeficientes brutos (raw coefficients) use transform = NULL\n\n\ntab_model(model_binomial, show.reflvl = TRUE)\n\n\n\n \n\n\nswitch\n\n\nPredictors\n\n\nOdds Ratios\n\n\nCI (95%)\n\n\n(Intercept)\n\n\n0.85\n\n\n0.70 – 1.04\n\n\narsenic\n\n\n1.60\n\n\n1.47 – 1.73\n\n\nassoc\n\n\n0.88\n\n\n0.76 – 1.03\n\n\ndist\n\n\n0.99\n\n\n0.99 – 0.99\n\n\neduc\n\n\n1.04\n\n\n1.02 – 1.06\n\n\nObservations\n\n\n3020\n\n\nR2 Bayes\n\n\n0.067\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.7         gtsummary_1.3.6      lm.beta_1.5-1       \n [4] DescTools_0.99.40    brms_2.14.4          dplyr_1.0.4         \n [7] patchwork_1.1.1      cowplot_1.1.1        rstan_2.21.2        \n[10] StanHeaders_2.21.0-7 MASS_7.3-53          ggforce_0.3.2       \n[13] gganimate_1.0.7      plotly_4.9.3         carData_3.0-4       \n[16] gapminder_0.3.0      skimr_2.1.2          rstanarm_2.21.1     \n[19] Rcpp_1.0.6           readxl_1.3.1         ggplot2_3.3.3       \n[22] DiagrammeR_1.0.6.1  \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.1.4           tidyselect_1.1.0     lme4_1.1-26         \n  [4] htmlwidgets_1.5.3    grid_4.0.3           munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.4.3     distill_1.2         \n [10] statmod_1.4.35       DT_0.17              gifski_0.8.6        \n [13] miniUI_0.1.1.1       withr_2.4.1          Brobdingnag_1.2-6   \n [16] colorspace_2.0-0     highr_0.8            knitr_1.31          \n [19] rstudioapi_0.13      stats4_4.0.3         bayesplot_1.8.0     \n [22] labeling_0.4.2       emmeans_1.5.4        repr_1.1.3          \n [25] mnormt_2.0.2         polyclip_1.10-0      farver_2.0.3        \n [28] bridgesampling_1.0-0 rprojroot_2.0.2      coda_0.19-4         \n [31] vctrs_0.3.6          generics_0.1.0       TH.data_1.0-10      \n [34] xfun_0.21            R6_2.5.0             markdown_1.1        \n [37] isoband_0.2.3        gamm4_0.2-6          projpred_2.0.2      \n [40] assertthat_0.2.1     promises_1.2.0.1     scales_1.1.1        \n [43] multcomp_1.4-16      rootSolve_1.8.2.1    gtable_0.3.0        \n [46] downlit_0.2.1        processx_3.4.5       lmom_2.8            \n [49] sandwich_3.0-0       rlang_0.4.10         splines_4.0.3       \n [52] lazyeval_0.2.2       checkmate_2.0.0      broom_0.7.4         \n [55] inline_0.3.17        yaml_2.2.1           reshape2_1.4.4      \n [58] abind_1.4-5          modelr_0.1.8         threejs_0.3.3       \n [61] crosstalk_1.1.1      backports_1.2.1      httpuv_1.5.5        \n [64] rsconnect_0.8.16     tools_4.0.3          usethis_2.0.1       \n [67] bookdown_0.21        ellipsis_0.3.1       RColorBrewer_1.1-2  \n [70] ggridges_0.5.3       plyr_1.8.6           base64enc_0.1-3     \n [73] visNetwork_2.0.9     progress_1.2.2       purrr_0.3.4         \n [76] ps_1.5.0             prettyunits_1.1.1    zoo_1.8-8           \n [79] fs_1.5.0             here_1.0.1           magrittr_2.0.1      \n [82] data.table_1.13.6    magick_2.6.0         colourpicker_1.1.0  \n [85] tmvnsim_1.0-2        mvtnorm_1.1-1        sjmisc_2.8.6        \n [88] matrixStats_0.58.0   hms_1.0.0            shinyjs_2.0.0       \n [91] mime_0.9             evaluate_0.14        xtable_1.8-4        \n [94] shinystan_2.5.0      sjstats_0.18.1       jpeg_0.1-8.1        \n [97] gridExtra_2.3        ggeffects_1.0.1      rstantools_2.1.1    \n[100] compiler_4.0.3       tibble_3.0.6         gt_0.2.2            \n[103] V8_3.4.0             crayon_1.4.1         minqa_1.2.4         \n[106] htmltools_0.5.1.1    mgcv_1.8-33          later_1.1.0.1       \n[109] tidyr_1.1.2          expm_0.999-6         Exact_2.1           \n[112] RcppParallel_5.0.2   lubridate_1.7.9.2    DBI_1.1.1           \n[115] tweenr_1.0.1         sjlabelled_1.1.7     broom.helpers_1.1.0 \n[118] boot_1.3-26          Matrix_1.3-2         cli_2.3.0           \n[121] parallel_4.0.3       insight_0.12.0       igraph_1.2.6        \n[124] forcats_0.5.1        pkgconfig_2.0.3      xml2_1.3.2          \n[127] dygraphs_1.1.1.6     estimability_1.3     snakecase_0.11.0    \n[130] stringr_1.4.0        callr_3.5.1          digest_0.6.27       \n[133] parameters_0.11.0    rmarkdown_2.6        cellranger_1.1.0    \n[136] gld_2.6.2            curl_4.3             commonmark_1.7      \n[139] shiny_1.6.0          gtools_3.8.2         nloptr_1.2.2.2      \n[142] lifecycle_0.2.0      nlme_3.1-152         jsonlite_1.7.2      \n[145] viridisLite_0.3.0    fansi_0.4.2          pillar_1.4.7        \n[148] lattice_0.20-41      loo_2.4.1            fastmap_1.1.0       \n[151] httr_1.4.2           pkgbuild_1.2.0       survival_3.2-7      \n[154] glue_1.4.2           xts_0.12.1           bayestestR_0.8.2    \n[157] png_0.1-7            shinythemes_1.2.0    sass_0.3.1          \n[160] class_7.3-18         stringi_1.5.3        performance_0.7.0   \n[163] e1071_1.7-4         \n\n\n\n\n",
      "last_modified": "2021-02-12T06:39:38-03:00"
    },
    {
      "path": "index.html",
      "title": "Estatística Bayesiana com R e Stan",
      "description": "Companion para a disciplina de Estatística Bayesiana para alunos de Mestrado e Doutorado da UNINOVE",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nStan\nComo usar esse conteúdo?\nAulas\nO que esta disciplina não é\nRStudio na Núvem Gratuito\nProfessor\nComo usar esse conteúdo?\nReferências\nLivros\nArtigos\n\nConteúdos Similares\nComo citar esse conteúdo\nLicença\n\n\n\nA Estatística Bayesiana é uma abordagem de Estatística inferencial que não usa hipóteses nulas (\\(H_0\\)) e \\(p\\)-valores. Se você não sabe o que é um \\(p\\)-valor, recomendo olhar o tutorial sobre o que são \\(p\\)-valores. Muitos cientistas e pesquisadores acreditam que sabe o que é um \\(p\\)-valor, mas sua compreensão é falha e imperfeita, por isso, mesmo que você acredite que saiba o que é um \\(p\\)-valor, eu ainda recomendo que veja o tutorial sobre o que são \\(p\\)-valores.\nStan\nStan (Carpenter et al., 2017) é uma plataforma para modelagem estatística e computação estatística de alto desempenho. Milhares de usuários contam com Stan para modelagem estatística, análise de dados e previsão nas ciências sociais, biológicas e físicas, engenharia e negócios. Além disso Stan tem o suporte financeiro da NumFOCUS, uma fundação sem fins lucrativos que dá apoio financeiro à projetos de softwares opensource. Dentre os patrocinadores da NumFOCUS podemos citar AWS Amazon, Bloomberg, Microsoft, IBM, RStudio, Facebook, NVIDIA, Netflix, entre outras.\nOs modelos em Stan são especificados pela sua própria linguagem (similar à C++) e são compilados em um arquivo executável que gera inferências estatísticas Bayesiana com amostragem MCMC de alto desempenho. Stan possui interfaces para as seguintes linguagens de programação1:\nR: RStan e CmdStanR\nPython: PyStan e CmdStanPy\nShell (Linha de Comando): CmdStan\nJulia: Stan.jl\nScala: ScalaStan\nMatlab: MatlabStan\nStata: StataStan\nMathematica: MathematicaStan\nA linguagem Stan possui uma curva de aprendizagem bem desafiadora, por isso Stan possui um ecossitemas de pacotes de interfaces que muitas vezes ajudam e simplificam a sua utilização:\nrstanarm: ajuda o usuário a especificar modelos usando a síntaxe familiar de fórmulas do R.\nbrms: similar ao rstanarm pois usa a síntaxe familiar de fórmulas do R, mas dá maior flexibilidade na especificação de modelos mais complexos2.\nStan3 usa um amostrador Monte Carlo de correntes Markov que utiliza dinâmica Hamiltoniana (Hamiltonian Monte Carlo – HMC) para guiar as propostas de amostragem de novos parâmetros no sentido do gradiente da densidade de probabilidade da posterior. Isto implica em um amostrador mais eficiente e que consegue explorar todo o espaço amostral da posterior com menos iterações; e também mais eficaz que consegue tolerar diferentes topologias de espaços amostrais da posterior. Em outras palavras, Stan usa técnicas de amostragem avançadas que permite com que modelos complexos Bayesianos atinjam convergência de maneira rápida. No Stan, raramente deve-se ajustar os parâmetros do algoritmo HMC, pois geralmente os parâmetros padrões (out-of-the-box) funcionam muito bem. Assim, o usuário foca no que é importante: a especificação dos componentes probabilísticos do seu modelo Bayesiano.\nComo usar esse conteúdo?\nEste conteúdo possui licença livre para uso (CC BY-SA). Caso queira utilizar o conteúdo para um curso ou estudos, por favor colabore nesse repositório quaisquer aprimorações que foram realizadas. O propósito do conteúdo não é o rigor matemático geralmente adotado em disciplinas e tutoriais de estatística Bayesiana, mas gerar uma forte intuição deixando de lado o rigor matemático e focar no ferramental (primariamente rstanarm e um pouco de brms).\nPara configurar um ambiente local:\nClone o repositório do GitHub: git clone https://github.com/storopoli/Estatistica-Bayesiana.git\nAcesse o diretório: cd Estatistica-Bayesiana\nInstale os pacotes necessários: Rscript .binder/install.R\nAulas\nO que são p-valores?\nO que é Estatística Bayesiana\nConteúdos Primários:\nComandos Básicos de R\nRegressão Linear\nDistribuições Estatísticas\nPriors\nMarkov Chain Montecarlo (MCMC)\nRegressão Binomial\nRegressão de Poisson\nRegressão Robusta\nModelos Multiníveis\nConteúdos Auxiliares:\nDados Faltantes\nCoeficientes de uma Regressão\nTabelas para Publicação\nO que esta disciplina não é\nNão será coberto conteúdos sobre leitura, manipulação e exportação de dados com R. Para isso recomendo fortemente o livro R para Data Science (Figura 1) que pode ser encontrado gratuitamente aqui e possui uma versão impressa em português4.\n\n\n\nFigure 1: R for Data Science\n\n\n\nRStudio na Núvem Gratuito\nClique no ícone abaixo para abrir uma sessão do RStudio no Projeto Binder.\n\nProfessor\nProf. Dr. José Eduardo Storopoli    \nComo usar esse conteúdo?\nEste conteúdo possui licença livre para uso (CC BY-SA). Caso queira utilizar o conteúdo para um curso ou estudos, por favor colabore nesse repositório quaisquer aprimorações que foram realizadas.\nPara configurar um ambiente local:\nClone o repositório do GitHub: git clone https://github.com/storopoli/Estatistica-Bayesiana.git\nAcesse o diretório: cd Estatistica-Bayesiana\nInstale os pacotes necessários: Rscript .binder/install.R\nReferências\nLivros\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis. Chapman and Hall/CRC.\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press.\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press.\nBrooks, S., Gelman, A., Jones, G., & Meng, X.-L. (2011). Handbook of Markov Chain Monte Carlo. CRC Press. http://books.google.com?id=qfRsAIKZ4rIC\nGeyer, C. J. (2011). Introduction to markov chain monte carlo. In S. Brooks, A. Gelman, G. L. Jones, & X.-L. Meng (Eds.), Handbook of markov chain monte carlo.\n\nArtigos\nBásicos\nGelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., Kennedy, L., Gabry, J., Bürkner, P.-C., & Modr’ak, M. (2020, November 3). Bayesian Workflow. http://arxiv.org/abs/2011.01808\nGabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378\nBenjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E.-J., Berk, R., Bollen, K. A., Brembs, B., Brown, L., Camerer, C., Cesarini, D., Chambers, C. D., Clyde, M., Cook, T. D., De Boeck, P., Dienes, Z., Dreber, A., Easwaran, K., Efferson, C., … Johnson, V. E. (2018). Redefine statistical significance. Nature Human Behaviour, 2(1), 6–10. https://doi.org/10.1038/s41562-017-0189-z\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., & Riddell, A. (2017). Stan : A Probabilistic Programming Language. Journal of Statistical Software, 76(1). https://doi.org/10.18637/jss.v076.i01\nEtz, A. (2018). Introduction to the Concept of Likelihood and Its Applications. Advances in Methods and Practices in Psychological Science, 1(1), 60–69. https://doi.org/10.1177/2515245917744314\nEtz, A., Gronau, Q. F., Dablander, F., Edelsbrunner, P. A., & Baribault, B. (2018). How to become a Bayesian in eight easy steps: An annotated reading list. Psychonomic Bulletin & Review, 25(1), 219–234. https://doi.org/10.3758/s13423-017-1317-5\nMcShane, B. B., Gal, D., Gelman, A., Robert, C., & Tackett, J. L. (2019). Abandon Statistical Significance. American Statistician, 73, 235–245. https://doi.org/10.1080/00031305.2018.1527253\nAmrhein, V., Greenland, S., & McShane, B. (2019). Scientists rise up against statistical significance. Nature, 567(7748), 305–307. https://doi.org/10.1038/d41586-019-00857-9\nvan Ravenzwaaij, D., Cassey, P., & Brown, S. D. (2018). A simple introduction to Markov Chain Monte–Carlo sampling. Psychonomic Bulletin and Review, 25(1), 143–154. https://doi.org/10.3758/s13423-016-1015-8\nVandekerckhove, J., Matzke, D., Wagenmakers, E.-J., & others. (2015). Model comparison and the principle of parsimony. In J. R. Busemeyer, Z. Wang, J. T. Townsend, & A. Eidels (Eds.), Oxford handbook of computational and mathematical psychology (pp. 300–319). Oxford University Press Oxford.\nvan de Schoot, R., Kaplan, D., Denissen, J., Asendorpf, J. B., Neyer, F. J., & van Aken, M. A. G. (2014). A Gentle Introduction to Bayesian Analysis: Applications to Developmental Research. Child Development, 85(3), 842–860. https://doi.org/10.1111/cdev.12169_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/cdev.12169\nWagenmakers, E.-J. (2007). A practical solution to the pervasive problems of p values. Psychonomic Bulletin & Review, 14(5), 779–804. https://doi.org/10.3758/BF03194105\nComplementares\nCohen, J. (1994). The earth is round (p \\(<\\) .05). American Psychologist, 49(12), 997–1003. https://doi.org/10.1037/0003-066X.49.12.997\nDienes, Z. (2011). Bayesian Versus Orthodox Statistics: Which Side Are You On? Perspectives on Psychological Science, 6(3), 274–290. https://doi.org/10.1177/1745691611406920\nEtz, A., & Vandekerckhove, J. (2018). Introduction to Bayesian Inference for Psychology. Psychonomic Bulletin & Review, 25(1), 5–34. https://doi.org/10.3758/s13423-017-1262-3\nJ’unior, C. A. M. (2020). Quanto vale o valor-p? Arquivos de Ciências Do Esporte, 7(2).\nKerr, N. L. (1998). HARKing: Hypothesizing after the results are known. Personality and Social Psychology Review, 2(3), 196–217. https://doi.org/10.1207/s15327957pspr0203_4\nKruschke, J. K., & Vanpaemel, W. (2015). Bayesian estimation in hierarchical models. In J. R. Busemeyer, Z. Wang, J. T. Townsend, & A. Eidels (Eds.), The Oxford handbook of computational and mathematical psychology (pp. 279–299). Oxford University Press Oxford, UK.\nKruschke, J. K., & Liddell, T. M. (2018). Bayesian data analysis for newcomers. Psychonomic Bulletin & Review, 25(1), 155–177. https://doi.org/10.3758/s13423-017-1272-1\nKruschke, J. K., & Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin & Review, 25(1), 178–206. https://doi.org/10.3758/s13423-016-1221-4\nLakens, D., Adolfi, F. G., Albers, C. J., Anvari, F., Apps, M. A. J., Argamon, S. E., Baguley, T., Becker, R. B., Benning, S. D., Bradford, D. E., Buchanan, E. M., Caldwell, A. R., Van Calster, B., Carlsson, R., Chen, S. C., Chung, B., Colling, L. J., Collins, G. S., Crook, Z., … Zwaan, R. A. (2018). Justify your alpha. Nature Human Behaviour, 2(3), 168–171. https://doi.org/10.1038/s41562-018-0311-x\nMorey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The fallacy of placing confidence in confidence intervals. Psychonomic Bulletin & Review, 23(1), 103–123. https://doi.org/10.3758/s13423-015-0947-8\nMurphy, K. R., & Aguinis, H. (2019). HARKing: How Badly Can Cherry-Picking and Question Trolling Produce Bias in Published Results? Journal of Business and Psychology, 34(1). https://doi.org/10.1007/s10869-017-9524-7\nStark, P. B., & Saltelli, A. (2018). Cargo-cult statistics and scientific crisis. Significance, 15(4), 40–43. https://doi.org/10.1111/j.1740-9713.2018.01174.x\nConteúdos Similares\nExistem alguns conteúdos em português similares que eu indico:\nMarco Inácio — Apostila de Stan Um dos desenvolvedores da equipe do Stan. A apostila está um pouco desatualizada (2018). O foco é o rigor matemático e a linguagem Stan. Muito bem escrita e com bons exemplos.\nRicardo Ehlers (USP) — Inferência Bayesiana (Notas de Aula) Notas de uma disciplina da USP pelo professor Ricardo Ehlers. O foco é o rigor matemática e as ferramentas utilizadas são desatualizadas (BUGS e JAGS). Também muito bem escrita e com bons exemplos.\nLuís Gustavo Esteves, Rafael Izbicki e Rafael Bassi Stern (UFSCar) — Inferência Bayesiana (Notas de Aula) Notas de uma disciplina da UFSCar pelos professores Luís Gustavo Esteves, Rafael Izbicki e Rafael Bassi Stern. O foco é o rigor matemático, mas o conteúdo é um pouco mais acessível com uma forte introdução à lógica Bayesiana. Fala um pouco da linguagem Stan e sua interface do R (rstan) no finalzinho.\nComo citar esse conteúdo\nPara citar o conteúdo use:\nStoropoli (2021). Estatística Bayesiana com R e Stan. Disponível em: https://storopoli.io/Estatistica-Bayesiana.\nOu em formato BibTeX para LaTeX:\n@misc{storopoli2021estatisticabayesianaR,\n  author = {Storopoli, Jose},\n  title = {Estatística Bayesiana com R e Stan},\n  url = {https://storopoli.io/Estatistica-Bayesiana},\n  year = {2021}\n}\nLicença\nEste obra está licenciado com uma Licença Creative Commons Atribuição-CompartilhaIgual 4.0 Internacional.\n\n\nestou riscando as linguagens que não são opensource por uma questão de princípios.↩︎\ne geralmente a amostragem é um pouco mais rápida que o rstanarm.↩︎\ne consequentemente todas suas interfaces com diversas linguagens de programação e todos os pacotes do seu ecossistema.↩︎\nNão temos nada a ver com a Amazon. Caso queira comprar em qualquer outra loja fique à vontade, ou algum sebo… Jeff Bezos nem sabe que eu existo…↩︎\n",
      "last_modified": "2021-02-12T15:40:46-03:00"
    },
    {
      "path": "pvalores.html",
      "title": "O que são p-valores?",
      "description": "Porque $p$-valor, hipótese nula ($H_0$) e pressupostos são importantes",
      "author": [
        {
          "name": "Jose Storopoli",
          "url": "https://scholar.google.com/citations?user=xGU7H1QAAAAJ&hl=en"
        }
      ],
      "date": "August 2, 2021",
      "contents": "\n\nContents\nEstatística Inferencial\n\\(p\\)-valor e Hipótese Nula \\(H_0\\)\nAlgumas questões históricas\nO que o \\(p\\)-valor não é\nIntervalos de Confiança\nSignificância Estatística vs Significância Prática\n\nErro Tipo I e Erro Tipo II\nTamanho da Amostra\nE aonde entra a Estatística Bayesiana?\nComentários Finais\nAmbiente\n\n\nEsse conteúdo foi criado com o intuito de despertar o leitor para a importância da Estatística para a ciência e geração de conhecimento. Nossa ideia é apresentar conceitos da maneira que gostaríamos de ter sido apresentados quando alunos prestes a serem iniciados na ciência. Nossa abordagem é simplificar os conceitos o máximo possível sem perder a sua essência. E, quando necessário, aliando-os com sua trajetória histórica para compreensão do “porque as coisas são como são.” Não estamos atrás de formalismo matemático, mas sim de conseguir desenvolver uma intuição clara do que é cada conceito, quando se deve usá-lo e quais são os principais cuidados que se deve ter.\nA estatística é dividida em duas partes:\nEstatística Descritiva: Sumariza e quantifica as características de uma amostra de dados observados. Métricas comuns são: média, mediana, moda, desvio padrão, variância, correlação, percentis.\nEstatística Inferencial: Permite gerar inferências (afirmações) a partir de um conjunto de uma amostra de dados observados sobre real processo de geração de dados (população). Há diversas maneiras de se gerar tais inferências, mas os principais são os testes de hipóteses clássicos que usam uma hipótese nula \\(H_0\\) pré-especificada. A figura 1 mostra a relação entre dados observados e o processo de geração de dados sob a ótica da probabilidade e da estatística.\n\n\n\n{\"x\":{\"diagram\":\"\\n digraph estatistica_inferencial {\\n  forcelabels = true;\\n  graph [overlap = false,\\n         fontsize = 12,\\n         rankdir = TD]\\n  node [shape = oval,\\n        fontname = Helvetica]\\n  A [label = \\\"Processo de\\nGeração de Dados\\\"]\\n  B [label = \\\"Dados\\nObservados\\\"]\\n  A -> B [dir = forward,\\n          xlabel = \\\"  Probabilidade  \\\",\\n          tailport = \\\"e\\\",\\n          headport = \\\"e\\\"]\\n  B -> A [dir = backward,\\n          label = \\\"  Inferência  \\\",\\n          tailport = \\\"w\\\",\\n          headport = \\\"w\\\"]\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\nFigure 1: Estatística Inferencial\n\n\n\nEstatística Inferencial\nO nosso intuito nesse conjunto de tutoriais é focar na Estatística inferencial, porque, ao contrário da Estatística descritiva, a Estatística inferencial é raramente compreendida ao ponto do usuário e consumidor estarem aptos à realizar e consumir análises, respectivamente.\nA Estatística inferencial têm suas origens no final do século XIX, especialmente no trabalho de Karl Pearson1 e se baseia em um conjunto de técnicas e procedimentos para testar hipóteses sobre uma amostra generalizando para uma população-alvo.\n\n\n\nFigure 2: Karl Pearson. Figura de https://www.wikipedia.org\n\n\n\nA chave para compreesão da Estatística inferencial se baseia em entender os testes de hipóteses, também chamado de testes estatísticos. Todos testes estatísticos2 segue o mesmo padrão universal (Downey, 2016):\nCalculamos uma estatística da amostra. Aqui estatística (em letras minúsculas) significa uma medida dos dados. Para fins de exemplo vamos chamar essa medida de \\(\\delta\\) (letra gregra delta). Essa é a medida que mais nos importamos: uma diferença de média, mediana ou proporções, entre outras…\nContrastamos essa estatística observada com uma estatística computada se o efeito fosse nulo. Em outras palavras, o que observamos é comparado com o resultado que esperaríamos caso estívessemos vivendo em um mundo no qual essa medida (diferença de média, mediana ou proporções, …) fosse nula (zero). Geralmente esse universo paralelo no qual o efeito observado é zero ou nulo é chamado de Hipótese Nula e é representada com o seguinte símbolo \\(H_0\\). A estatística \\(\\delta\\) no mundo da \\(H_0\\) não é calculada, mas sim dada por um valor que fora matematicamente provado como o valor de \\(\\delta\\) no mundo da \\(H_0\\). Vamos chamar esse valor de \\(\\delta_0\\)\nCalculamos a probabidalide de obtermos algo como \\(\\delta\\) no mundo da \\(H_0\\): chamamos isso de \\(p\\)-valor. O \\(p\\)-valor é a probabilidade de observarmos um \\(\\delta\\) no mínimo tão grande quanto o observado num mundo no qual não há o efeito \\(\\delta\\). Ou seja \\(\\delta = 0\\), e consequentemente \\(\\delta = \\delta_0\\). Como sabemos do valor \\(\\delta_0\\) de antemão, basta compararmos o nosso \\(\\delta\\) com \\(\\delta_0\\) para gerar o \\(p\\)-valor. Por isso que muitos livros de Estatística possuem um vasto arsenal de tabelas. O leitor pode facilmente ver o seu \\(\\delta\\) e com alguns dados sobre a amostra, em especial o número da amostra, obter o \\(\\delta_0\\) e \\(p\\)-valor respectivos.\nDecidimos se \\(\\delta\\) possui significância estatística. Escolhemos um limiar de rejeição da \\(H_0\\), muitas vezes chamado de \\(\\alpha\\) (letra gregra alpha). Esse limiar será o nosso critério de decisão se há evidências suficientes para rejeitarmos o mundo da \\(H_0\\).\nEste paradigma descrito nos quatro passos acima é chamado de Null Hypothesis Significance Testing – NHST (tradução: teste de significância de hipótese nula) e é o que predomina em grande parte da ciência do passado e atual.\nUma segunda chave para a compreensão da Estatística inferencial possui razões históricas. As técnicas de Estatística inferencial clássicas são em grande parte um mecanismo técnico de aproximações numéricas baseadas na distribuição Normal e suas muitas engrenagens subsidiárias. Essa máquina já foi necessária, porque a alternativa conceitualmente mais simples baseada em permutações estava computacionalmente além de nosso alcance3. Antes dos computadores, os estatísticos não tinham escolha (Cobb, 2007).\n\nQuem ficou curioso com a história da Estatística. Recomendo um livro de Stephen Stigler intitulado Statistics on the Table: The History of Statistical Concepts and Methods. O primeiro autor comprou uma cópia em um sebo online.\n\\(p\\)-valor e Hipótese Nula \\(H_0\\)\n\n\\(p\\)-valores são de difícil entendimento, \\(p < 0.05\\).\n\n\n\n\nSem dúvida, esta parte da Estatística inferencial é a mais complicada e menos intuitiva. Parafraseando Andrew Gelman, estatístico da Columbia University, “Para definir \\(p\\)-valores, escolha uma das duas características: intuitiva ou precisa. Ou sua definição é intuitiva mas imprecisa, ou sua definição é precisa mas não intuitiva.” A grande maioria dos pesquisadores4 possui uma definição incorreta do que é um \\(p\\)-valor (Cumming, 2009). E quando vemos evidências do campo da medicina, que talvez seja o campo com maior quantidade de recursos disponíveis para pesquisa e avanço do conhecimento, também encontramos muitos problemas no uso dos \\(p\\)-valores (Ioannidis, 2019). Antes de entrarmos nas definições de \\(p\\)-valores, vale a pena tranquilizá-los: \\(p\\)-valores são uma coisa complicada e se você não entender na primeira vez que ler as definições abaixo, não se preocupe, você não estará em má companhia; respire fundo e tente ler mais uma vez.\nPrimeiramente a definição estatística:\n\n\\(p\\)-valor é a probabilidade de obter resultados no mínimo tão extremos quanto os que foram observados, dado que a hipótese nula \\(H_0\\) é verdadeira.\n\nSe você escrever essa definição em qualquer prova, livro ou artigo científico, você estará 100% preciso e correto na definição do que é um \\(p\\)-valor. Agora, a compreensão dessa definição é algo complicado. Para isso, vamos quebrar essa definição em algumas partes para melhor compreensão:\n“probabilidade de obter resultados…”: vejam que \\(p\\)-valores são uma característica dos seus dados e não da sua teoria ou hipótese.\n“…no mínimo tão extremos quanto os que foram observados…”: “no minimo tão” implica em definir um limiar para a caracterização de algum achado relevante, que é comumente chamado de \\(\\alpha\\). Geralmente estipulamos alpha em 5% (\\(\\alpha = 0.05\\)) e qualquer coisa mais extrema que alpha (ou seja menor que 5%) caracterizamos como significante5.\n“..dado que a hipótese nula é verdadeira…”: Todo teste estatístico que possui um \\(p\\)-valor possui uma Hipótese Nula (geralmente escrita como \\(H_0\\)). Hipótese nula, sempre tem a ver com algum efeito nulo. Por exemplo, a hipótese nula do teste Shapiro-Wilk e Komolgorov-Smirnov é “os dados são distribuídos conforme uma distribuição Normal” e a do teste de Levene é “as variâncias dos dados são iguais.” Sempre que ver um \\(p\\)-valor, se pergunte: “Qual a hipótese nula que este teste presupõe correta?6”\nPara entender o \\(p\\)-valor qualquer teste estatístico primeiro descubra qual é a hipótese nula por trás daquele teste. A definição do \\(p\\)-valor não mudará. Em todo teste ela é sempre a mesma. O que muda com o teste é a hipótese nula. Cada teste possui sua \\(H_0\\).\n\n\n\n\\(p\\)-valor é a probabilidade dos dados que você obteve dado que a hipótese nula é verdadeira. Para os que gostam do formalismo matemático: \\(p = P(D|H_0)\\). Em português, essa expressão significa “a probabilidade de \\(D\\) condicionado à \\(H_0\\).” Antes de avançarmos para alguns exemplos e tentativas de formalizar uma intuição sobre os \\(p\\)-valores, é importante ressaltar que \\(p\\)-valores dizem algo à respeito dos dados e não de hipóteses. Para o \\(p\\)-valor, a hipótese nula é verdadeira, e estamos apenas avaliando se os dados se conformam à essa hipótese nula ou não. Se vocês saírem desse tutorial munidos com essa intuição, o mundo será agraciado com pesquisadores mais preparados para qualificar e interpretar evidências (\\(p < 0.05\\)).\nExemplo intuitivo:\n\nImagine que você tem uma moeda que suspeita ser enviesada para uma probabilidade maior de dar cara. (Sua hipótese nula é então que a moeda é justa.) Você joga a moeda 100 vezes e obtém mais cara do que coroa. O \\(p\\)-valor não dirá se a moeda é justa, mas dirá a probabilidade de você obter pelo menos tantas caras quanto se a moeda fosse justa. É isso - nada mais.\n\n\nApesar de termos falado anterior que definições intuitivas não são precisas, elas sem dúvida facilitam o entendimento do \\(p\\)-valor.\nAlgumas questões históricas\nNão tem como entendermos \\(p\\)-valores se não compreendermos as suas origens e trajetória histórica. A primeira menção do termo foi feita pelo estatístico Ronald Fisher7 em 1925 (Fisher, 1925) que define o \\(p\\)-valor como um “índice que mede a força da evidência contra a hipótese nula.” Para quantificar a força da evidência contra a hipótese nula, Fisher defendeu “\\(p<0.05\\) (5% de significância) como um nível padrão para concluir que há evidência contra a hipótese testada, embora não como uma regra absoluta.” Fisher não parou por aí mas classificou a força da evidência contra a hipótese nula. Ele propôs “se \\(p\\) está entre 0.1 e 0.9, certamente não há razão para suspeitar da hipótese testada. Se estiver abaixo de 0.02, é fortemente indicado que a hipótese falha em explicar o conjunto dos fatos. Não seremos frequentemente perdidos se traçarmos uma linha convencional de 0.05” Desde que Fisher fez esta declaração há quase 100 anos, o limiar de 0.05 foi usado por pesquisadores e cientistas em todo o mundo e tornou-se ritualístico usar 0.05 como limiar como se outros limiares não pudessem ser usados.\n\n\n\nFigure 3: Ronald Fisher. Figura de https://www.wikipedia.org\n\n\n\nApós isso, o limiar de 0.05 agora instaurado como inquestionável influenciou fortemente a estatística e a ciência. Mas não há nenhuma razão contra a adoção de outros limiares (\\(\\alpha\\)) como 0.1 ou 0.01. Se bem argumentados, a escolha de limiares diferentes de 0.05 pode ser bem-vista por editores, revisores e orientadores. Como o \\(p\\)-valor é uma probabilidade, ele não é um quantidade contínua. Não há razão para diferenciarmos um \\(p\\) de 0.049 contra um \\(p\\) de 0.051. Robert Rosenthal, um psicólogo já dizia “Deus ama \\(p\\) de 0.06 tanto quanto um \\(p\\) de 0.05” (Rosnow & Rosenthal, 1989).\n\n\n\nO que o \\(p\\)-valor não é\nCom a definição e intuição do que é um \\(p\\)-valor bem ancoradas, podemos avançar para o que o \\(p\\)-valor não é!\n\n\n\n\\(p\\)-valor não é a probabilidade da Hipótese nula - Famosa confusão entre \\(P(D|H_0)\\) e \\(P(H_0|D)\\). \\(p\\)-valor não é a probabilidade da hipótese nula, mas sim a probabilidade dos dados que você obteve. Por exemplo: a probabilidade de você tossir dado que você está com COVID é diferente da probabilidade de você estar com COVID dado que você tossiu: \\(P(\\text{tosse} | \\text{COVID}) \\neq P(\\text{COVID} | \\text{tosse})\\). Acredito que a primeira, \\(P(\\text{tosse} | \\text{COVID})\\) é bem alta, enquanto a segunda, \\(P(\\text{COVID} | \\text{tosse})\\) deve ser bem baixa (afinal tossimos a todo momento).\n\nO primeiro autor tentou explicar essa diferença para uma senhora que o viu tossir na fila do mercado, mas os seus esforços foram em vão…\n\\(p\\)-valor não é a probabilidade dos dados serem produzidos pelo acaso - Não! Ninguém falou nada de acaso. Mais uma vez: \\(p\\)-valor é probabilidade de obter resultados no mínimo tão extremos quanto os que foram observados, dado que a hipótese nula é verdadeira.\n\\(p\\)-valor mensura o tamanho do efeito de um teste estatístico - Também não… \\(p\\)-valor não diz nada sobre o tamanho do efeito. Apenas sobre se o quanto os dados observados divergem do esperado sob a hipótese nula. É claro que efeitos grandes são mais prováveis de serem estatisticamente significantes que efeitos pequenos. Mas isto não é via de regra e nunca julguem um achado pelo seu \\(p\\)-valor, mas sim pelo seu tamanho de efeito. Além disso, \\(p\\)-valores podem ser “hackeados” de diversas maneiras (Head, Holman, Lanfear, Kahn, & Jennions, 2015) e muitas vezes seu valor é uma consequência direta do tamanho da amostra. Mais sobre isso no conteúdo auxiliar sobre tamanho de amostra.\nIntervalos de Confiança\nIntervalos de confiança foram criados como uma solução para os problemas de má-interpretação dos \\(p\\)-valores e sua aplicação se destina ao tamanho do efeito. Se você achou \\(p\\)-valor confuso, se prepare! Intervalos de confiança são ainda mais confusos e muitos pesquisadores e cientistas também não possuem a compreensão correta (Hoekstra, Morey, Rouder, & Wagenmakers, 2014)8…Vamos para a definição estatística do idealizador dos intervalos de confiança, Jerzy Neyman, em 1937 (Neyman, 1937):\n\n“Um intervalo de confiança de X% para um parâmetro é um intervalo (a, b) gerado por um procedimento que em amostragem repetida tem uma probabilidade de X% de conter o valor verdadeiro do parâmetro, para todos os valores possíveis do parâmetro.”9 (Neyman, 1937)\n\nMais uma vez vamos quebrar essa definição em em algumas partes para melhor compreensão:\n“… intervalo (a,b) …”: intervalo de confiança sempre serão expressados como um intervalo \\(a\\) - \\(b\\), onde \\(a\\) é menor que \\(b\\) (\\(a < b\\)).\n“… gerado por um procedimento que em amostragem repetida…”: aqui estamos falando de população. E o que você geralmente tem nas suas mãos quando está fazendo uma análise estatística é uma amostra. Uma população é um conjunto de pessoas, itens ou eventos sobre os quais você quer fazer inferências. Uma amostra é um é um subconjunto de pessoas, itens ou eventos de uma população maior que você coleta e analisa para fazer inferências. Geralmente o tamanho da amostra é bem menor que o tamanho da população. Então, intervalos de confiança expressam a frequência de longo-prazo que vocês esperaria obter de um tamanho de efeito caso replicasse o teste estatístico para diversas amostras da MESMA população.\n“… tem uma probabilidade de X% de conter o valor verdadeiro do parâmetro, para todos os valores possíveis do parâmetro.”: os intervalos de confiança sempre serão expressados acompanhados de uma probabilidade (algo entre 0.001% e 99.999%) que quantifica a certeza de encontrar o intervalo em uma replicações do teste estatístico para diversas amostras da MESMA população.\nPor exemplo: digamos que você executou uma análise estatística para comparar eficácia de uma política pública em dois grupos e você obteve a diferença entre a média desses grupos. Você pode expressar essa diferença como um intervalo de confiança. Geralmente escolhemos a confiança de 95% (sim, está relacionado com o 0.05 do \\(p\\)-valor). Você então escreve no seu artigo que a “diferença entre grupos observada é de 10.5 - 23.5 (95% IC).” Isso quer dizer que 95 estudos de 100, que usem o mesmo tamanho de amostra e população-alvo, aplicando o mesmo teste estatístico, esperarão encontrar um resultado de diferenças de média entre grupos entre 10.5 e 23.5. Aqui as unidades são arbitrárias, mas para continuar o exemplo vamos supor que sejam espectativa de vida.\nFalácias\nEm um artigo bem controverso, Morey, Hoekstra, Rouder, Lee, & Wagenmakers (2016) mostram as três grandes falácias (qualquer enunciado ou raciocínio falso que entretanto simula a veracidade) dos intervalos de confiança (a tradução é livre e feita por nós):\nA falácia fundamental dos intervalos de confiança: Um intervalo de confiança de X% para um parâmetro é um intervalo (a, b) gerado por um procedimento que na amostragem repetida tem uma probabilidade de X% de conter o valor verdadeiro do parâmetro, para todos os valores possíveis do parâmetro. probabilidade de que um intervalo aleatório contém o valor verdadeiro é X%, então a plausibilidade ou probabilidade de que um determinado intervalo observado contém o valor verdadeiro também é X%; ou, alternativamente, podemos ter X% de confiança de que o intervalo observado contém o valor real10.\nA falácia da precisão: A largura de um intervalo de confiança indica a precisão de nosso conhecimento sobre o parâmetro. Intervalos de confiança estreitos correspondem a conhecimentos precisos, enquanto erros de confiança amplos correspondem a conhecimentos imprecisos11.\nA falácia da probabilidade: Um intervalo de confiança contém os valores prováveis para o parâmetro. Os valores dentro do intervalo de confiança são mais prováveis do que os externos. Essa falácia existe em várias variedades, às vezes envolvendo plausibilidade, credibilidade ou razoabilidade de crenças sobre o parâmetro12.\nNote que todas essas três falácias estão erradas e são uma compreensão errônea ou incompleta de intervalos de confiança.\nRelação entre intervalos de confiança e \\(p\\)-valores\nIntervalos de confiança estão profundamente relacionados com \\(p\\)-valores. Primeiro, para que uma estimativa tenha um \\(p\\)-valor menor que 0.05, seu intervalo de confiança 95% não pode capturar o zero. Ou seja, o intervalo não pode compreender o efeito nulo (Hipótese Nula - \\(H_0\\)). Isso segue para outros valores de \\(p\\) correspondentes com outros níveis de confiança dos intervalos. Por exemplo, para uma estimativa com \\(p\\)-valor menor que 0.01, seu intervalo de confiança 99% não pode capturar o 0. Além disso, intervalos de confiança (assim como \\(p\\)-valores) estão intrinsicamente conectados com o tamanho da amostra. Quanto maior o tamanho de amostra, mais estreito será o intervalo de confiança. A intuição por trás disso é que conforme a sua amostra aumenta, também aumentarão a sua confiança e precisão em inferências sobre a população-alvo. Por fim, intervalos de confiança (assim como \\(p\\)-valores) não falam nada sobre a sua teoria ou hipótese, mas sobre a relação dos seus dados (amostra) com a população-alvo. Eles não são a probabilidade do parâmetro estimado (\\(P(\\text{parâmetro} | D)\\), no nosso exemplo diferença entre médias de grupos), mas sim a probabilidade de amostras com o mesmo parâmetro estimado (\\(P(D | \\text{parâmetro})\\)).\nUma boa maneira de resumir \\(p\\)-valores e intervalos de confiança é a seguinte:\n\nConsidere \\(p\\)-valores algo que mensura a possibilidade de existir um efeito ou não e intervalos de confiança quantificam o tamanho desse efeito.\n\n\nMas sempre se atente nas definições. Lembre-se que se tentarmos ser intuitivos com \\(p\\)-valores e intervalos de confiança não seremos precisos nas definições.\nSignificância Estatística vs Significância Prática\n\nConsidere isso uma introdução rápida à \\(p\\)-hacking.\nPara encerrar esse tour de \\(p\\)-valores e intervalos de confiança, temos que nos atentar que significância estatística não é a mesma coisa que significância prática. Significância estatística é se algum achado de um teste/modelo estatístico diverge o suficiente da hipótese nula e, sendo que hipótese nula sempre são sobre efeitos ou diferenças nulas, podemos afirmar que significância estatística quer dizer um achado é diferente de um efeito nulo. Diversos testes da Estatística inferencial clássica quando submetidos à amostras grandes13 vão detectar uma diferença significante, mesmo que praticamente insignificante. Com uma amostra suficientemente grande nós conseguimos gerar \\(p\\)-valores significantes para diferenças minúsculas, como por exemplo uma diferença de 0.01cm altura entre dois grupos de uma amostra.\nPor isso que defendemos que nunca se interprete análises estatísticas somente com \\(p\\)-valores, mas sempre em conjunto com os intervalos de confiança que quantificam o tamanho do efeito. Nunca gere argumentos sobre evidências somente a partir de significância estatística, sempre inclua tamanho do efeito.\nErro Tipo I e Erro Tipo II\nNa Estatística inferencial temos dois erros possíveis quando estamos realizando um teste estatístico contra uma hipótese nula.\nErro tipo I, também chamado de “falso positivo”, é a chance de rejeitarmos a hipótese nula quando ela é verdadeira. Esse erro é o alpha \\(\\alpha\\) que é usado como limiar de significância do \\(p\\)-valor.\nErro tipo II, também chamado de “falso negativo”, é a chance de não rejeitarmos a hipótese nula quando ela é falsa. Esse erro é identificado como a letra grega beta \\(\\beta\\). Além disso, o poder de um teste estatístico é mensurado como \\(1 - \\beta\\). O poder de um teste estatístico aumenta proporcionalmente ao tamanho amostral. Quanto maior a amostra, maior o poder do teste.\n\nEsses conceitos foram criados por matemáticos, então a nomenclatura erro tipo I e erro tipo II é perfeita matematicamente, pois no contexto de testes estatísticos contra uma hipótese nula só existem dois tipos de erros. Mas para o ensino da Estatística e comunicação de incertezas é péssima. Sempre que possível optamos por usar termos como “falso positivo” e “falso negativo” ao invés de erro tipo I e erro tipo II.\n\n\n\nPor questões históricas, o erro tipo I14 foi considerado mais importante de ser controlado do que o erro tipo II. Portanto, quase todos os testes de hipótese nula focam no controle dos “falsos positivos” enquanto o controle dos “falsos negativos” são colocados em segundo plano. No mundo ideal, tanto \\(\\alpha\\) quando \\(\\beta\\) devem ser reduzidos o máximo possível. Isto requer um tamanho amostral frequentemente maior do que os recursos disponíveis para o pesquisador, portanto é comum pesquisadores usarem um \\(\\alpha\\) de 5% e um \\(\\beta\\) de 20% (poder de 80%).\nTamanho da Amostra\nA maioria dos testes estatísticos que computam um \\(p\\)-valor são extremamente sensíveis a tamanho da amostra. A hipótese nula sempre representa a ausência de qualquer efeito e nunca a diferença observada na amostra é igual a zero. Sempre há algum digito, menor que seja, que faz com que a diferença seja diferente de zero, ex: 0.00001. Quanto maior o tamanho da amostra maior a probabilidade de obtermos um \\(p\\)-valor significante, pois ele indica que o efeito é diferente de zero, mesmo que essa diferença seja insignificante do ponto de vista prático. Em certos contextos, defendemos que o \\(p\\)-valor é uma aproximação (proxy) de tamanho da amostra.\n\n\n\nE aonde entra a Estatística Bayesiana?\nA Estatística Frequentista15 se baseia em realizarmos testes de hipóteses sempre comparando os dados disponíveis com um cenário hipótetico de efeito nulo – \\(H_0\\):\n\\[P(D | H_0)\\]\nO resultado dessa probabilidade é o \\(p\\)-valor: a probabilidade dos dados obtidos condicionado à hipótese nula ser verdadeira. E se quisermos a probabilidade da hipótese nula16 e não dos dados obtidos?\nPara isso temos que “inverter” a probabilidade. Estamos interessados em:\n\\[P(H_0 | D)\\] Isso somente pode ser feito com o teorema de Bayes. Generalizando de \\(H_0\\) para qualquer \\(H\\), o teorema de Bayes nos permite “inverter” a probabilidade condicional:\n\\[P(H | D)=\\frac{P(H) \\cdot P(D | H)}{P(D)}\\] Aqui temos as seguintes probabilidades (note que podemos trocar \\(H\\) aqui para qualquer parâmetro \\(\\theta\\)):\n\\(P(H|D)\\) – probabilidade posterior de \\(H\\) depois de observamos os dados \\(D\\).\n\\(P(H)\\) – probabilidade prévia de \\(H\\) antes de observarmos os dados \\(D\\).\n\\(P(D|H)\\) – probabilidade dos dados obtidos sob a hipótese \\(H\\), também chamada de verossimilhança (do inglês likelihood).\n\\(P(D)\\) – chamada de evidência ou verossimilhança marginal (do inglês marginal likelihood), é a probabilidade geral dos dados de acordo com o modelo, determinada pela média de todos os valores de hipóteses ou paramêtros possíveis ponderados pela força da crença nesses valores de hipóteses ou parâmetros. Para hipóteses valores discretos de parâmetros: \\(P(D) = \\sum_\\theta P(D|H_0) P(H_0)\\). Já para valores contínuos de parâmetros: \\(P(D) = \\int_\\theta P(D|\\theta) P(\\theta) d \\theta\\). Em outras palavras, tome a probabilidade média \\(P(D|\\theta)\\) em todos os valores de \\(\\theta\\), ponderada pela probabilidade anterior de \\(\\theta\\) - \\(P(\\theta)\\). A única função de \\(P(D)\\) é garantir que a probabilidade posterior \\(P(H|D)\\) seja válida (algo entre 0 e 1).\nPortanto, a Estatística Bayesiana é qualquer técnica inferencial caracterizada pelo uso de informação prévia embutida como probabilidade prévia \\(P(H)\\). Nós não usamos \\(p\\)-valores nem intervalo de confiança, pois o conceito de hipótese nula é inexistente. Você pode especificar qualquer hipótese que queria, não necessariamente uma hipótese nula. Aqui temos o conceitos de probabilidade posterior de uma hipótese ou parâmetro ao invés de \\(p\\)-valores e também o conceito de intervalos de credibilidade que, ao invés de intervalos de confiança, nos dão a probabilidade de um parâmetro estar entre um intervalo de valores (muito mais intuitivo e simples de usar que um intervalo de confiança).\nComentários Finais\nSim, \\(p\\)-valores, intervalos de confiança, hipóteses nulas são conceitos complexos e muitos pesquisadores e cientistas não possuem a compreensão mínima necessária para a prática de Estatística inferencial. Acreditamos que a ciência (e a sociedade como um todo) se beneficiará de um maior número de cidadãos e pesquisadores que consigam avaliar, quantificar e qualificar evidências científicas. O paradigma da evidência científica atual (e, acreditamos que perdurará assim por bastante tempo) é o NHST e, apesar de termos algumas alternativas – como a Estatística Bayesiana – NHST irá predominar em boa parte da ciência pelas próximas décadas. Por isso, caro leitor, saiba que com “grandes poderes, vêm grandes responsabilidades.” Não deixe alguém torturar dados em práticas anti-éticas de \\(p\\)-hacking ou fundamentarem seus argumentos em compreensões incorretas de \\(p\\)-valor e \\(H_0\\).\n\n\n\nAmbiente\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] DiagrammeR_1.0.6.1\n\nloaded via a namespace (and not attached):\n [1] visNetwork_2.0.9   fansi_0.4.2        digest_0.6.27     \n [4] jsonlite_1.7.2     magrittr_2.0.1     evaluate_0.14     \n [7] highr_0.8          rlang_0.4.10       stringi_1.5.3     \n[10] rstudioapi_0.13    vctrs_0.3.6        rmarkdown_2.6     \n[13] distill_1.2        RColorBrewer_1.1-2 tools_4.0.3       \n[16] stringr_1.4.0      htmlwidgets_1.5.3  glue_1.4.2        \n[19] xfun_0.21          yaml_2.2.1         parallel_4.0.3    \n[22] compiler_4.0.3     htmltools_0.5.1.1  knitr_1.31        \n[25] downlit_0.2.1     \n\n\n\n\nBaird, D. (1983). The fisher/pearson chi-squared controversy: A turning point for inductive inference. The British Journal for the Philosophy of Science, 34(2), 105–118. Retrieved from http://www.jstor.org/stable/687444\n\n\nCobb, G. W. (2007). The introductory statistics course: A ptolemaic curriculum? Technology Innovations in Statistics Education, 1(1).\n\n\nCumming, G. (2009). Inference by eye: Reading the overlap of independent confidence intervals. Statistics in Medicine, 28(2), 205–220.\n\n\nDowney, A. (2016). Probably overthinking it: There is still only one test. Retrieved from http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html\n\n\nFisher, R. A. (1925). Statistical methods for research workers. Oliver; Boyd.\n\n\nHead, M. L., Holman, L., Lanfear, R., Kahn, A. T., & Jennions, M. D. (2015). The extent and consequences of p-hacking in science. PLoS Biol, 13(3), e1002106.\n\n\nHoekstra, R., Morey, R. D., Rouder, J. N., & Wagenmakers, E.-J. (2014). Robust misinterpretation of confidence intervals. Psychonomic Bulletin & Review, 21(5), 1157–1164. https://doi.org/10.3758/s13423-013-0572-3\n\n\nIoannidis, J. P. A. (2019). What Have We (Not) Learnt from Millions of Scientific Papers with <i>P<\/i> Values? The American Statistician, 73(sup1), 20–25. https://doi.org/10.1080/00031305.2018.1447512\n\n\nMorey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The fallacy of placing confidence in confidence intervals. Psychonomic Bulletin & Review, 23(1), 103–123. https://doi.org/10.3758/s13423-015-0947-8\n\n\nNeyman, J. (1937). Outline of a theory of statistical estimation based on the classical theory of probability. Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences, 236(767), 333–380.\n\n\nNeyman, J., & Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706), 289–337.\n\n\nRosnow, R. L., & Rosenthal, R. (1989). Statistical procedures and the justification of knowledge in psychological science. American Psychologist, 44, 1276–1284.\n\n\nStigler, S. M., & others. (2007). The epic story of maximum likelihood. Statistical Science, 22(4), 598–620.\n\n\nMatemático inglês que viveu entre 1857-1936. Considerado o fundador do campo da Estatística.↩︎\nem especial as técnicas clássicas/frequentistas de Estatística inferencial.↩︎\nTeoricamente não precisamos da hipótese nula se, no passo 2, simulássemos e permutássemos valores da amostra para calcular um \\(\\delta_0\\) (é provado matematicamente que se gerarmos amostras e permutações simuladas o suficiente, conseguiremos ter um \\(\\delta_0\\) no mínimo tão verídico que a abordagem clássica) ao invés de nos embasarmos em uma aproximação numérica pré-estabelecida de \\(\\delta_0\\). É claro que todas essas permutações e simulações são computacionalmente intensas.↩︎\nInclusive muitos renomados e citados em abundância em suas áreas.↩︎\nCuidado com essa palavra. Ela é precisa e somente deve ser usada em contextos estatísticos. Significância estatística quer dizer que os dados observados são mais extremos que um alpha prédefinido de que a hipótese nula é verdadeira.↩︎\nEsse conselho é extremamente útil. Por diversas vezes temos alunos que nos procuram com uma pergunta mais ou menos assim: “Professor, o que é o teste de Sobrenome que nunca ouvi falar na minha vida hífen outro sobrenome ainda mais estranho?” Graças a Wikipedia e Google, nós simplesmente vamos atrás da \\(H_0\\) desse teste (busca Google: “sobrenome1-sobrenome2 null hypothesis”) e com isso conseguimos responder ao aluno.↩︎\nA controvérsia da personalidade e vida de Ronald Fisher merece uma nota de rodapé. Suas contribuições, sem dúvida, foram cruciais para o avanço da ciência e da estatística. Seu intelecto era brilhante e seu talento já floresceu jovem: antes de completar 33 anos de idade ele tinha proposto o método de estimação por máxima verossimilhança (maximum likelihood estimation) (Stigler & others, 2007) e também criou o conceito de graus de liberdade (degrees of freedom) ao propor uma correção no teste de chi-quadrado de Pearson (Baird, 1983). Também inventou a Análise de Variância (ANOVA) e foi o primeiro a propor randomização como uma maneira de realizar experimentos, sendo considerado o “pai” dos ensaios clínicos randomizados. Nem tudo é florido na vida de Fisher, ele foi um eugenista e possuía uma visão muito forte sobre etnia e raça preconizando a superioridade de certas etnias. Além disso, era extremamente invariante, perseguindo, prejudicando e debochando qualquer crítico à suas teorias e publicações. O que vemos hoje no monopólio do paradigma Neyman-Pearson (Neyman & Pearson, 1933) com \\(p\\)-valores e hipóteses nulas é resultado desse esforço Fisheriano em calar os críticos e deixar apenas sua voz ecoar.↩︎\ninclusive muitos professores de estatística, veja a referência↩︎\nOriginal em ingles: “An X% confidence interval for a parameter is an interval (a, b) generated by a procedure that in repeated sampling has an X% probability of containing the true value of the parameter, for all possible values of the parameter.”↩︎\nOriginal em inglês: If the probability that a random interval contains the true value is X%, then the plausibility or probability that a particular observed interval contains the true value is also X%;or, alternatively, we can have X% confidence that the observed interval contains the true value.↩︎\nOriginal em inglês: The width of a confidence interval indicates the precision ofour knowledge about the parameter. Narrow confidence intervals correspond to precise knowledge, while wide confidence errors correspond to imprecise knowledge.↩︎\nOriginal em inglês: A confidence interval contains the likely values for the parameter. Values inside the confidence interval are more likely than those outside. This fallacy exists in several varieties, sometimes involving plausibility, credibility, or reasonableness of beliefs about the parameter.↩︎\nO que é muito comum em 2020s com o advento de Big Data e facilidade de obtenção de dados.↩︎\nJerzy Newman, fundador do paradigma NHST, e criador dos erros tipo I e tipo II defendia a ideia de que é melhor absolver um culpado (erro tipo II) do que culpar um inocente (erro tipo I).↩︎\ntambém chamada de Estatística Clássica↩︎\nou de maneira geral qualquer hipótese ou paramêtro estimado↩︎\n",
      "last_modified": "2021-02-12T06:47:43-03:00"
    }
  ],
  "collections": []
}

@article{ioannidis2005most,
  title={Why most published research findings are false},
  author={Ioannidis, John PA},
  journal={PLoS medicine},
  volume={2},
  number={8},
  pages={e124},
  year={2005},
  publisher={Public Library of Science}
}
@book{ritchie2020science,
  title={Science fictions: Exposing fraud, bias, negligence and hype in science},
  author={Ritchie, Stuart},
  year={2020},
  publisher={Random House}
}
@inproceedings{saculinggan2013empirical,
  title={Empirical power comparison of goodness of fit tests for normality in the presence of outliers},
  author={Saculinggan, Mayette and Balase, Emily Amor},
  booktitle={Journal of Physics: Conference Series},
  volume={435},
  number={1},
  pages={012041},
  year={2013},
  organization={IOP Publishing}
}
@article{cumming2009inference,
  title={Inference by eye: reading the overlap of independent confidence intervals},
  author={Cumming, Geoff},
  journal={Statistics in medicine},
  volume={28},
  number={2},
  pages={205--220},
  year={2009},
  publisher={Wiley Online Library}
}
@article{Ioannidis2019,
author = {Ioannidis, John P. A.},
doi = {10.1080/00031305.2018.1447512},
file = {:Users/storopoli/Documents/Mendeley Desktop/Ioannidis - 2019 - What Have We (Not) Learnt from Millions of Scientific Papers with iPi Values.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
mendeley-groups = {Statistics,Disciplina de M{\'{e}}todos Quanti,Disciplina de M{\'{e}}todos Quanti/B{\'{a}}sicos},
month = {mar},
number = {sup1},
pages = {20--25},
title = {{What Have We (Not) Learnt from Millions of Scientific Papers with {\textless}i{\textgreater}P{\textless}/i{\textgreater} Values?}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1447512},
volume = {73},
year = {2019}
}
@article{Baird1983,
 ISSN = {00070882, 14643537},
 URL = {http://www.jstor.org/stable/687444},
 author = {Davis Baird},
 journal = {The British Journal for the Philosophy of Science},
 number = {2},
 pages = {105--118},
 publisher = {[Oxford University Press, The British Society for the Philosophy of Science]},
 title = {The Fisher/Pearson Chi-Squared Controversy: A Turning Point for Inductive Inference},
 volume = {34},
 year = {1983}
}
@article{stigler2007epic,
  title={The epic story of maximum likelihood},
  author={Stigler, Stephen M and others},
  journal={Statistical Science},
  volume={22},
  number={4},
  pages={598--620},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}
@book{fisher1925statistical,
  title={Statistical methods for research workers},
  author={Fisher, Ronald Aylmer},
  year={1925},
  publisher={Oliver and Boyd}
}
@article{head2015extent,
  title={The extent and consequences of p-hacking in science},
  author={Head, Megan L and Holman, Luke and Lanfear, Rob and Kahn, Andrew T and Jennions, Michael D},
  journal={PLoS Biol},
  volume={13},
  number={3},
  pages={e1002106},
  year={2015},
  publisher={Public Library of Science}
}
@article{rosnow1989statistical,
  title={Statistical procedures and the justification of knowledge in psychological science},
  author={Rosnow, Ralph L and Rosenthal, Robert},
  journal={American Psychologist},
  volume={44},
  pages={1276--1284},
  year={1989},
  publisher={American Psychological Association (PsycARTICLES)}
}
@article{Morey2016,
author = {Morey, Richard D. and Hoekstra, Rink and Rouder, Jeffrey N. and Lee, Michael D. and Wagenmakers, Eric-Jan},
doi = {10.3758/s13423-015-0947-8},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
keywords = {Bayesian inference and parameter estimation,Bayesian statistics,Statistical inference,Statistics},
mendeley-groups = {Statistics Bayesian},
month = {feb},
number = {1},
pages = {103--123},
publisher = {Springer New York LLC},
title = {{The fallacy of placing confidence in confidence intervals}},
url = {http://link.springer.com/10.3758/s13423-015-0947-8},
volume = {23},
year = {2016}
}
@manual{downey2016,
 title  = "Probably Overthinking It: There is still only one test",
 author = "Allen Downey",
 url    = "http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html",
 year   = "2016"
}
@article{cobb2007introductory,
  title={The introductory statistics course: A Ptolemaic curriculum?},
  author={Cobb, George W},
  journal={Technology innovations in statistics education},
  volume={1},
  number={1},
  year={2007}
}
@article{Hoekstra2014,
author = {Hoekstra, Rink and Morey, Richard D and Rouder, Jeffrey N and Wagenmakers, Eric-Jan},
doi = {10.3758/s13423-013-0572-3},
issn = {1531-5320},
journal = {Psychonomic Bulletin {\&} Review},
mendeley-groups = {Statistics Bayesian},
number = {5},
pages = {1157--1164},
title = {{Robust misinterpretation of confidence intervals}},
url = {https://doi.org/10.3758/s13423-013-0572-3},
volume = {21},
year = {2014}
}
@article{neyman1937outline,
  title={Outline of a theory of statistical estimation based on the classical theory of probability},
  author={Neyman, Jerzy},
  journal={Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences},
  volume={236},
  number={767},
  pages={333--380},
  year={1937},
  publisher={The Royal Society London}
}
@article{neyman1933,
  title={On the problem of the most efficient tests of statistical hypotheses},
  author={Neyman, Jerzy and Pearson, Egon Sharpe},
  journal={Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume={231},
  number={694-706},
  pages={289--337},
  year={1933},
  publisher={The Royal Society London}
}
@Manual{postcards,
  title = {postcards: Create Beautiful, Simple Personal Websites},
  author = {Sean Kross},
  year = {2021},
  note = {R package version 0.2.0},
  url = {https://CRAN.R-project.org/package=postcards},
}
@Manual{vitae,
  title = {vitae: Curriculum Vitae for R Markdown},
  author = {Mitchell O'Hara-Wild and Rob Hyndman},
  year = {2021},
  note = {R package version 0.4.1},
  url = {https://CRAN.R-project.org/package=vitae},
}
@Book{gelman2013bayesian,
  title = {Bayesian {{Data Analysis}}},
  author = {Andrew Gelman and John B Carlin and Hal S Stern and David B Dunson and Aki Vehtari and Donald B Rubin},
  date = {2013},
  publisher = {{Chapman and Hall/CRC}},
}
@Book{mcelreath2020statistical,
  title = {Statistical Rethinking: {{A Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  author = {Richard McElreath},
  date = {2020},
  publisher = {{CRC press}},
}
@Book{gelman2020regression,
  title = {Regression and Other Stories},
  author = {Andrew Gelman and Jennifer Hill and Aki Vehtari},
  date = {2020},
  publisher = {{Cambridge University Press}},
}
@Article{benjaminRedefineStatisticalSignificance2018,
  title = {Redefine Statistical Significance},
  author = {Daniel J. Benjamin and James O. Berger and Magnus Johannesson and Brian A. Nosek and E.-J. Wagenmakers and Richard Berk and Kenneth A. Bollen and Bj{\"o}rn Brembs and Lawrence Brown and Colin Camerer and David Cesarini and Christopher D. Chambers and Merlise Clyde and Thomas D. Cook and Paul {De Boeck} and Zoltan Dienes and Anna Dreber and Kenny Easwaran and Charles Efferson and Ernst Fehr and Fiona Fidler and Andy P. Field and Malcolm Forster and Edward I. George and Richard Gonzalez and Steven Goodman and Edwin Green and Donald P. Green and Anthony G. Greenwald and Jarrod D. Hadfield and Larry V. Hedges and Leonhard Held and Teck {Hua Ho} and Herbert Hoijtink and Daniel J. Hruschka and Kosuke Imai and Guido Imbens and John P. A. Ioannidis and Minjeong Jeon and James Holland Jones and Michael Kirchler and David Laibson and John List and Roderick Little and Arthur Lupia and Edouard Machery and Scott E. Maxwell and Michael McCarthy and Don A. Moore and Stephen L. Morgan and Marcus Munaf{\a'o} and Shinichi Nakagawa and Brendan Nyhan and Timothy H. Parker and Luis Pericchi and Marco Perugini and Jeff Rouder and Judith Rousseau and Victoria Savalei and Felix D. Sch{\"o}nbrodt and Thomas Sellke and Betsy Sinclair and Dustin Tingley and Trisha {Van Zandt} and Simine Vazire and Duncan J. Watts and Christopher Winship and Robert L. Wolpert and Yu Xie and Cristobal Young and Jonathan Zinman and Valen E. Johnson},
  date = {2018-01-01},
  journaltitle = {Nature Human Behaviour},
  volume = {2},
  pages = {6--10},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0189-z},
  url = {http://www.nature.com/articles/s41562-017-0189-z},
  urldate = {2019-09-08},
  abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
  file = {/Users/storopoli/Zotero/storage/PA6HB5RL/Benjamin et al. - 2018 - Redefine statistical significance(2).pdf;/Users/storopoli/Zotero/storage/V8TY8Y5H/Benjamin et al. - 2018 - Redefine statistical significance.pdf},
  keywords = {★},
  number = {1},
}
@Article{carpenterStanProbabilisticProgramming2017,
  title = {Stan : {{A Probabilistic Programming Language}}},
  author = {Bob Carpenter and Andrew Gelman and Matthew D. Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
  date = {2017},
  journaltitle = {Journal of Statistical Software},
  volume = {76},
  publisher = {{American Statistical Association}},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  url = {http://www.jstatsoft.org/v76/i01/},
  urldate = {2019-10-29},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
  file = {/Users/storopoli/Zotero/storage/7SIEZ4SU/Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf},
  keywords = {Algorithmic differentiation,Bayesian inference,Probabilistic program,Stan},
  number = {1},
}
@Article{etzIntroductionConceptLikelihood2018,
  title = {Introduction to the {{Concept}} of {{Likelihood}} and {{Its Applications}}},
  author = {Alexander Etz},
  date = {2018-03-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {60--69},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245917744314},
  url = {https://doi.org/10.1177/2515245917744314},
  urldate = {2021-02-04},
  abstract = {This Tutorial explains the statistical concept known as likelihood and discusses how it underlies common frequentist and Bayesian statistical methods. The article is suitable for researchers interested in understanding the basis of their statistical tools and is also intended as a resource for teachers to use in their classrooms to introduce the topic to students at a conceptual level.},
  file = {/Users/storopoli/Zotero/storage/TSYEG9WA/Etz - 2018 - Introduction to the Concept of Likelihood and Its .pdf},
  keywords = {Bayes factor,Bayesian,estimation,frequentist,likelihood ratio,tutorial},
  langid = {english},
  number = {1},
}
@Article{etzHowBecomeBayesian2018,
  title = {How to Become a {{Bayesian}} in Eight Easy Steps: {{An}} Annotated Reading List},
  shorttitle = {How to Become a {{Bayesian}} in Eight Easy Steps},
  author = {Alexander Etz and Quentin F. Gronau and Fabian Dablander and Peter A. Edelsbrunner and Beth Baribault},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {219--234},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1317-5},
  url = {https://doi.org/10.3758/s13423-017-1317-5},
  urldate = {2021-02-04},
  abstract = {In this guide, we present a reading list to serve as a concise introduction to Bayesian data analysis. The introduction is geared toward reviewers, editors, and interested researchers who are new to Bayesian statistics. We provide commentary for eight recommended sources, which together cover the theoretical and practical cornerstones of Bayesian statistics in psychology and related sciences. The resources are presented in an incremental order, starting with theoretical foundations and moving on to applied issues. In addition, we outline an additional 32 articles and books that can be consulted to gain background knowledge about various theoretical specifics and Bayesian approaches to frequently used models. Our goal is to offer researchers a starting point for understanding the core tenets of Bayesian analysis, while requiring a low level of time commitment. After consulting our guide, the reader should understand how and why Bayesian methods work, and feel able to evaluate their use in the behavioral and social sciences.},
  file = {/Users/storopoli/Zotero/storage/PHLWSB4U/Etz et al. - 2018 - How to become a Bayesian in eight easy steps An a.pdf},
  langid = {english},
  number = {1},
}
@Book{brooksHandbookMarkovChain2011,
  title = {Handbook of {{Markov Chain Monte Carlo}}},
  author = {Steve Brooks and Andrew Gelman and Galin Jones and Xiao-Li Meng},
  date = {2011-05-10},
  publisher = {{CRC Press}},
  abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisherie},
  eprint = {qfRsAIKZ4rIC},
  eprinttype = {googlebooks},
  isbn = {978-1-4200-7942-5},
  keywords = {Mathematics / Probability & Statistics / General},
  langid = {english},
  pagetotal = {620},
}
@InCollection{geyer2011introduction,
  title = {Introduction to Markov Chain Monte Carlo},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  author = {Charles J Geyer},
  editor = {Steve Brooks and Andrew Gelman and Galin L. Jones and Xiao-Li Meng},
  date = {2011},
  file = {/Users/storopoli/Zotero/storage/N8ET8IFD/Geyer - 2011 - Introduction to markov chain monte carlo.pdf},
}
@Article{mcshaneAbandonStatisticalSignificance2019,
  title = {Abandon {{Statistical Significance}}},
  author = {Blakeley B. McShane and David Gal and Andrew Gelman and Christian Robert and Jennifer L. Tackett},
  date = {2019-03-29},
  journaltitle = {American Statistician},
  volume = {73},
  pages = {235--245},
  publisher = {{American Statistical Association}},
  issn = {15372731},
  doi = {10.1080/00031305.2018.1527253},
  abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm--and the p-value thresholds intrinsic to it--as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to {"}ban{"} p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
  file = {/Users/storopoli/Zotero/storage/ETT2EEP5/McShane et al. - 2019 - Abandon Statistical Significance.pdf},
  issue = {sup1},
  keywords = {★,Null hypothesis significance testing,p-Value,Replication,Sociology of science,Statistical significance},
}
@Article{amrheinScientistsRiseStatistical2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Valentin Amrhein and Sander Greenland and Blake McShane},
  date = {2019-03-21},
  journaltitle = {Nature},
  volume = {567},
  pages = {305--307},
  publisher = {{Nature Publishing Group}},
  issn = {14764687},
  doi = {10.1038/d41586-019-00857-9},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  eprint = {30894741},
  eprinttype = {pmid},
  file = {/Users/storopoli/Zotero/storage/PGP5KXFC/Amrhein, Greenland, McShane - 2019 - Scientists rise up against statistical significance.pdf},
  keywords = {Research data,Research management},
  number = {7748},
}
@Article{vanravenzwaaijSimpleIntroductionMarkov2018,
  title = {A Simple Introduction to {{Markov Chain Monte}}–{{Carlo}} Sampling},
  author = {Don {van Ravenzwaaij} and Pete Cassey and Scott D. Brown},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin and Review},
  volume = {25},
  pages = {143--154},
  publisher = {{Springer New York LLC}},
  issn = {15315320},
  doi = {10.3758/s13423-016-1015-8},
  abstract = {© 2016, The Author(s). Markov Chain Monte–Carlo (MCMC) is an increasingly popular method for obtaining information about distributions, especially for estimating posterior distributions in Bayesian inference. This article provides a very basic introduction to MCMC sampling. It describes what MCMC is, and what it can be used for, with simple illustrative examples. Highlighted are some of the benefits and limitations of MCMC sampling, as well as different approaches to circumventing the limitations most likely to trouble cognitive scientists.},
  file = {/Users/storopoli/Zotero/storage/MHB6FDY3/van Ravenzwaaij, Cassey, Brown - 2018 - A simple introduction to Markov Chain Monte–Carlo sampling.pdf},
  keywords = {★,Bayesian inference,Markov Chain Monte–Carlo,MCMC,Tutorial},
  number = {1},
  options = {useprefix=true},
}
@InCollection{vandekerckhove2015model,
  title = {Model Comparison and the Principle of Parsimony},
  booktitle = {Oxford Handbook of Computational and Mathematical Psychology},
  author = {Joachim Vandekerckhove and Dora Matzke and Eric-Jan Wagenmakers and {others}},
  editor = {Jerome R. Busemeyer and Zheng Wang and James T. Townsend and Ami Eidels},
  date = {2015},
  pages = {300--319},
  publisher = {{Oxford University Press Oxford}},
  file = {/Users/storopoli/Zotero/storage/GQUFF6S5/Vandekerckhove et al. - 2015 - Model comparison and the principle of parsimony.pdf},
  isbn = {978-0-19-995799-6},
  keywords = {★},
}
@Article{schootGentleIntroductionBayesian2014,
  title = {A {{Gentle Introduction}} to {{Bayesian Analysis}}: {{Applications}} to {{Developmental Research}}},
  shorttitle = {A {{Gentle Introduction}} to {{Bayesian Analysis}}},
  author = {Rens {van de Schoot} and David Kaplan and Jaap Denissen and Jens B. Asendorpf and Franz J. Neyer and Marcel A. G. {van Aken}},
  date = {2014},
  journaltitle = {Child Development},
  volume = {85},
  pages = {842--860},
  issn = {1467-8624},
  doi = {10.1111/cdev.12169},
  url = {https://srcd.onlinelibrary.wiley.com/doi/abs/10.1111/cdev.12169},
  urldate = {2021-02-04},
  abstract = {Bayesian statistical methods are becoming ever more popular in applied and fundamental research. In this study a gentle introduction to Bayesian analysis is provided. It is shown under what circumstances it is attractive to use Bayesian estimation, and how to interpret properly the results. First, the ingredients underlying Bayesian methods are introduced using a simplified example. Thereafter, the advantages and pitfalls of the specification of prior knowledge are discussed. To illustrate Bayesian methods explained in this study, in a second example a series of studies that examine the theoretical framework of dynamic interactionism are considered. In the Discussion the advantages and disadvantages of using Bayesian statistics are reviewed, and guidelines on how to report on Bayesian statistics are provided.},
  annotation = {\_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/cdev.12169},
  file = {/Users/storopoli/Zotero/storage/H53J5TDA/Schoot et al. - 2014 - A Gentle Introduction to Bayesian Analysis Applic.pdf},
  langid = {english},
  number = {3},
}

@Article{Wagenmakers2007,
  title = {A Practical Solution to the Pervasive Problems of p Values},
  author = {Eric-Jan Wagenmakers},
  date = {2007-10},
  journaltitle = {Psychonomic Bulletin \& Review},
  volume = {14},
  pages = {779--804},
  publisher = {{Psychonomic Society Inc.}},
  issn = {1069-9384},
  doi = {10.3758/BF03194105},
  url = {http://www.springerlink.com/index/10.3758/BF03194105},
  urldate = {2019-09-09},
  abstract = {The primary goal of this article is to promote awareness of the various statistical problems associated with the use of p value null-hypothesis significance testing (NHST). Making no claim of completeness, I review three prob-lems with NHST, briefly explaining their causes and con-sequences (see Karabatsos, 2006). The discussion of each problem is accompanied by concrete examples and refer-ences to the statistical literature. In the psychological literature, the pros and cons of NHST have been, and continue to be, hotly debated The issues that have dominated the NHST discussion in the psychological lit-erature are that (1) NHST tempts the user into confusing the probability of the hypothesis given the data with the probability of the data given the hypothesis; (2) (.05 is an arbitrary criterion for significance; and (3) in real-world applications, the null hypothesis is never exactly true, and will therefore always be rejected as the number of observations grows large. In the statistical literature, the pros and cons of NHST are also the topic of an ongoing dispute (e. A comparison of these two literatures shows that in psychology, the NHST discussion has focused mostly on problems of interpretation, whereas in statistics, the NHST discussion has focused mostly on problems of for-mal construction. The statistical perspective on the prob-lems associated with NHST is therefore fundamentally different from the psychological perspective. In this ar-ticle, the goal is to explain NHST and its problems from a statistical perspective. Many psychologists are oblivious to certain statistical problems associated with NHST, and the examples below show that this ignorance can have im-portant ramifications. In this article, I will show that an NHST p value de-pends on data that were never observed: The p value is a tail-area integral, and this integral is effectively over data that are not observed but only hypothesized. The prob-ability of these hypothesized data depends crucially on the possibly unknown subjective intentions of the researcher who carried out the experiment. If these intentions were to be ignored, a user of NHST could always obtain a sig-nificant result through optional stopping (i.e., analyzing the data as they accumulate and stopping the experiment whenever the p value reaches some desired significance level). In the context of NHST, it is therefore necessary to know the subjective intention with which an experi-ment was carried out. This key requirement is unattain-able in a practical sense, and arguably undesirable in a philosophical sense. In addition, I will review a proof that the NHST p value does not measure statistical evidence. In order for the p value to qualify as a measure of statisti-cal evidence, a minimum requirement is that identical p values convey identical levels of evidence, irrespective},
  file = {/Users/storopoli/Zotero/storage/T5R582FH/Wagenmakers - 2007 - A practical solution to the pervasive problems of p values.pdf},
  keywords = {★},
  number = {5},
}
@Article{cohenEarth051994,
  title = {The Earth Is Round (p {$<$} .05)},
  author = {Jacob Cohen},
  date = {1994},
  journaltitle = {American Psychologist},
  volume = {49},
  pages = {997--1003},
  publisher = {{American Psychological Association Inc.}},
  issn = {0003066X},
  doi = {10.1037/0003-066X.49.12.997},
  abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.},
  file = {/Users/storopoli/Zotero/storage/33N57CEV/Cohen - 1994 - The earth is round (p .05).pdf},
  keywords = {★},
  number = {12},
}

@Article{dienesBayesianOrthodoxStatistics2011,
  title = {Bayesian {{Versus Orthodox Statistics}}: {{Which Side Are You On}}?},
  shorttitle = {Bayesian {{Versus Orthodox Statistics}}},
  author = {Zoltan Dienes},
  date = {2011-05-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {6},
  pages = {274--290},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691611406920},
  url = {https://doi.org/10.1177/1745691611406920},
  urldate = {2021-02-04},
  abstract = {Researchers are often confused about what can be inferred from significance tests. One problem occurs when people apply Bayesian intuitions to significance testing—two approaches that must be firmly separated. This article presents some common situations in which the approaches come to different conclusions; you can see where your intuitions initially lie. The situations include multiple testing, deciding when to stop running participants, and when a theory was thought of relative to finding out results. The interpretation of nonsignificant results has also been persistently problematic in a way that Bayesian inference can clarify. The Bayesian and orthodox approaches are placed in the context of different notions of rationality, and I accuse myself and others as having been irrational in the way we have been using statistics on a key notion of rationality. The reader is shown how to apply Bayesian inference in practice, using free online software, to allow more coherent inferences from data.},
  file = {/Users/storopoli/Zotero/storage/3KISFEQS/Dienes - 2011 - Bayesian Versus Orthodox Statistics Which Side Ar.pdf},
  keywords = {Bayes,evidence,likelihood principle,significance testing,statistical inference},
  langid = {english},
  number = {3},
}

@Article{etzIntroductionBayesianInference2018,
  title = {Introduction to {{Bayesian Inference}} for {{Psychology}}},
  author = {Alexander Etz and Joachim Vandekerckhove},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {5--34},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1262-3},
  url = {https://doi.org/10.3758/s13423-017-1262-3},
  urldate = {2021-02-04},
  abstract = {We introduce the fundamental tenets of Bayesian inference, which derive from two basic laws of probability theory. We cover the interpretation of probabilities, discrete and continuous versions of Bayes’ rule, parameter estimation, and model comparison. Using seven worked examples, we illustrate these principles and set up some of the technical background for the rest of this special issue of Psychonomic Bulletin \& Review. Supplemental material is available via https://osf.io/wskex/.},
  file = {/Users/storopoli/Zotero/storage/MSQDFTM6/Etz and Vandekerckhove - 2018 - Introduction to Bayesian Inference for Psychology.pdf},
  langid = {english},
  number = {1},
}

@Article{junior2020vale,
  title = {Quanto Vale o Valor-p?},
  author = {Carlos Alberto Mour{\~a}o J{\a'u}nior},
  date = {2020},
  journaltitle = {Arquivos de Ciências do Esporte},
  volume = {7},
  number = {2},
}

@Article{kerrHARKingHypothesizingResults1998,
  title = {{{HARKing}}: {{Hypothesizing}} after the Results Are Known},
  author = {Norbert L. Kerr},
  date = {1998},
  journaltitle = {Personality and Social Psychology Review},
  volume = {2},
  pages = {196--217},
  publisher = {{SAGE Publications Inc.}},
  issn = {10888683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  file = {/Users/storopoli/Zotero/storage/PKLRS2YS/Kerr - 1998 - HARKing Hypothesizing after the results are known.pdf},
  keywords = {★},
  number = {3},
}

@InCollection{kruschke2015bayesian,
  title = {Bayesian Estimation in Hierarchical Models},
  booktitle = {The {{Oxford}} Handbook of Computational and Mathematical Psychology},
  author = {John K Kruschke and Wolf Vanpaemel},
  editor = {Jerome R. Busemeyer and Zheng Wang and James T. Townsend and Ami Eidels},
  date = {2015},
  pages = {279--299},
  publisher = {{Oxford University Press Oxford, UK}},
  file = {/Users/storopoli/Zotero/storage/MX88HPCI/Kruschke, Vanpaemel - 2015 - Bayesian estimation in hierarchical models.pdf},
  isbn = {978-0-19-995799-6},
  keywords = {★},
}

@Article{kruschkeBayesianDataAnalysis2018,
  title = {Bayesian Data Analysis for Newcomers},
  author = {John K. Kruschke and Torrin M. Liddell},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {155--177},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1272-1},
  url = {https://doi.org/10.3758/s13423-017-1272-1},
  urldate = {2021-02-04},
  abstract = {This article explains the foundational concepts of Bayesian data analysis using virtually no mathematical notation. Bayesian ideas already match your intuitions from everyday reasoning and from traditional data analysis. Simple examples of Bayesian data analysis are presented that illustrate how the information delivered by a Bayesian analysis can be directly interpreted. Bayesian approaches to null-value assessment are discussed. The article clarifies misconceptions about Bayesian methods that newcomers might have acquired elsewhere. We discuss prior distributions and explain how they are not a liability but an important asset. We discuss the relation of Bayesian data analysis to Bayesian models of mind, and we briefly discuss what methodological problems Bayesian data analysis is not meant to solve. After you have read this article, you should have a clear sense of how Bayesian data analysis works and the sort of information it delivers, and why that information is so intuitive and useful for drawing conclusions from data.},
  file = {/Users/storopoli/Zotero/storage/6NFCW2P8/Kruschke and Liddell - 2018 - Bayesian data analysis for newcomers.pdf},
  langid = {english},
  number = {1},
}

@Article{kruschkeBayesianNewStatistics2018,
  title = {The {{Bayesian New Statistics}}: {{Hypothesis}} Testing, Estimation, Meta-Analysis, and Power Analysis from a {{Bayesian}} Perspective},
  shorttitle = {The {{Bayesian New Statistics}}},
  author = {John K. Kruschke and Torrin M. Liddell},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {178--206},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1221-4},
  url = {https://doi.org/10.3758/s13423-016-1221-4},
  urldate = {2021-02-04},
  abstract = {In the practice of data analysis, there is a conceptual distinction between hypothesis testing, on the one hand, and estimation with quantified uncertainty on the other. Among frequentists in psychology, a shift of emphasis from hypothesis testing to estimation has been dubbed “the New Statistics” (Cumming 2014). A second conceptual distinction is between frequentist methods and Bayesian methods. Our main goal in this article is to explain how Bayesian methods achieve the goals of the New Statistics better than frequentist methods. The article reviews frequentist and Bayesian approaches to hypothesis testing and to estimation with confidence or credible intervals. The article also describes Bayesian approaches to meta-analysis, randomized controlled trials, and power analysis.},
  file = {/Users/storopoli/Zotero/storage/U3IMNGH5/Kruschke and Liddell - 2018 - The Bayesian New Statistics Hypothesis testing, e.pdf},
  langid = {english},
  number = {1},
}

@Article{lakensJustifyYourAlpha2018,
  title = {Justify Your Alpha},
  author = {Daniel Lakens and Federico G. Adolfi and Casper J. Albers and Farid Anvari and Matthew A.J. Apps and Shlomo E. Argamon and Thom Baguley and Raymond B. Becker and Stephen D. Benning and Daniel E. Bradford and Erin M. Buchanan and Aaron R. Caldwell and Ben {Van Calster} and Rickard Carlsson and Sau Chin Chen and Bryan Chung and Lincoln J. Colling and Gary S. Collins and Zander Crook and Emily S. Cross and Sameera Daniels and Henrik Danielsson and Lisa Debruine and Daniel J. Dunleavy and Brian D. Earp and Michele I. Feist and Jason D. Ferrell and James G. Field and Nicholas W. Fox and Amanda Friesen and Caio Gomes and Monica Gonzalez-Marquez and James A. Grange and Andrew P. Grieve and Robert Guggenberger and James Grist and Anne Laura {Van Harmelen} and Fred Hasselman and Kevin D. Hochard and Mark R. Hoffarth and Nicholas P. Holmes and Michael Ingre and Peder M. Isager and Hanna K. Isotalus and Christer Johansson and Konrad Juszczyk and David A. Kenny and Ahmed A. Khalil and Barbara Konat and Junpeng Lao and Erik Gahner Larsen and Gerine M.A. Lodder and Ji{\v r}{\a'\i} Lukavsk{\a`y} and Christopher R. Madan and David Manheim and Stephen R. Martin and Andrea E. Martin and Deborah G. Mayo and Randy J. McCarthy and Kevin McConway and Colin McFarland and Amanda Q.X. Nio and Gustav Nilsonne and Cilene Lino {De Oliveira} and Jean Jacques Orban {De Xivry} and Sam Parsons and Gerit Pfuhl and Kimberly A. Quinn and John J. Sakon and S. Adil Saribay and Iris K. Schneider and Manojkumar Selvaraju and Zsuzsika Sjoerds and Samuel G. Smith and Tim Smits and Jeffrey R. Spies and Vishnu Sreekumar and Crystal N. Steltenpohl and Neil Stenhouse and Wojciech {{\a'S}wi{\k a}tkowski} and Miguel A. Vadillo and Marcel A.L.M. {Van Assen} and Matt N. Williams and Samantha E. Williams and Donald R. Williams and Tal Yarkoni and Ignazio Ziano and Rolf A. Zwaan},
  date = {2018-03-01},
  journaltitle = {Nature Human Behaviour},
  volume = {2},
  pages = {168--171},
  publisher = {{Nature Publishing Group}},
  issn = {23973374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P ≤ 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  file = {/Users/storopoli/Zotero/storage/KUUEK8R3/Lakens et al. - 2018 - Justify your alpha.pdf},
  keywords = {★},
  number = {3},
}

@Article{murphyHARKingHowBadly2019,
  title = {{{HARKing}}: {{How Badly Can Cherry}}-{{Picking}} and {{Question Trolling Produce Bias}} in {{Published Results}}?},
  author = {Kevin R. Murphy and Herman Aguinis},
  date = {2019-02-15},
  journaltitle = {Journal of Business and Psychology},
  volume = {34},
  publisher = {{Springer New York LLC}},
  issn = {08893268},
  doi = {10.1007/s10869-017-9524-7},
  abstract = {© 2017 Springer Science+Business Media, LLC, part of Springer Nature The practice of hypothesizing after results are known (HARKing) has been identified as a potential threat to the credibility of research results. We conducted simulations using input values based on comprehensive meta-analyses and reviews in applied psychology and management (e.g., strategic management studies) to determine the extent to which two forms of HARKing behaviors might plausibly bias study outcomes and to examine the determinants of the size of this effect. When HARKing involves cherry-picking, which consists of searching through data involving alternative measures or samples to find the results that offer the strongest possible support for a particular hypothesis or research question, HARKing has only a small effect on estimates of the population effect size. When HARKing involves question trolling, which consists of searching through data involving several different constructs, measures of those constructs, interventions, or relationships to find seemingly notable results worth writing about, HARKing produces substantial upward bias particularly when it is prevalent and there are many effects from which to choose. Results identify the precise circumstances under which different forms of HARKing behaviors are more or less likely to have a substantial impact on a study’s substantive conclusions and the field’s cumulative knowledge. We offer suggestions for authors, consumers of research, and reviewers and editors on how to understand, minimize, detect, and deter detrimental forms of HARKing in future research.},
  file = {/Users/storopoli/Zotero/storage/U44AUQFK/Murphy, Aguinis - 2019 - HARKing How Badly Can Cherry-Picking and Question Trolling Produce Bias in Published Results.pdf},
  keywords = {★,Data snooping,HARKing,Publication bias,Simulation},
  number = {1},
}

@Article{starkCargocultStatisticsScientific2018,
  title = {Cargo-Cult Statistics and Scientific Crisis},
  author = {Philip B. Stark and Andrea Saltelli},
  date = {2018-08-01},
  journaltitle = {Significance},
  volume = {15},
  pages = {40--43},
  publisher = {{John Wiley \& Sons, Ltd (10.1111)}},
  issn = {17409705},
  doi = {10.1111/j.1740-9713.2018.01174.x},
  url = {http://doi.wiley.com/10.1111/j.1740-9713.2018.01174.x},
  urldate = {2019-08-01},
  file = {/Users/storopoli/Zotero/storage/LYJA9MIV/Stark, Saltelli - 2018 - Cargo-cult statistics and scientific crisis.pdf},
  keywords = {★},
  number = {4},
}
@Article{gabryVisualizationBayesianWorkflow2019,
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Jonah Gabry and Daniel Simpson and Aki Vehtari and Michael Betancourt and Andrew Gelman},
  date = {2019-02-01},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {182},
  pages = {389--402},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {09641998},
  doi = {10.1111/rssa.12378},
  url = {http://doi.wiley.com/10.1111/rssa.12378},
  urldate = {2019-09-16},
  abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
  file = {/Users/storopoli/Zotero/storage/H3J8LKVH/Gabry et al. - 2019 - Visualization in Bayesian workflow(2).pdf;/Users/storopoli/Zotero/storage/ZD32SJNY/Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf},
  keywords = {★,Bayesian data analysis,bayesplot,Statistical graphics,Statistical workflow},
  number = {2},
}

@Online{gelmanBayesianWorkflow2020,
  title = {Bayesian {{Workflow}}},
  author = {Andrew Gelman and Aki Vehtari and Daniel Simpson and Charles C. Margossian and Bob Carpenter and Yuling Yao and Lauren Kennedy and Jonah Gabry and Paul-Christian B{\"u}rkner and Martin Modr{\a'a}k},
  date = {2020-11-03},
  url = {http://arxiv.org/abs/2011.01808},
  urldate = {2021-02-04},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  eprint = {2011.01808},
  eprinttype = {arxiv},
  file = {/Users/storopoli/Zotero/storage/KBQP2U94/Gelman et al. - 2020 - Bayesian Workflow.pdf;/Users/storopoli/Zotero/storage/79ZD8TLD/2011.html},
  keywords = {Statistics - Methodology},
  primaryclass = {stat},
}
@Article{metropolisEquationStateCalculations1953,
  title = {Equation of {{State Calculations}} by {{Fast Computing Machines}}},
  author = {Nicholas Metropolis and Arianna W. Rosenbluth and Marshall N. Rosenbluth and Augusta H. Teller and Edward Teller},
  date = {1953-06-01},
  journaltitle = {The Journal of Chemical Physics},
  shortjournal = {J. Chem. Phys.},
  volume = {21},
  pages = {1087--1092},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.1699114},
  url = {https://aip.scitation.org/doi/abs/10.1063/1.1699114},
  urldate = {2021-02-07},
  file = {/Users/storopoli/Zotero/storage/8LHVAJF2/Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf;/Users/storopoli/Zotero/storage/4C9KZB7D/1.html},
  number = {6},
}
@Article{hastingsMonteCarloSampling1970,
  title = {Monte {{Carlo}} Sampling Methods Using {{Markov}} Chains and Their Applications},
  author = {W. K. Hastings},
  date = {1970-04-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {57},
  pages = {97--109},
  issn = {0006-3444},
  doi = {10.1093/biomet/57.1.97},
  url = {https://doi.org/10.1093/biomet/57.1.97},
  urldate = {2021-02-07},
  abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
  file = {/Users/storopoli/Zotero/storage/59BDV5WJ/Hastings - 1970 - Monte Carlo sampling methods using Markov chains a.pdf;/Users/storopoli/Zotero/storage/TP9SDKJJ/284580.html},
  number = {1},
}
@Article{robertsWeakConvergenceOptimal1997,
  title = {Weak Convergence and Optimal Scaling of Random Walk {{Metropolis}} Algorithms},
  author = {G. O. Roberts and A. Gelman and W. R. Gilks},
  date = {1997-02},
  journaltitle = {Annals of Applied Probability},
  shortjournal = {Ann. Appl. Probab.},
  volume = {7},
  pages = {110--120},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1050-5164, 2168-8737},
  doi = {10.1214/aoap/1034625254},
  url = {https://projecteuclid.org/euclid.aoap/1034625254},
  urldate = {2021-02-07},
  abstract = {This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to ∞∞\textbackslash infty. When the proposal variance is appropriately scaled according to n, the sequence of stochastic processes formed by the first component of each Markov chain converges to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straightforward efficiency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.},
  file = {/Users/storopoli/Zotero/storage/TY4KFNMY/Roberts et al. - 1997 - Weak convergence and optimal scaling of random wal.pdf;/Users/storopoli/Zotero/storage/ADGSPKZ4/1034625254.html},
  keywords = {Markov chain Monte Carlo,Metropolis algorithm,optimal scaling,weak convergence},
  langid = {english},
  mrnumber = {MR1428751},
  number = {1},
  zmnumber = {0876.60015},
}
@InCollection{gelmanBasicsMarkovChain2013,
  title = {Basics of {{Markov Chain Simulation}}},
  booktitle = {Bayesian {{Data Analysis}}},
  author = {Andrew Gelman and John B Carlin and Hal S Stern and David B Dunson and Aki Vehtari and Donald B Rubin},
  date = {2013},
  publisher = {{Chapman and Hall/CRC}},
}
@InProceedings{gelmanIterativeNonIterativeSimulation1992,
  title = {Iterative and {{Non}}-{{Iterative Simulation Algorithms}}},
  booktitle = {Computing {{Science}} and {{Statistics}} ({{Interface Proceedings}})},
  author = {Andrew Gelman},
  date = {1992},
  volume = {24},
  pages = {457--511},
  publisher = {{PROCEEDINGS PUBLISHED BY VARIOUS PUBLISHERS}},
}
@article{chibUnderstandingMetropolisHastingsAlgorithm1995,
  title = {Understanding the {{Metropolis}}-{{Hastings Algorithm}}},
  author = {Chib, Siddhartha and Greenberg, Edward},
  year = {1995},
  month = nov,
  volume = {49},
  pages = {327--335},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1995.10476177},
  journal = {The American Statistician},
  number = {4}
}
@article{casellaExplainingGibbsSampler1992,
	title = {Explaining the Gibbs Sampler},
	author = {{Casella}, {George} and {George}, {Edward I.}},
	year = {1992},
	month = {08},
	date = {1992-08-01},
	journal = {The American Statistician},
	pages = {167--174},
	volume = {46},
	number = {3},
	doi = {10.1080/00031305.1992.10475878},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1992.10475878},
	note = {Publisher: Taylor & Francis
{\_}eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1992.10475878}
}
@Article{gemanStochasticRelaxationGibbs1984,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {S. Geman and D. Geman},
  date = {1984-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-6},
  pages = {721--741},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1984.4767596},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (“annealing”), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel “relaxation” algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/Users/storopoli/Zotero/storage/MI46XZV3/4767596.html},
  keywords = {Additive noise,Annealing,Bayesian methods,Deformable models,Degradation,Energy states,Gibbs distribution,image restoration,Image restoration,line process,MAP estimate,Markov random field,Markov random fields,relaxation,scene modeling,spatial degradation,Stochastic processes,Temperature distribution},
  number = {6},
}
@Article{nealImprovedAcceptanceProcedure1994,
  title = {An {{Improved Acceptance Procedure}} for the {{Hybrid Monte Carlo Algorithm}}},
  author = {Radford M. Neal},
  date = {1994-03-01},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {111},
  pages = {194--203},
  issn = {0021-9991},
  doi = {10.1006/jcph.1994.1054},
  url = {https://www.sciencedirect.com/science/article/pii/S0021999184710540},
  urldate = {2021-02-07},
  abstract = {The probability of accepting a candidate move in the hybrid Monte Carlo algorithm can be increased by considering a transition to be between windows of several states at the beginning and end of the trajectory, with a particular state within the selected window then being chosen according to the Boltzmann probabilities. The detailed balance condition used to justify the algorithm still holds with this procedure, provided the start state is randomly positioned within its window. The new procedure is shown empirically to significantly improve the acceptance rate for a test system of uncoupled oscillators. It also allows expectations to be estimated using data from all states in the windows, rather than just states that are accepted.},
  file = {/Users/storopoli/Zotero/storage/CY4MA543/Neal - 1994 - An Improved Acceptance Procedure for the Hybrid Mo.pdf;/Users/storopoli/Zotero/storage/ZTHWBW5C/S0021999184710540.html},
  langid = {english},
  number = {1},
}
@Article{duaneHybridMonteCarlo1987,
  title = {Hybrid {{Monte Carlo}}},
  author = {Simon Duane and A. D. Kennedy and Brian J. Pendleton and Duncan Roweth},
  date = {1987-09-03},
  journaltitle = {Physics Letters B},
  shortjournal = {Physics Letters B},
  volume = {195},
  pages = {216--222},
  issn = {0370-2693},
  doi = {10.1016/0370-2693(87)91197-X},
  url = {https://www.sciencedirect.com/science/article/pii/037026938791197X},
  urldate = {2021-02-07},
  abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
  file = {/Users/storopoli/Zotero/storage/RM6G52EC/037026938791197X.html},
  langid = {english},
  number = {2},
}
@InCollection{neal2011mcmc,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  author = {Radford M Neal},
  editor = {Steve Brooks and Andrew Gelman and Galin L. Jones and Xiao-Li Meng},
  date = {2011},
  file = {/Users/storopoli/Zotero/storage/TVZD8FF2/Neal - 2011 - MCMC using Hamiltonian dynamics.pdf},
}
@Online{betancourtConceptualIntroductionHamiltonian2017,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Michael Betancourt},
  date = {2017-01-09},
  url = {http://arxiv.org/abs/1701.02434},
  urldate = {2019-11-06},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  file = {/Users/storopoli/Zotero/storage/9F746FH9/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Carlo.pdf},
}
@Article{hoffman2014no,
  title = {The {{No}}-{{U}}-{{Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  author = {Matthew D Hoffman and Andrew Gelman},
  date = {2011-11-17},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  pages = {1593--1623},
  url = {http://arxiv.org/abs/1111.4246},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{\textbackslash epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{\textbackslash epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient {"}turnkey{"} sampling algorithms.},
  archiveprefix = {arXiv},
  eprint = {1111.4246},
  eprinttype = {arxiv},
  file = {/Users/storopoli/Zotero/storage/7X834QV4/Hoffman, Gelman - 2011 - The No-U-Turn Sampler Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.pdf},
  number = {1},
}
@Article{brooksGeneralMethodsMonitoring1998,
  title = {General {{Methods}} for {{Monitoring Convergence}} of {{Iterative Simulations}}},
  author = {Stephen P. Brooks and Andrew Gelman},
  date = {1998-12-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {null},
  volume = {7},
  pages = {434--455},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.1998.10474787},
  url = {https://amstat.tandfonline.com/doi/abs/10.1080/10618600.1998.10474787},
  urldate = {2021-02-07},
  abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
  file = {/Users/storopoli/Zotero/storage/IANAIQHZ/Brooks and Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf;/Users/storopoli/Zotero/storage/UDD3H9VR/10618600.1998.html},
  number = {4},
}
@Article{gelmanInferenceIterativeSimulation1992,
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  author = {Andrew Gelman and Donald B. Rubin},
  date = {1992-11},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {7},
  pages = {457--472},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177011136},
  url = {https://projecteuclid.org/euclid.ss/1177011136},
  urldate = {2021-02-07},
  abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
  file = {/Users/storopoli/Zotero/storage/YU6A4XAH/Gelman and Rubin - 1992 - Inference from Iterative Simulation Using Multiple.pdf;/Users/storopoli/Zotero/storage/T3TPTHVF/1177011136.html},
  keywords = {Bayesian inference,convergence of stochastic processes,ECM,EM,Gibbs sampler,importance sampling,Metropolis algorithm,multiple imputation,random-effects model,SIR},
  langid = {english},
  number = {4},
  zmnumber = {06853057},
}
@Online{gelmanFolkTheoremStatistical2008,
  title = {The Folk Theorem of Statistical Computing},
  author = {Andrew Gelman},
  date = {2008},
  url = {https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/},
  file = {/Users/storopoli/Zotero/storage/L2PX7D4Q/the_folk_theore.html},
}
@Article{vandeschootBayesianStatisticsModelling2021,
  title = {Bayesian Statistics and Modelling},
  author = {Rens {van de Schoot} and Sarah Depaoli and Ruth King and Bianca Kramer and Kaspar M{\"a}rtens and Mahlet G. Tadesse and Marina Vannucci and Andrew Gelman and Duco Veen and Joukje Willemsen and Christopher Yau},
  date = {2021-01-14},
  journaltitle = {Nature Reviews Methods Primers},
  volume = {1},
  pages = {1--26},
  publisher = {{Nature Publishing Group}},
  issn = {2662-8449},
  doi = {10.1038/s43586-020-00001-2},
  url = {https://www.nature.com/articles/s43586-020-00001-2},
  urldate = {2021-02-15},
  abstract = {Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.},
  file = {/Users/storopoli/Zotero/storage/NL6PCGZY/van de Schoot et al. - 2021 - Bayesian statistics and modelling.pdf;/Users/storopoli/Zotero/storage/LPV57XR4/s43586-020-00001-2.html},
  issue = {1},
  langid = {english},
  number = {1},
  options = {useprefix=true},
}
@Book{definettiTheoryProbability1974,
  title = {Theory of {{Probability}}},
  shorttitle = {Theory of {{Probability}}},
  author = {Bruno {de Finetti}},
  date = {1974},
  edition = {Volume 1},
  publisher = {{John Wiley \& Sons}},
  location = {{New York}},
  abstract = {First issued in translation as a two-volume work in 1975, this classic book~provides the first complete development of the theory of probability from a subjectivist viewpoint. It proceeds from a detailed discussion of the philosophical mathematical aspects to a detailed mathematical treatment of probability and statistics. De Finetti’s theory of probability is one of the foundations of Bayesian theory. De Finetti stated that probability is nothing but a subjective analysis of the likelihood that something will happen and that that probability does not exist outside the mind. ~It is the rate at which a person is willing to bet on something happening. ~This view is directly opposed to the classicist/ frequentist view of the likelihood of a particular outcome of an event, which assumes that the same event could be identically repeated many times over, and the 'probability' of a particular outcome has to do with the fraction of the time that outcome results from the repeated trials.},
  isbn = {978-1-119-28637-0},
  langid = {english},
  options = {useprefix=true},
}

@Article{nauFinettiWasRight2001,
  title = {De {{Finetti}} Was {{Right}}: {{Probability Does Not Exist}}},
  shorttitle = {De {{Finetti}} Was {{Right}}},
  author = {Robert F. Nau},
  date = {2001-12-01},
  journaltitle = {Theory and Decision},
  shortjournal = {Theory and Decision},
  volume = {51},
  pages = {89--124},
  issn = {1573-7187},
  doi = {10.1023/A:1015525808214},
  url = {https://doi.org/10.1023/A:1015525808214},
  urldate = {2021-02-07},
  abstract = {De Finetti's treatise on the theory of probability begins with the provocative statement PROBABILITY DOES NOT EXIST, meaning that probability does not exist in an objective sense. Rather, probability exists only subjectively within the minds of individuals. De Finetti defined subjective probabilities in terms of the rates at which individuals are willing to bet money on events, even though, in principle, such betting rates could depend on state-dependent marginal utility for money as well as on beliefs. Most later authors, from Savage onward, have attempted to disentangle beliefs from values by introducing hypothetical bets whose payoffs are abstract consequences that are assumed to have state-independent utility. In this paper, I argue that de Finetti was right all along: PROBABILITY, considered as a numerical measure of pure belief uncontaminated by attitudes toward money, does not exist. Rather, what exist are de Finetti's `previsions', or betting rates for money, otherwise known in the literature as `risk neutral probabilities'. But the fact that previsions are not measures of pure belief turns out not to be problematic for statistical inference, decision analysis, or economic modeling.},
  annotation = {80 citations (Semantic Scholar/DOI) [2021-02-13]},
  file = {/Users/storopoli/Zotero/storage/5QGSXK3A/Nau - 2001 - De Finetti was Right Probability Does Not Exist.pdf},
  langid = {english},
  number = {2},
}
@Book{kolmogorovFoundationsTheoryProbability1933,
  title = {Foundations of the {{Theory}} of {{Probability}}},
  shorttitle = {Foundations of the {{Theory}} of {{Probability}}},
  author = {Andrey Nikolaevich Kolmogorov},
  date = {1933},
  publisher = {{Julius Springer}},
  location = {{Berlin}},
  file = {/Users/storopoli/Zotero/storage/NMSXCGSW/books.html},
}
@article{neyman1937outline,
  title={Outline of a theory of statistical estimation based on the classical theory of probability},
  author={Neyman, Jerzy},
  journal={Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences},
  volume={236},
  number={767},
  pages={333--380},
  year={1937},
  publisher={The Royal Society London}
}

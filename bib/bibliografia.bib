@article{ioannidis2005most,
  title={Why most published research findings are false},
  author={Ioannidis, John PA},
  journal={PLoS medicine},
  volume={2},
  number={8},
  pages={e124},
  year={2005},
  publisher={Public Library of Science}
}
@book{ritchie2020science,
  title={Science fictions: Exposing fraud, bias, negligence and hype in science},
  author={Ritchie, Stuart},
  year={2020},
  publisher={Random House}
}
@inproceedings{saculinggan2013empirical,
  title={Empirical power comparison of goodness of fit tests for normality in the presence of outliers},
  author={Saculinggan, Mayette and Balase, Emily Amor},
  booktitle={Journal of Physics: Conference Series},
  volume={435},
  number={1},
  pages={012041},
  year={2013},
  organization={IOP Publishing}
}
@article{cumming2009inference,
  title={Inference by eye: reading the overlap of independent confidence intervals},
  author={Cumming, Geoff},
  journal={Statistics in medicine},
  volume={28},
  number={2},
  pages={205--220},
  year={2009},
  publisher={Wiley Online Library}
}
@article{Ioannidis2019,
author = {Ioannidis, John P. A.},
doi = {10.1080/00031305.2018.1447512},
file = {:Users/storopoli/Documents/Mendeley Desktop/Ioannidis - 2019 - What Have We (Not) Learnt from Millions of Scientific Papers with iPi Values.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
mendeley-groups = {Statistics,Disciplina de M{\'{e}}todos Quanti,Disciplina de M{\'{e}}todos Quanti/B{\'{a}}sicos},
month = {mar},
number = {sup1},
pages = {20--25},
title = {{What Have We (Not) Learnt from Millions of Scientific Papers with {\textless}i{\textgreater}P{\textless}/i{\textgreater} Values?}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1447512},
volume = {73},
year = {2019}
}
@article{Baird1983,
 ISSN = {00070882, 14643537},
 URL = {http://www.jstor.org/stable/687444},
 author = {Davis Baird},
 journal = {The British Journal for the Philosophy of Science},
 number = {2},
 pages = {105--118},
 publisher = {[Oxford University Press, The British Society for the Philosophy of Science]},
 title = {The Fisher/Pearson Chi-Squared Controversy: A Turning Point for Inductive Inference},
 volume = {34},
 year = {1983}
}
@article{stigler2007epic,
  title={The epic story of maximum likelihood},
  author={Stigler, Stephen M and others},
  journal={Statistical Science},
  volume={22},
  number={4},
  pages={598--620},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}
@book{fisher1925statistical,
  title={Statistical methods for research workers},
  author={Fisher, Ronald Aylmer},
  year={1925},
  publisher={Oliver and Boyd}
}
@article{head2015extent,
  title={The extent and consequences of p-hacking in science},
  author={Head, Megan L and Holman, Luke and Lanfear, Rob and Kahn, Andrew T and Jennions, Michael D},
  journal={PLoS Biol},
  volume={13},
  number={3},
  pages={e1002106},
  year={2015},
  publisher={Public Library of Science}
}
@article{rosnow1989statistical,
  title={Statistical procedures and the justification of knowledge in psychological science},
  author={Rosnow, Ralph L and Rosenthal, Robert},
  journal={American Psychologist},
  volume={44},
  pages={1276--1284},
  year={1989},
  publisher={American Psychological Association (PsycARTICLES)}
}
@article{Morey2016,
author = {Morey, Richard D. and Hoekstra, Rink and Rouder, Jeffrey N. and Lee, Michael D. and Wagenmakers, Eric-Jan},
doi = {10.3758/s13423-015-0947-8},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
keywords = {Bayesian inference and parameter estimation,Bayesian statistics,Statistical inference,Statistics},
mendeley-groups = {Statistics Bayesian},
month = {feb},
number = {1},
pages = {103--123},
publisher = {Springer New York LLC},
title = {{The fallacy of placing confidence in confidence intervals}},
url = {http://link.springer.com/10.3758/s13423-015-0947-8},
volume = {23},
year = {2016}
}
@manual{downey2016,
 title  = "Probably Overthinking It: There is still only one test",
 author = "Allen Downey",
 url    = "http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html",
 year   = "2016"
}
@article{cobb2007introductory,
  title={The introductory statistics course: A Ptolemaic curriculum?},
  author={Cobb, George W},
  journal={Technology innovations in statistics education},
  volume={1},
  number={1},
  year={2007}
}
@article{Hoekstra2014,
author = {Hoekstra, Rink and Morey, Richard D and Rouder, Jeffrey N and Wagenmakers, Eric-Jan},
doi = {10.3758/s13423-013-0572-3},
issn = {1531-5320},
journal = {Psychonomic Bulletin {\&} Review},
mendeley-groups = {Statistics Bayesian},
number = {5},
pages = {1157--1164},
title = {{Robust misinterpretation of confidence intervals}},
url = {https://doi.org/10.3758/s13423-013-0572-3},
volume = {21},
year = {2014}
}
@article{neyman1937outline,
  title={Outline of a theory of statistical estimation based on the classical theory of probability},
  author={Neyman, Jerzy},
  journal={Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences},
  volume={236},
  number={767},
  pages={333--380},
  year={1937},
  publisher={The Royal Society London}
}
@article{neyman1933,
  title={On the problem of the most efficient tests of statistical hypotheses},
  author={Neyman, Jerzy and Pearson, Egon Sharpe},
  journal={Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume={231},
  number={694-706},
  pages={289--337},
  year={1933},
  publisher={The Royal Society London}
}
@Manual{postcards,
  title = {postcards: Create Beautiful, Simple Personal Websites},
  author = {Sean Kross},
  year = {2021},
  note = {R package version 0.2.0},
  url = {https://CRAN.R-project.org/package=postcards},
}
@Manual{vitae,
  title = {vitae: Curriculum Vitae for R Markdown},
  author = {Mitchell O'Hara-Wild and Rob Hyndman},
  year = {2021},
  note = {R package version 0.4.1},
  url = {https://CRAN.R-project.org/package=vitae},
}
@Book{gelman2013bayesian,
  title = {Bayesian {{Data Analysis}}},
  author = {Andrew Gelman and John B Carlin and Hal S Stern and David B Dunson and Aki Vehtari and Donald B Rubin},
  date = {2013},
  publisher = {{Chapman and Hall/CRC}},
}
@Book{mcelreath2020statistical,
  title = {Statistical Rethinking: {{A Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  author = {Richard McElreath},
  date = {2020},
  publisher = {{CRC press}},
}
@Book{gelman2020regression,
  title = {Regression and Other Stories},
  author = {Andrew Gelman and Jennifer Hill and Aki Vehtari},
  date = {2020},
  publisher = {{Cambridge University Press}},
}
@Article{benjaminRedefineStatisticalSignificance2018,
  title = {Redefine Statistical Significance},
  author = {Daniel J. Benjamin and James O. Berger and Magnus Johannesson and Brian A. Nosek and E.-J. Wagenmakers and Richard Berk and Kenneth A. Bollen and Bj{\"o}rn Brembs and Lawrence Brown and Colin Camerer and David Cesarini and Christopher D. Chambers and Merlise Clyde and Thomas D. Cook and Paul {De Boeck} and Zoltan Dienes and Anna Dreber and Kenny Easwaran and Charles Efferson and Ernst Fehr and Fiona Fidler and Andy P. Field and Malcolm Forster and Edward I. George and Richard Gonzalez and Steven Goodman and Edwin Green and Donald P. Green and Anthony G. Greenwald and Jarrod D. Hadfield and Larry V. Hedges and Leonhard Held and Teck {Hua Ho} and Herbert Hoijtink and Daniel J. Hruschka and Kosuke Imai and Guido Imbens and John P. A. Ioannidis and Minjeong Jeon and James Holland Jones and Michael Kirchler and David Laibson and John List and Roderick Little and Arthur Lupia and Edouard Machery and Scott E. Maxwell and Michael McCarthy and Don A. Moore and Stephen L. Morgan and Marcus Munaf{\a'o} and Shinichi Nakagawa and Brendan Nyhan and Timothy H. Parker and Luis Pericchi and Marco Perugini and Jeff Rouder and Judith Rousseau and Victoria Savalei and Felix D. Sch{\"o}nbrodt and Thomas Sellke and Betsy Sinclair and Dustin Tingley and Trisha {Van Zandt} and Simine Vazire and Duncan J. Watts and Christopher Winship and Robert L. Wolpert and Yu Xie and Cristobal Young and Jonathan Zinman and Valen E. Johnson},
  date = {2018-01-01},
  journaltitle = {Nature Human Behaviour},
  volume = {2},
  pages = {6--10},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0189-z},
  url = {http://www.nature.com/articles/s41562-017-0189-z},
  urldate = {2019-09-08},
  abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
  file = {/Users/storopoli/Zotero/storage/PA6HB5RL/Benjamin et al. - 2018 - Redefine statistical significance(2).pdf;/Users/storopoli/Zotero/storage/V8TY8Y5H/Benjamin et al. - 2018 - Redefine statistical significance.pdf},
  keywords = {★},
  number = {1},
}
@Article{carpenterStanProbabilisticProgramming2017,
  title = {Stan : {{A Probabilistic Programming Language}}},
  author = {Bob Carpenter and Andrew Gelman and Matthew D. Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
  date = {2017},
  journaltitle = {Journal of Statistical Software},
  volume = {76},
  publisher = {{American Statistical Association}},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  url = {http://www.jstatsoft.org/v76/i01/},
  urldate = {2019-10-29},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
  file = {/Users/storopoli/Zotero/storage/7SIEZ4SU/Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf},
  keywords = {Algorithmic differentiation,Bayesian inference,Probabilistic program,Stan},
  number = {1},
}
@Article{etzIntroductionConceptLikelihood2018,
  title = {Introduction to the {{Concept}} of {{Likelihood}} and {{Its Applications}}},
  author = {Alexander Etz},
  date = {2018-03-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {60--69},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245917744314},
  url = {https://doi.org/10.1177/2515245917744314},
  urldate = {2021-02-04},
  abstract = {This Tutorial explains the statistical concept known as likelihood and discusses how it underlies common frequentist and Bayesian statistical methods. The article is suitable for researchers interested in understanding the basis of their statistical tools and is also intended as a resource for teachers to use in their classrooms to introduce the topic to students at a conceptual level.},
  file = {/Users/storopoli/Zotero/storage/TSYEG9WA/Etz - 2018 - Introduction to the Concept of Likelihood and Its .pdf},
  keywords = {Bayes factor,Bayesian,estimation,frequentist,likelihood ratio,tutorial},
  langid = {english},
  number = {1},
}
@Article{etzHowBecomeBayesian2018,
  title = {How to Become a {{Bayesian}} in Eight Easy Steps: {{An}} Annotated Reading List},
  shorttitle = {How to Become a {{Bayesian}} in Eight Easy Steps},
  author = {Alexander Etz and Quentin F. Gronau and Fabian Dablander and Peter A. Edelsbrunner and Beth Baribault},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {219--234},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1317-5},
  url = {https://doi.org/10.3758/s13423-017-1317-5},
  urldate = {2021-02-04},
  abstract = {In this guide, we present a reading list to serve as a concise introduction to Bayesian data analysis. The introduction is geared toward reviewers, editors, and interested researchers who are new to Bayesian statistics. We provide commentary for eight recommended sources, which together cover the theoretical and practical cornerstones of Bayesian statistics in psychology and related sciences. The resources are presented in an incremental order, starting with theoretical foundations and moving on to applied issues. In addition, we outline an additional 32 articles and books that can be consulted to gain background knowledge about various theoretical specifics and Bayesian approaches to frequently used models. Our goal is to offer researchers a starting point for understanding the core tenets of Bayesian analysis, while requiring a low level of time commitment. After consulting our guide, the reader should understand how and why Bayesian methods work, and feel able to evaluate their use in the behavioral and social sciences.},
  file = {/Users/storopoli/Zotero/storage/PHLWSB4U/Etz et al. - 2018 - How to become a Bayesian in eight easy steps An a.pdf},
  langid = {english},
  number = {1},
}
@Book{brooksHandbookMarkovChain2011,
  title = {Handbook of {{Markov Chain Monte Carlo}}},
  author = {Steve Brooks and Andrew Gelman and Galin Jones and Xiao-Li Meng},
  date = {2011-05-10},
  publisher = {{CRC Press}},
  abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisherie},
  eprint = {qfRsAIKZ4rIC},
  eprinttype = {googlebooks},
  isbn = {978-1-4200-7942-5},
  keywords = {Mathematics / Probability & Statistics / General},
  langid = {english},
  pagetotal = {620},
}
@InCollection{geyer2011introduction,
  title = {Introduction to Markov Chain Monte Carlo},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  author = {Charles J Geyer},
  editor = {Steve Brooks and Andrew Gelman and Galin L. Jones and Xiao-Li Meng},
  date = {2011},
  file = {/Users/storopoli/Zotero/storage/N8ET8IFD/Geyer - 2011 - Introduction to markov chain monte carlo.pdf},
}
@Article{mcshaneAbandonStatisticalSignificance2019,
  title = {Abandon {{Statistical Significance}}},
  author = {Blakeley B. McShane and David Gal and Andrew Gelman and Christian Robert and Jennifer L. Tackett},
  date = {2019-03-29},
  journaltitle = {American Statistician},
  volume = {73},
  pages = {235--245},
  publisher = {{American Statistical Association}},
  issn = {15372731},
  doi = {10.1080/00031305.2018.1527253},
  abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm--and the p-value thresholds intrinsic to it--as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to {"}ban{"} p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
  file = {/Users/storopoli/Zotero/storage/ETT2EEP5/McShane et al. - 2019 - Abandon Statistical Significance.pdf},
  issue = {sup1},
  keywords = {★,Null hypothesis significance testing,p-Value,Replication,Sociology of science,Statistical significance},
}
@Article{amrheinScientistsRiseStatistical2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Valentin Amrhein and Sander Greenland and Blake McShane},
  date = {2019-03-21},
  journaltitle = {Nature},
  volume = {567},
  pages = {305--307},
  publisher = {{Nature Publishing Group}},
  issn = {14764687},
  doi = {10.1038/d41586-019-00857-9},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  eprint = {30894741},
  eprinttype = {pmid},
  file = {/Users/storopoli/Zotero/storage/PGP5KXFC/Amrhein, Greenland, McShane - 2019 - Scientists rise up against statistical significance.pdf},
  keywords = {Research data,Research management},
  number = {7748},
}
@Article{vanravenzwaaijSimpleIntroductionMarkov2018,
  title = {A Simple Introduction to {{Markov Chain Monte}}–{{Carlo}} Sampling},
  author = {Don {van Ravenzwaaij} and Pete Cassey and Scott D. Brown},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin and Review},
  volume = {25},
  pages = {143--154},
  publisher = {{Springer New York LLC}},
  issn = {15315320},
  doi = {10.3758/s13423-016-1015-8},
  abstract = {© 2016, The Author(s). Markov Chain Monte–Carlo (MCMC) is an increasingly popular method for obtaining information about distributions, especially for estimating posterior distributions in Bayesian inference. This article provides a very basic introduction to MCMC sampling. It describes what MCMC is, and what it can be used for, with simple illustrative examples. Highlighted are some of the benefits and limitations of MCMC sampling, as well as different approaches to circumventing the limitations most likely to trouble cognitive scientists.},
  file = {/Users/storopoli/Zotero/storage/MHB6FDY3/van Ravenzwaaij, Cassey, Brown - 2018 - A simple introduction to Markov Chain Monte–Carlo sampling.pdf},
  keywords = {★,Bayesian inference,Markov Chain Monte–Carlo,MCMC,Tutorial},
  number = {1},
  options = {useprefix=true},
}
@InCollection{vandekerckhove2015model,
  title = {Model Comparison and the Principle of Parsimony},
  booktitle = {Oxford Handbook of Computational and Mathematical Psychology},
  author = {Joachim Vandekerckhove and Dora Matzke and Eric-Jan Wagenmakers and {others}},
  editor = {Jerome R. Busemeyer and Zheng Wang and James T. Townsend and Ami Eidels},
  date = {2015},
  pages = {300--319},
  publisher = {{Oxford University Press Oxford}},
  file = {/Users/storopoli/Zotero/storage/GQUFF6S5/Vandekerckhove et al. - 2015 - Model comparison and the principle of parsimony.pdf},
  isbn = {978-0-19-995799-6},
  keywords = {★},
}
@Article{schootGentleIntroductionBayesian2014,
  title = {A {{Gentle Introduction}} to {{Bayesian Analysis}}: {{Applications}} to {{Developmental Research}}},
  shorttitle = {A {{Gentle Introduction}} to {{Bayesian Analysis}}},
  author = {Rens {van de Schoot} and David Kaplan and Jaap Denissen and Jens B. Asendorpf and Franz J. Neyer and Marcel A. G. {van Aken}},
  date = {2014},
  journaltitle = {Child Development},
  volume = {85},
  pages = {842--860},
  issn = {1467-8624},
  doi = {10.1111/cdev.12169},
  url = {https://srcd.onlinelibrary.wiley.com/doi/abs/10.1111/cdev.12169},
  urldate = {2021-02-04},
  abstract = {Bayesian statistical methods are becoming ever more popular in applied and fundamental research. In this study a gentle introduction to Bayesian analysis is provided. It is shown under what circumstances it is attractive to use Bayesian estimation, and how to interpret properly the results. First, the ingredients underlying Bayesian methods are introduced using a simplified example. Thereafter, the advantages and pitfalls of the specification of prior knowledge are discussed. To illustrate Bayesian methods explained in this study, in a second example a series of studies that examine the theoretical framework of dynamic interactionism are considered. In the Discussion the advantages and disadvantages of using Bayesian statistics are reviewed, and guidelines on how to report on Bayesian statistics are provided.},
  annotation = {\_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/cdev.12169},
  file = {/Users/storopoli/Zotero/storage/H53J5TDA/Schoot et al. - 2014 - A Gentle Introduction to Bayesian Analysis Applic.pdf},
  langid = {english},
  number = {3},
}

@Article{Wagenmakers2007,
  title = {A Practical Solution to the Pervasive Problems of p Values},
  author = {Eric-Jan Wagenmakers},
  date = {2007-10},
  journaltitle = {Psychonomic Bulletin \& Review},
  volume = {14},
  pages = {779--804},
  publisher = {{Psychonomic Society Inc.}},
  issn = {1069-9384},
  doi = {10.3758/BF03194105},
  url = {http://www.springerlink.com/index/10.3758/BF03194105},
  urldate = {2019-09-09},
  abstract = {The primary goal of this article is to promote awareness of the various statistical problems associated with the use of p value null-hypothesis significance testing (NHST). Making no claim of completeness, I review three prob-lems with NHST, briefly explaining their causes and con-sequences (see Karabatsos, 2006). The discussion of each problem is accompanied by concrete examples and refer-ences to the statistical literature. In the psychological literature, the pros and cons of NHST have been, and continue to be, hotly debated The issues that have dominated the NHST discussion in the psychological lit-erature are that (1) NHST tempts the user into confusing the probability of the hypothesis given the data with the probability of the data given the hypothesis; (2) (.05 is an arbitrary criterion for significance; and (3) in real-world applications, the null hypothesis is never exactly true, and will therefore always be rejected as the number of observations grows large. In the statistical literature, the pros and cons of NHST are also the topic of an ongoing dispute (e. A comparison of these two literatures shows that in psychology, the NHST discussion has focused mostly on problems of interpretation, whereas in statistics, the NHST discussion has focused mostly on problems of for-mal construction. The statistical perspective on the prob-lems associated with NHST is therefore fundamentally different from the psychological perspective. In this ar-ticle, the goal is to explain NHST and its problems from a statistical perspective. Many psychologists are oblivious to certain statistical problems associated with NHST, and the examples below show that this ignorance can have im-portant ramifications. In this article, I will show that an NHST p value de-pends on data that were never observed: The p value is a tail-area integral, and this integral is effectively over data that are not observed but only hypothesized. The prob-ability of these hypothesized data depends crucially on the possibly unknown subjective intentions of the researcher who carried out the experiment. If these intentions were to be ignored, a user of NHST could always obtain a sig-nificant result through optional stopping (i.e., analyzing the data as they accumulate and stopping the experiment whenever the p value reaches some desired significance level). In the context of NHST, it is therefore necessary to know the subjective intention with which an experi-ment was carried out. This key requirement is unattain-able in a practical sense, and arguably undesirable in a philosophical sense. In addition, I will review a proof that the NHST p value does not measure statistical evidence. In order for the p value to qualify as a measure of statisti-cal evidence, a minimum requirement is that identical p values convey identical levels of evidence, irrespective},
  file = {/Users/storopoli/Zotero/storage/T5R582FH/Wagenmakers - 2007 - A practical solution to the pervasive problems of p values.pdf},
  keywords = {★},
  number = {5},
}
@Article{cohenEarth051994,
  title = {The Earth Is Round (p {$<$} .05)},
  author = {Jacob Cohen},
  date = {1994},
  journaltitle = {American Psychologist},
  volume = {49},
  pages = {997--1003},
  publisher = {{American Psychological Association Inc.}},
  issn = {0003066X},
  doi = {10.1037/0003-066X.49.12.997},
  abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.},
  file = {/Users/storopoli/Zotero/storage/33N57CEV/Cohen - 1994 - The earth is round (p .05).pdf},
  keywords = {★},
  number = {12},
}

@Article{dienesBayesianOrthodoxStatistics2011,
  title = {Bayesian {{Versus Orthodox Statistics}}: {{Which Side Are You On}}?},
  shorttitle = {Bayesian {{Versus Orthodox Statistics}}},
  author = {Zoltan Dienes},
  date = {2011-05-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {6},
  pages = {274--290},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691611406920},
  url = {https://doi.org/10.1177/1745691611406920},
  urldate = {2021-02-04},
  abstract = {Researchers are often confused about what can be inferred from significance tests. One problem occurs when people apply Bayesian intuitions to significance testing—two approaches that must be firmly separated. This article presents some common situations in which the approaches come to different conclusions; you can see where your intuitions initially lie. The situations include multiple testing, deciding when to stop running participants, and when a theory was thought of relative to finding out results. The interpretation of nonsignificant results has also been persistently problematic in a way that Bayesian inference can clarify. The Bayesian and orthodox approaches are placed in the context of different notions of rationality, and I accuse myself and others as having been irrational in the way we have been using statistics on a key notion of rationality. The reader is shown how to apply Bayesian inference in practice, using free online software, to allow more coherent inferences from data.},
  file = {/Users/storopoli/Zotero/storage/3KISFEQS/Dienes - 2011 - Bayesian Versus Orthodox Statistics Which Side Ar.pdf},
  keywords = {Bayes,evidence,likelihood principle,significance testing,statistical inference},
  langid = {english},
  number = {3},
}

@Article{etzIntroductionBayesianInference2018,
  title = {Introduction to {{Bayesian Inference}} for {{Psychology}}},
  author = {Alexander Etz and Joachim Vandekerckhove},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {5--34},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1262-3},
  url = {https://doi.org/10.3758/s13423-017-1262-3},
  urldate = {2021-02-04},
  abstract = {We introduce the fundamental tenets of Bayesian inference, which derive from two basic laws of probability theory. We cover the interpretation of probabilities, discrete and continuous versions of Bayes’ rule, parameter estimation, and model comparison. Using seven worked examples, we illustrate these principles and set up some of the technical background for the rest of this special issue of Psychonomic Bulletin \& Review. Supplemental material is available via https://osf.io/wskex/.},
  file = {/Users/storopoli/Zotero/storage/MSQDFTM6/Etz and Vandekerckhove - 2018 - Introduction to Bayesian Inference for Psychology.pdf},
  langid = {english},
  number = {1},
}

@Article{junior2020vale,
  title = {Quanto Vale o Valor-p?},
  author = {Carlos Alberto Mour{\~a}o J{\a'u}nior},
  date = {2020},
  journaltitle = {Arquivos de Ciências do Esporte},
  volume = {7},
  number = {2},
}

@Article{kerrHARKingHypothesizingResults1998,
  title = {{{HARKing}}: {{Hypothesizing}} after the Results Are Known},
  author = {Norbert L. Kerr},
  date = {1998},
  journaltitle = {Personality and Social Psychology Review},
  volume = {2},
  pages = {196--217},
  publisher = {{SAGE Publications Inc.}},
  issn = {10888683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  file = {/Users/storopoli/Zotero/storage/PKLRS2YS/Kerr - 1998 - HARKing Hypothesizing after the results are known.pdf},
  keywords = {★},
  number = {3},
}

@InCollection{kruschke2015bayesian,
  title = {Bayesian Estimation in Hierarchical Models},
  booktitle = {The {{Oxford}} Handbook of Computational and Mathematical Psychology},
  author = {John K Kruschke and Wolf Vanpaemel},
  editor = {Jerome R. Busemeyer and Zheng Wang and James T. Townsend and Ami Eidels},
  date = {2015},
  pages = {279--299},
  publisher = {{Oxford University Press Oxford, UK}},
  file = {/Users/storopoli/Zotero/storage/MX88HPCI/Kruschke, Vanpaemel - 2015 - Bayesian estimation in hierarchical models.pdf},
  isbn = {978-0-19-995799-6},
  keywords = {★},
}

@Article{kruschkeBayesianDataAnalysis2018,
  title = {Bayesian Data Analysis for Newcomers},
  author = {John K. Kruschke and Torrin M. Liddell},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {155--177},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1272-1},
  url = {https://doi.org/10.3758/s13423-017-1272-1},
  urldate = {2021-02-04},
  abstract = {This article explains the foundational concepts of Bayesian data analysis using virtually no mathematical notation. Bayesian ideas already match your intuitions from everyday reasoning and from traditional data analysis. Simple examples of Bayesian data analysis are presented that illustrate how the information delivered by a Bayesian analysis can be directly interpreted. Bayesian approaches to null-value assessment are discussed. The article clarifies misconceptions about Bayesian methods that newcomers might have acquired elsewhere. We discuss prior distributions and explain how they are not a liability but an important asset. We discuss the relation of Bayesian data analysis to Bayesian models of mind, and we briefly discuss what methodological problems Bayesian data analysis is not meant to solve. After you have read this article, you should have a clear sense of how Bayesian data analysis works and the sort of information it delivers, and why that information is so intuitive and useful for drawing conclusions from data.},
  file = {/Users/storopoli/Zotero/storage/6NFCW2P8/Kruschke and Liddell - 2018 - Bayesian data analysis for newcomers.pdf},
  langid = {english},
  number = {1},
}

@Article{kruschkeBayesianNewStatistics2018,
  title = {The {{Bayesian New Statistics}}: {{Hypothesis}} Testing, Estimation, Meta-Analysis, and Power Analysis from a {{Bayesian}} Perspective},
  shorttitle = {The {{Bayesian New Statistics}}},
  author = {John K. Kruschke and Torrin M. Liddell},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {178--206},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1221-4},
  url = {https://doi.org/10.3758/s13423-016-1221-4},
  urldate = {2021-02-04},
  abstract = {In the practice of data analysis, there is a conceptual distinction between hypothesis testing, on the one hand, and estimation with quantified uncertainty on the other. Among frequentists in psychology, a shift of emphasis from hypothesis testing to estimation has been dubbed “the New Statistics” (Cumming 2014). A second conceptual distinction is between frequentist methods and Bayesian methods. Our main goal in this article is to explain how Bayesian methods achieve the goals of the New Statistics better than frequentist methods. The article reviews frequentist and Bayesian approaches to hypothesis testing and to estimation with confidence or credible intervals. The article also describes Bayesian approaches to meta-analysis, randomized controlled trials, and power analysis.},
  file = {/Users/storopoli/Zotero/storage/U3IMNGH5/Kruschke and Liddell - 2018 - The Bayesian New Statistics Hypothesis testing, e.pdf},
  langid = {english},
  number = {1},
}

@Article{lakensJustifyYourAlpha2018,
  title = {Justify Your Alpha},
  author = {Daniel Lakens and Federico G. Adolfi and Casper J. Albers and Farid Anvari and Matthew A.J. Apps and Shlomo E. Argamon and Thom Baguley and Raymond B. Becker and Stephen D. Benning and Daniel E. Bradford and Erin M. Buchanan and Aaron R. Caldwell and Ben {Van Calster} and Rickard Carlsson and Sau Chin Chen and Bryan Chung and Lincoln J. Colling and Gary S. Collins and Zander Crook and Emily S. Cross and Sameera Daniels and Henrik Danielsson and Lisa Debruine and Daniel J. Dunleavy and Brian D. Earp and Michele I. Feist and Jason D. Ferrell and James G. Field and Nicholas W. Fox and Amanda Friesen and Caio Gomes and Monica Gonzalez-Marquez and James A. Grange and Andrew P. Grieve and Robert Guggenberger and James Grist and Anne Laura {Van Harmelen} and Fred Hasselman and Kevin D. Hochard and Mark R. Hoffarth and Nicholas P. Holmes and Michael Ingre and Peder M. Isager and Hanna K. Isotalus and Christer Johansson and Konrad Juszczyk and David A. Kenny and Ahmed A. Khalil and Barbara Konat and Junpeng Lao and Erik Gahner Larsen and Gerine M.A. Lodder and Ji{\v r}{\a'\i} Lukavsk{\a`y} and Christopher R. Madan and David Manheim and Stephen R. Martin and Andrea E. Martin and Deborah G. Mayo and Randy J. McCarthy and Kevin McConway and Colin McFarland and Amanda Q.X. Nio and Gustav Nilsonne and Cilene Lino {De Oliveira} and Jean Jacques Orban {De Xivry} and Sam Parsons and Gerit Pfuhl and Kimberly A. Quinn and John J. Sakon and S. Adil Saribay and Iris K. Schneider and Manojkumar Selvaraju and Zsuzsika Sjoerds and Samuel G. Smith and Tim Smits and Jeffrey R. Spies and Vishnu Sreekumar and Crystal N. Steltenpohl and Neil Stenhouse and Wojciech {{\a'S}wi{\k a}tkowski} and Miguel A. Vadillo and Marcel A.L.M. {Van Assen} and Matt N. Williams and Samantha E. Williams and Donald R. Williams and Tal Yarkoni and Ignazio Ziano and Rolf A. Zwaan},
  date = {2018-03-01},
  journaltitle = {Nature Human Behaviour},
  volume = {2},
  pages = {168--171},
  publisher = {{Nature Publishing Group}},
  issn = {23973374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P ≤ 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  file = {/Users/storopoli/Zotero/storage/KUUEK8R3/Lakens et al. - 2018 - Justify your alpha.pdf},
  keywords = {★},
  number = {3},
}

@Article{murphyHARKingHowBadly2019,
  title = {{{HARKing}}: {{How Badly Can Cherry}}-{{Picking}} and {{Question Trolling Produce Bias}} in {{Published Results}}?},
  author = {Kevin R. Murphy and Herman Aguinis},
  date = {2019-02-15},
  journaltitle = {Journal of Business and Psychology},
  volume = {34},
  publisher = {{Springer New York LLC}},
  issn = {08893268},
  doi = {10.1007/s10869-017-9524-7},
  abstract = {© 2017 Springer Science+Business Media, LLC, part of Springer Nature The practice of hypothesizing after results are known (HARKing) has been identified as a potential threat to the credibility of research results. We conducted simulations using input values based on comprehensive meta-analyses and reviews in applied psychology and management (e.g., strategic management studies) to determine the extent to which two forms of HARKing behaviors might plausibly bias study outcomes and to examine the determinants of the size of this effect. When HARKing involves cherry-picking, which consists of searching through data involving alternative measures or samples to find the results that offer the strongest possible support for a particular hypothesis or research question, HARKing has only a small effect on estimates of the population effect size. When HARKing involves question trolling, which consists of searching through data involving several different constructs, measures of those constructs, interventions, or relationships to find seemingly notable results worth writing about, HARKing produces substantial upward bias particularly when it is prevalent and there are many effects from which to choose. Results identify the precise circumstances under which different forms of HARKing behaviors are more or less likely to have a substantial impact on a study’s substantive conclusions and the field’s cumulative knowledge. We offer suggestions for authors, consumers of research, and reviewers and editors on how to understand, minimize, detect, and deter detrimental forms of HARKing in future research.},
  file = {/Users/storopoli/Zotero/storage/U44AUQFK/Murphy, Aguinis - 2019 - HARKing How Badly Can Cherry-Picking and Question Trolling Produce Bias in Published Results.pdf},
  keywords = {★,Data snooping,HARKing,Publication bias,Simulation},
  number = {1},
}

@Article{starkCargocultStatisticsScientific2018,
  title = {Cargo-Cult Statistics and Scientific Crisis},
  author = {Philip B. Stark and Andrea Saltelli},
  date = {2018-08-01},
  journaltitle = {Significance},
  volume = {15},
  pages = {40--43},
  publisher = {{John Wiley \& Sons, Ltd (10.1111)}},
  issn = {17409705},
  doi = {10.1111/j.1740-9713.2018.01174.x},
  url = {http://doi.wiley.com/10.1111/j.1740-9713.2018.01174.x},
  urldate = {2019-08-01},
  file = {/Users/storopoli/Zotero/storage/LYJA9MIV/Stark, Saltelli - 2018 - Cargo-cult statistics and scientific crisis.pdf},
  keywords = {★},
  number = {4},
}
@Article{gabryVisualizationBayesianWorkflow2019,
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Jonah Gabry and Daniel Simpson and Aki Vehtari and Michael Betancourt and Andrew Gelman},
  date = {2019-02-01},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {182},
  pages = {389--402},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {09641998},
  doi = {10.1111/rssa.12378},
  url = {http://doi.wiley.com/10.1111/rssa.12378},
  urldate = {2019-09-16},
  abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
  file = {/Users/storopoli/Zotero/storage/H3J8LKVH/Gabry et al. - 2019 - Visualization in Bayesian workflow(2).pdf;/Users/storopoli/Zotero/storage/ZD32SJNY/Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf},
  keywords = {★,Bayesian data analysis,bayesplot,Statistical graphics,Statistical workflow},
  number = {2},
}

@Online{gelmanBayesianWorkflow2020,
  title = {Bayesian {{Workflow}}},
  author = {Andrew Gelman and Aki Vehtari and Daniel Simpson and Charles C. Margossian and Bob Carpenter and Yuling Yao and Lauren Kennedy and Jonah Gabry and Paul-Christian B{\"u}rkner and Martin Modr{\a'a}k},
  date = {2020-11-03},
  url = {http://arxiv.org/abs/2011.01808},
  urldate = {2021-02-04},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  eprint = {2011.01808},
  eprinttype = {arxiv},
  file = {/Users/storopoli/Zotero/storage/KBQP2U94/Gelman et al. - 2020 - Bayesian Workflow.pdf;/Users/storopoli/Zotero/storage/79ZD8TLD/2011.html},
  keywords = {Statistics - Methodology},
  primaryclass = {stat},
}
